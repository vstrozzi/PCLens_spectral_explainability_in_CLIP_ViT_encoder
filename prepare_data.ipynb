{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System Requirement ###\n",
    "# At least 32 GB RAM (Cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the necessary data for the experiments in playground.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"imagenet\"\n",
    "device = 'cuda'\n",
    "model_name = 'ViT-L-14-336' # 'ViT-H-14'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = 10\n",
    "tot_samples_per_class = 50\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"imagenet\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "path = './datasets'\n",
    "# Additional params for LLAVA\n",
    "full_output = False # If we want intermediate hiddn tokens, def False\n",
    "vision_proj = True # If we want to project in shared space, def True\n",
    "\n",
    "if model_name == \"ViT-H-14\":\n",
    "    pretrained = \"laion2B-s32B-b79K\"\n",
    "    precision = \"fp32\"\n",
    "elif model_name == \"ViT-L-14\":\n",
    "    pretrained = \"laion2B-s32B-b82K\"\n",
    "    precision = \"fp32\"\n",
    "elif model_name == \"ViT-B-16\":\n",
    "    pretrained = \"laion2B-s34B-b88K\"\n",
    "    precision = \"fp32\"\n",
    "elif model_name == \"ViT-B-32\":\n",
    "    pretrained = \"laion2B-s34B-b79K\"\n",
    "    precision = \"fp32\"\n",
    "elif model_name == \"ViT-L-14-336\":\n",
    "    pretrained = \"openai\"\n",
    "    precision = \"fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "Len of res: 24\n",
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "We are using a dataset containing 10000 images.\n",
      "100%|█████████████████████████████████████| 10000/10000 [06:59<00:00, 23.83it/s]\n",
      "\n",
      "Concatenating chunk files into final .npy arrays...\n",
      "Final single-file arrays created:\n",
      "  ./output_dir/imagenet_attn_ViT-L-14-336_seed_0.npy\n",
      "  ./output_dir/imagenet_mlp_ViT-L-14-336_seed_0.npy\n",
      "  ./output_dir/imagenet_cls_attn_ViT-L-14-336_seed_0.npy\n",
      "  ./output_dir/imagenet_labels_ViT-L-14-336_seed_0.npy\n",
      "Deleting chunk files...\n",
      "Chunk files removed.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the activations (hooks) for each head in each layer, of a given ViT-model.\n",
    "# Opt.: Setup a seed and use only a subset of the dataset (Imagenet)\n",
    "!python -m utils.scripts.compute_activation_values --dataset {dataset} --device {device} --model {model_name} --pretrained {pretrained} --seed {seed} --batch_size 1  --samples_per_class {subset_dim} --tot_samples_per_class {tot_samples_per_class} --quantization {precision} --cache_dir \"../cache/\" --full_output {full_output} --vision_proj {vision_proj} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 24, 16, 768) (10000, 25, 768)\n"
     ]
    }
   ],
   "source": [
    "# Use the previous outputs (i.e. all the activations) to derive the final clip embeddings for the image dataset.\n",
    "!python -m utils.scripts.compute_images_embedding --dataset {dataset} --model {model_name} --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:25<00:00, 38.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Derive the CLIP-embeddings of the classes' labels of a given dataset (i.e. allow zero-shot classifications)\n",
    "!python -m utils.scripts.compute_classes_embeddings --dataset {dataset} --device {device} --model {model_name} --pretrained {pretrained} --cache_dir \"../cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Given a dataset of text compute their CLIP's embeddings.\n",
    "!python -m utils.scripts.compute_text_embeddings --device {device} --model {model_name} --pretrained {pretrained} --cache_dir \"../cache/\" --data_path \"utils/text_descriptions/{dataset_text_name}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test text explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 24\n",
      "100%|███████████████████████████████████████████| 20/20 [00:08<00:00,  2.25it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\n",
      "Layer [20], Head: 0\n",
      "\n",
      "Layer [20], Head: 1\n",
      "\n",
      "Layer [20], Head: 2\n",
      "\n",
      "Layer [20], Head: 3\n",
      "\n",
      "Layer [20], Head: 4\n",
      "\n",
      "Layer [20], Head: 5\n",
      "\n",
      "Layer [20], Head: 6\n",
      "\n",
      "Layer [20], Head: 7\n",
      "\n",
      "Layer [20], Head: 8\n",
      "\n",
      "Layer [20], Head: 9\n",
      "\n",
      "Layer [20], Head: 10\n",
      "\n",
      "Layer [20], Head: 11\n",
      "\n",
      "Layer [20], Head: 12\n",
      "\n",
      "Layer [20], Head: 13\n",
      "\n",
      "Layer [20], Head: 14\n",
      "\n",
      "Layer [20], Head: 15\n",
      " 25%|███████████▎                                 | 1/4 [00:06<00:19,  6.62s/it]\n",
      "Layer [21], Head: 0\n",
      "\n",
      "Layer [21], Head: 1\n",
      "\n",
      "Layer [21], Head: 2\n",
      "\n",
      "Layer [21], Head: 3\n",
      "\n",
      "Layer [21], Head: 4\n",
      "\n",
      "Layer [21], Head: 5\n",
      "\n",
      "Layer [21], Head: 6\n",
      "\n",
      "Layer [21], Head: 7\n",
      "\n",
      "Layer [21], Head: 8\n",
      "\n",
      "Layer [21], Head: 9\n",
      "\n",
      "Layer [21], Head: 10\n",
      "\n",
      "Layer [21], Head: 11\n",
      "\n",
      "Layer [21], Head: 12\n",
      "\n",
      "Layer [21], Head: 13\n",
      "\n",
      "Layer [21], Head: 14\n",
      "\n",
      "Layer [21], Head: 15\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.65s/it]\n",
      "Layer [22], Head: 0\n",
      "\n",
      "Layer [22], Head: 1\n",
      "\n",
      "Layer [22], Head: 2\n",
      "\n",
      "Layer [22], Head: 3\n",
      "\n",
      "Layer [22], Head: 4\n",
      "\n",
      "Layer [22], Head: 5\n",
      "\n",
      "Layer [22], Head: 6\n",
      "\n",
      "Layer [22], Head: 7\n",
      "\n",
      "Layer [22], Head: 8\n",
      "\n",
      "Layer [22], Head: 9\n",
      "\n",
      "Layer [22], Head: 10\n",
      "\n",
      "Layer [22], Head: 11\n",
      "\n",
      "Layer [22], Head: 12\n",
      "\n",
      "Layer [22], Head: 13\n",
      "\n",
      "Layer [22], Head: 14\n",
      "\n",
      "Layer [22], Head: 15\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:16<00:05,  5.34s/it]\n",
      "Layer [23], Head: 0\n",
      "\n",
      "Layer [23], Head: 1\n",
      "\n",
      "Layer [23], Head: 2\n",
      "\n",
      "Layer [23], Head: 3\n",
      "\n",
      "Layer [23], Head: 4\n",
      "\n",
      "Layer [23], Head: 5\n",
      "\n",
      "Layer [23], Head: 6\n",
      "\n",
      "Layer [23], Head: 7\n",
      "\n",
      "Layer [23], Head: 8\n",
      "\n",
      "Layer [23], Head: 9\n",
      "\n",
      "Layer [23], Head: 10\n",
      "\n",
      "Layer [23], Head: 11\n",
      "\n",
      "Layer [23], Head: 12\n",
      "\n",
      "Layer [23], Head: 13\n",
      "\n",
      "Layer [23], Head: 14\n",
      "\n",
      "Layer [23], Head: 15\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:21<00:00,  5.35s/it]\n",
      "\n",
      "Layer [-1], Head: -1\n",
      "Current accuracy: 70.34 \n",
      "Number of texts: 0\n"
     ]
    }
   ],
   "source": [
    "# Test an algorithm to explain the CLIP-embeddings with text (ours: svd_data_approx, their: text_span)\n",
    "!python -m utils.scripts.compute_text_explanations --device {device} --model {model_name} --algorithm svd_data_approx --dataset {dataset} --seed {seed} --num_of_last_layers 4 --text_descriptions {dataset_text_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the components with have generated\n",
    "import json\n",
    "\n",
    "# Read JSON lines\n",
    "with open(\"output_dir/CIFAR10_completeness_top_1500_nouns_5_sentences_imagenet_clean_ViT-L-14-336_algo_svd_data_approx_seed_0_max_text80.jsonl\", \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        entry = json.loads(line)  # Parse each line as a JSON object\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        texts = entry[\"embeddings_sort\"]\n",
    "\n",
    "        if entry[\"head\"] == -1:\n",
    "            print(entry.keys())\n",
    "            print(entry[\"accuracy\"])\n",
    "        \n",
    "        print(len(entry[\"s\"]))\n",
    "        print(f\"Layer: {layer}, Head: {head}\")\n",
    "        print(\"Texts:\")\n",
    "        for text in texts:\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go on playground to analyze the results\n",
    "models = [\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-H-14\", \"ViT-L-14-336\"]\n",
    "algorithms = [\"svd_data_approx\", \"text_span\"]\n",
    "seed = 0\n",
    "for model in models:\n",
    "    for algorithm in algorithms:\n",
    "        with open(f\"output_dir/imagenet_completeness_top_1500_nouns_5_sentences_imagenet_clean_ViT-B-32_algo_text_span_seed_0.jsonl\", \"r\") as json_file:\n",
    "\n",
    "            for line in json_file:\n",
    "                entry = json.loads(line)  # Parse each line as a JSON object\n",
    "                layer = entry[\"layer\"]\n",
    "                head = entry[\"head\"]\n",
    "                texts = entry[\"embeddings_sort\"]\n",
    "\n",
    "                if entry[\"head\"] == -1:\n",
    "                    print(entry.keys())\n",
    "                    print(entry[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
