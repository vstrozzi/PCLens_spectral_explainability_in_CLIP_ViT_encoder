{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System Requirement ###\n",
    "# At least 32 GB RAM (Cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the necessary data for the experiments in playground.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"imagenet\"\n",
    "model =\"ViT-L-14-336\"## Parameters\n",
    "device = 'cuda'\n",
    "model_name = 'ViT-L-14-336' # 'ViT-H-14'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = 10\n",
    "tot_samples_per_class = 50\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"imagenet\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "path = './datasets'\n",
    "\n",
    "if model_name == \"ViT-H-14\":\n",
    "    pretrained = \"laion2B-s32B-b79K\"\n",
    "elif model_name == \"ViT-L-14\":\n",
    "    pretrained = \"laion2B-s32B-b82K\"\n",
    "elif model_name == \"ViT-B-16\":\n",
    "    pretrained = \"laion2B-s34B-b88K\"\n",
    "elif model_name == \"ViT-B-32\":\n",
    "    pretrained = \"laion2B-s34B-b79K\"\n",
    "elif model_name == \"ViT-L-14-336\":\n",
    "    pretrained = \"openai\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "Len of res: 24\n",
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "We are using a dataset containing 10000 images.\n",
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]palle\n",
      "palle\n",
      "  0%|                                      | 1/10000 [00:06<18:50:32,  6.78s/it]palle\n",
      "palle\n",
      "  0%|                                      | 2/10000 [00:11<15:13:08,  5.48s/it]palle\n",
      "palle\n",
      "  0%|                                      | 3/10000 [00:15<13:53:02,  5.00s/it]palle\n",
      "palle\n",
      "  0%|                                      | 4/10000 [00:20<13:30:22,  4.86s/it]palle\n",
      "palle\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the activations (hooks) for each head in each layer, of a given ViT-model.\n",
    "# Opt.: Setup a seed and use only a subset of the dataset (Imagenet)\n",
    "!python -m utils.scripts.compute_activation_values --dataset {dataset} --device {device} --model {model_name} --pretrained {pretrained} --seed {seed} --batch_size 1  --samples_per_class 10 --tot_samples_per_class 50 --quantization float16 --cache_dir \"../cache/\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/scripts/compute_images_embedding.py\", line 63, in <module>\n",
      "    main(args)\n",
      "  File \"/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/scripts/compute_images_embedding.py\", line 44, in main\n",
      "    attns = np.load(os.path.join(args.output_dir, f\"{args.dataset}_attn_{args.model}_seed_{args.seed}.npy\"), mmap_mode=\"r\")  # [b, l, h, d]\n",
      "  File \"/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 405, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './output_dir/imagenet_attn_ViT-L-14-336_seed_0.npy'\n"
     ]
    }
   ],
   "source": [
    "# Use the previous outputs (i.e. all the activations) to derive the final clip embeddings for the image dataset.\n",
    "!python -m utils.scripts.compute_images_embedding --dataset {dataset} --model {model_name} --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:25<00:00, 39.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Derive the CLIP-embeddings of the classes' labels of a given dataset (i.e. allow zero-shot classifications)\n",
    "!python -m utils.scripts.compute_classes_embeddings --dataset {dataset} --device {device} --model {model_name} --pretrained {pretrained} --cache_dir \"../cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Given a dataset of text compute their CLIP's embeddings.\n",
    "!python -m utils.scripts.compute_text_embeddings --device {device} --model {model_name} --pretrained {pretrained} --cache_dir \"../cache/\" --data_path utils/text_descriptions/top_1500_nouns_5_sentences_imagenet_bias_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test text explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test an algorithm to explain the CLIP-embeddings with text (ours: svd_data_approx, their: text_span)\n",
    "!python -m utils.scripts.compute_text_explanations --device {device} --model {model_name} --algorithm svd_data_approx --dataset {dataset} --seed {seed} --num_of_last_layers 4 --text_descriptions top_1500_nouns_5_sentences_imagenet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the components with have generated\n",
    "import json\n",
    "\n",
    "# Read JSON lines\n",
    "with open(\"output_dir/CIFAR10_completeness_top_1500_nouns_5_sentences_imagenet_clean_ViT-B-32_algo_svd_data_approx_seed_0_max_text80.jsonl\", \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        entry = json.loads(line)  # Parse each line as a JSON object\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        texts = entry[\"embeddings_sort\"]\n",
    "\n",
    "        if entry[\"head\"] == -1:\n",
    "            print(entry.keys())\n",
    "            print(entry[\"accuracy\"])\n",
    "        \n",
    "        print(len(entry[\"s\"]))\n",
    "        print(f\"Layer: {layer}, Head: {head}\")\n",
    "        print(\"Texts:\")\n",
    "        for text in texts:\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go on playground to analyze the results\n",
    "models = [\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-H-14\"]\n",
    "algorithms = [\"svd_data_approx\", \"text_span\"]\n",
    "seed = 0\n",
    "for model in models:\n",
    "    for algorithm in algorithms:\n",
    "        with open(f\"output_dir/imagenet_completeness_top_1500_nouns_5_sentences_imagenet_clean_ViT-B-32_algo_text_span_seed_0.jsonl\", \"r\") as json_file:\n",
    "\n",
    "            for line in json_file:\n",
    "                entry = json.loads(line)  # Parse each line as a JSON object\n",
    "                layer = entry[\"layer\"]\n",
    "                head = entry[\"head\"]\n",
    "                texts = entry[\"embeddings_sort\"]\n",
    "\n",
    "                if entry[\"head\"] == -1:\n",
    "                    print(entry.keys())\n",
    "                    print(entry[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
