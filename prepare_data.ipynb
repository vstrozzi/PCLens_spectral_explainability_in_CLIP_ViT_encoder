{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System Requirement ###\n",
    "# At least 32 GB RAM (Cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the necessary data for the experiments in playground.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"CIFAR10\"\n",
    "model =\"ViT-L-14\"## Parameters\n",
    "device = 'cuda'\n",
    "model_name = 'ViT-L-14' # 'ViT-H-14'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = 100\n",
    "tot_samples_per_class = 100\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"CIFAR10\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "path = './datasets/'\n",
    "\n",
    "if model_name == \"ViT-H-14\":\n",
    "    pretrained = \"laion2B-s32B-b79K\"\n",
    "elif model_name == \"ViT-L-14\":\n",
    "    pretrained = \"laion2B-s32B-b82K\"\n",
    "elif model_name == \"ViT-B-16\":\n",
    "    pretrained = \"laion2B-s34B-b88K\"\n",
    "elif model_name == \"ViT-B-32\":\n",
    "    pretrained = \"laion2B-s34B-b79K\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Using local files\n",
      "/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/factory.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "Model parameters: 427,616,513\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "Len of res: 24\n",
      "Files already downloaded and verified\n",
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "We are using a dataset containing 1000 images.\n",
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]tensor([6])\n",
      "  0%|                                        | 1/1000 [00:03<1:05:46,  3.95s/it]tensor([0])\n",
      "  0%|                                          | 2/1000 [00:05<39:59,  2.40s/it]tensor([3])\n",
      "  0%|▏                                         | 3/1000 [00:06<32:57,  1.98s/it]tensor([6])\n",
      "  0%|▏                                         | 4/1000 [00:07<27:58,  1.69s/it]tensor([5])\n",
      "  0%|▏                                         | 5/1000 [00:09<25:34,  1.54s/it]tensor([2])\n",
      "  1%|▎                                         | 6/1000 [00:10<24:23,  1.47s/it]tensor([6])\n",
      "  1%|▎                                         | 7/1000 [00:11<23:03,  1.39s/it]tensor([8])\n",
      "  1%|▎                                         | 8/1000 [00:13<22:04,  1.33s/it]tensor([9])\n",
      "  1%|▍                                         | 9/1000 [00:14<22:15,  1.35s/it]tensor([3])\n",
      "  1%|▍                                        | 10/1000 [00:15<21:34,  1.31s/it]tensor([9])\n",
      "  1%|▍                                        | 11/1000 [00:16<21:44,  1.32s/it]tensor([0])\n",
      "  1%|▍                                        | 12/1000 [00:18<21:48,  1.32s/it]tensor([0])\n",
      "  1%|▌                                        | 13/1000 [00:19<21:36,  1.31s/it]tensor([6])\n",
      "  1%|▌                                        | 14/1000 [00:21<22:37,  1.38s/it]tensor([7])\n",
      "  2%|▌                                        | 15/1000 [00:22<22:23,  1.36s/it]tensor([4])\n",
      "  2%|▋                                        | 16/1000 [00:23<22:07,  1.35s/it]tensor([5])\n",
      "  2%|▋                                        | 17/1000 [00:25<22:24,  1.37s/it]tensor([5])\n",
      "  2%|▋                                        | 18/1000 [00:26<21:46,  1.33s/it]tensor([8])\n",
      "  2%|▊                                        | 19/1000 [00:28<23:08,  1.42s/it]tensor([4])\n",
      "  2%|▊                                        | 20/1000 [00:29<22:44,  1.39s/it]tensor([3])\n",
      "  2%|▊                                        | 21/1000 [00:30<22:08,  1.36s/it]tensor([3])\n",
      "  2%|▉                                        | 22/1000 [00:31<21:53,  1.34s/it]tensor([8])\n",
      "  2%|▉                                        | 23/1000 [00:33<21:28,  1.32s/it]tensor([5])\n",
      "  2%|▉                                        | 24/1000 [00:34<21:10,  1.30s/it]tensor([1])\n",
      "  2%|█                                        | 25/1000 [00:35<21:35,  1.33s/it]tensor([1])\n",
      "  3%|█                                        | 26/1000 [00:37<21:43,  1.34s/it]tensor([5])\n",
      "  3%|█                                        | 27/1000 [00:38<21:28,  1.32s/it]tensor([7])\n",
      "  3%|█▏                                       | 28/1000 [00:39<21:25,  1.32s/it]tensor([6])\n",
      "  3%|█▏                                       | 29/1000 [00:41<21:17,  1.32s/it]tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the activations (hooks) for each head in each layer, of a given ViT-model.\n",
    "# Opt.: Setup a seed and use only a subset of the dataset (Imagenet)\n",
    "!python -m utils.scripts.compute_activation_values --dataset {dataset} --device {device} --model {model_name} --pretrained {pretrained} --seed {seed} --batch_size 1  --samples_per_class 100 --tot_samples_per_class 100 --cache_dir \"../cache/\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous outputs (i.e. all the activations) to derive the final clip embeddings for the image dataset.\n",
    "!python -m utils.scripts.compute_images_embedding --dataset {dataset} --model {model_name} --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the CLIP-embeddings of the classes' labels of a given dataset (i.e. allow zero-shot classifications)\n",
    "!python -m utils.scripts.compute_classes_embeddings --dataset {dataset} --device {device} --model {model_name} --pretrained {pretrained} --cache_dir \"../cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataset of text compute their CLIP's embeddings.\n",
    "!python -m utils.scripts.compute_text_embeddings --device {device} --model {model_name} --pretrained {pretrained} --cache_dir \"../cache/\" --data_path utils/text_descriptions/top_1500_nouns_5_sentences_imagenet_bias_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test text explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test an algorithm to explain the CLIP-embeddings with text (ours: svd_data_approx, their: text_span)\n",
    "!python -m utils.scripts.compute_text_explanations --device {device} --model {model_name} --algorithm svd_data_approx --dataset {dataset} --seed {seed} --num_of_last_layers 4 --text_descriptions top_1500_nouns_5_sentences_imagenet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the components with have generated\n",
    "import json\n",
    "\n",
    "# Read JSON lines\n",
    "with open(\"output_dir/CIFAR10_completeness_top_1500_nouns_5_sentences_imagenet_clean_ViT-B-32_algo_svd_data_approx_seed_0_max_text80.jsonl\", \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        entry = json.loads(line)  # Parse each line as a JSON object\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        texts = entry[\"embeddings_sort\"]\n",
    "\n",
    "        if entry[\"head\"] == -1:\n",
    "            print(entry.keys())\n",
    "            print(entry[\"accuracy\"])\n",
    "        \n",
    "        print(len(entry[\"s\"]))\n",
    "        print(f\"Layer: {layer}, Head: {head}\")\n",
    "        print(\"Texts:\")\n",
    "        for text in texts:\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go on playground to analyze the results\n",
    "models = [\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-H-14\"]\n",
    "algorithms = [\"svd_data_approx\", \"text_span\"]\n",
    "seed = 0\n",
    "for model in models:\n",
    "    for algorithm in algorithms:\n",
    "        with open(f\"output_dir/imagenet_completeness_top_1500_nouns_5_sentences_imagenet_clean_ViT-B-32_algo_text_span_seed_0.jsonl\", \"r\") as json_file:\n",
    "\n",
    "            for line in json_file:\n",
    "                entry = json.loads(line)  # Parse each line as a JSON object\n",
    "                layer = entry[\"layer\"]\n",
    "                head = entry[\"head\"]\n",
    "                texts = entry[\"embeddings_sort\"]\n",
    "\n",
    "                if entry[\"head\"] == -1:\n",
    "                    print(entry.keys())\n",
    "                    print(entry[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
