{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-24 16:25:05,399] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "import gc\n",
    "import requests\n",
    "import copy\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.nn.functional import mse_loss\n",
    "import numpy as np\n",
    "import einops\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.models.prs_hook import hook_prs_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def image_parser(image_file, sep=\",\"):\n",
    "    out = image_file.split(sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLava Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaConfig {\n",
      "  \"_name_or_path\": \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaLlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"freeze_mm_mlp_adapter\": false,\n",
      "  \"freeze_mm_vision_resampler\": false,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"image_aspect_ratio\": \"pad\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
      "  \"mm_resampler_type\": null,\n",
      "  \"mm_use_im_patch_token\": false,\n",
      "  \"mm_use_im_start_end\": false,\n",
      "  \"mm_vision_select_feature\": \"patch\",\n",
      "  \"mm_vision_select_layer\": -2,\n",
      "  \"mm_vision_tower\": \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/\",\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"tune_mm_mlp_adapter\": false,\n",
      "  \"tune_mm_vision_resampler\": false,\n",
      "  \"unfreeze_mm_vision_tower\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_mm_proj\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 303,507,456\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "### IMPORT: On Biomedcluster change .config under model_path to point towards correct vision_tower clip path\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_name),\n",
    "    device_map=\"cpu\",  # HERE\n",
    ")\n",
    "\n",
    "vision_enc = model.get_vision_tower()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in vision_enc.parameters()]):,}\")\n",
    "\n",
    "\n",
    "model1 = vision_enc\n",
    "print(model1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local files\n",
      "Model parameters: 304,293,888\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "Len of res: 24\n",
      "VisionTransformer(\n",
      "  (patchnorm_pre_ln): Identity()\n",
      "  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "  (patch_dropout): Identity()\n",
      "  (ln_pre): LayerNorm()\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (gelu): QuickGELUActivation()\n",
      "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CLIP model\n",
    "model_CLIP_name = 'ViT-L-14-336' \n",
    "pretrained = \"hf\"\n",
    "precision = \"fp16\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_CLIP, _, preprocess_clip = create_model_and_transforms(model_CLIP_name, pretrained=pretrained, precision=precision, cache_dir=\"../cache\")\n",
    "\n",
    "model_CLIP.eval()\n",
    "context_length = model_CLIP.context_length\n",
    "# Not needed anymore\n",
    "vocab_size = model_CLIP.vocab_size\n",
    "# tokenizer_CLIP = get_tokenizer(model_CLIP_name)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model_CLIP.visual.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model_CLIP.visual.transformer.resblocks))\n",
    "# Replace with the one we want to use\n",
    "model2 = model_CLIP.visual\n",
    "prs = hook_prs_logger(model_CLIP, device, spatial=True, vision_projection=False) # This attach hook to get the residual stream\n",
    "print(model_CLIP.visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "direct\n",
      "CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "VisionTransformer(\n",
      "  (patchnorm_pre_ln): Identity()\n",
      "  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "  (patch_dropout): Identity()\n",
      "  (ln_pre): LayerNorm()\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (gelu): QuickGELUActivation()\n",
      "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm()\n",
      ")\n",
      "=== High-Level Activations Comparison ===\n",
      "Model1 patch_embedding: shape torch.Size([1, 1024, 24, 24]), norm 30960.0\n",
      "Model2 conv1: shape torch.Size([1, 1024, 24, 24]), norm 30960.0\n",
      "Difference: 0.0\n",
      "\n",
      "Model1 pre_layrnorm: shape torch.Size([1, 577, 1024]), norm 433.0\n",
      "Model2 ln_pre: shape torch.Size([1, 577, 1024]), norm 433.0\n",
      "Difference: 0.0\n",
      "\n",
      "Model1 postlayernorm: shape torch.Size([1, 1024]), norm 31.71875\n",
      "Model2 ln_post: shape torch.Size([1, 1024]), norm 31.71875\n",
      "Difference: 0.0\n",
      "\n",
      "=== Internal Blocks Comparison ===\n",
      "--- Block 0 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 1 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 2 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 3 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 4 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 5 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 6 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 7 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 8 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 9 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 10 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 11 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 12 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 13 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 14 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 15 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 16 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 17 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 18 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 19 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 20 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 21 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 22 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "--- Block 23 ---\n",
      "LayerNorm1 difference: 0.0\n",
      "Attention Out Projection difference: 0.0\n",
      "LayerNorm2 difference: 0.0\n",
      "MLP FC1 difference: 0.0\n",
      "MLP Activation difference: 0.0\n",
      "MLP FC2 difference: 0.0\n",
      "\n",
      "=== Final Output Comparison ===\n",
      "Model1 output: shape torch.Size([1, 576, 1024]), norm 617.5\n",
      "Model2 output: shape torch.Size([1, 768]), norm 17.796875\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (576) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 240\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel2 output: shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_model2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, norm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mnorm(output_model2)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Here, we compare model1 output to the transpose of model2 output; adjust if needed.\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m final_diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43moutput_model1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_model2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_model1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal output difference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_diff\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (576) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume model1 and model2 are defined, loaded, and moved to the proper device.\n",
    "# For example:\n",
    "#   model1 = ...  # a CLIP encoder-based model with: vision_tower.vision_model.encoder.layers (CLIPEncoderLayer)\n",
    "#   model2 = ...  # a VisionTransformer model with: transformer.resblocks (ResidualAttentionBlock)\n",
    "\n",
    "# Set both models to evaluation mode.\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dictionaries to store activations (outputs) from each model.\n",
    "activations_model1 = {}\n",
    "activations_model2 = {}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Hook function to capture module outputs.\n",
    "def get_activation(name, activation_dict):\n",
    "    def hook(module, input, output):\n",
    "        # If the module returns a tuple, take the first element.\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        activation_dict[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Register high-level hooks for Model1 (CLIPEncoder-based)\n",
    "model1_patch_embed = model1.vision_tower.vision_model.embeddings.patch_embedding\n",
    "model1_patch_embed.register_forward_hook(get_activation(\"patch_embedding\", activations_model1))\n",
    "\n",
    "model1_pre_ln = model1.vision_tower.vision_model.pre_layrnorm\n",
    "model1_pre_ln.register_forward_hook(get_activation(\"pre_layrnorm\", activations_model1))\n",
    "\n",
    "encoder_layers_model1 = model1.vision_tower.vision_model.encoder.layers\n",
    "for i, layer in enumerate(encoder_layers_model1):\n",
    "    layer.register_forward_hook(get_activation(f\"encoder_layer_{i}\", activations_model1))\n",
    "    \n",
    "model1_post_ln = model1.vision_tower.vision_model.post_layernorm\n",
    "model1_post_ln.register_forward_hook(get_activation(\"postlayernorm\", activations_model1))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Register high-level hooks for Model2 (ResidualAttentionBlock-based)\n",
    "model2_patch_embed = model2.conv1\n",
    "model2_patch_embed.register_forward_hook(get_activation(\"conv1\", activations_model2))\n",
    "\n",
    "model2_pre_ln = model2.ln_pre\n",
    "model2_pre_ln.register_forward_hook(get_activation(\"ln_pre\", activations_model2))\n",
    "\n",
    "resblocks_model2 = model2.transformer.resblocks\n",
    "for i, block in enumerate(resblocks_model2):\n",
    "    block.register_forward_hook(get_activation(f\"resblock_{i}\", activations_model2))\n",
    "    \n",
    "model2_post_ln = model2.ln_post\n",
    "model2_post_ln.register_forward_hook(get_activation(\"ln_post\", activations_model2))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Register internal hooks for each transformer/encoder block.\n",
    "# For Model2 (ResidualAttentionBlock): capture internal submodule outputs.\n",
    "for i, block in enumerate(resblocks_model2):\n",
    "    # First layer normalization.\n",
    "    block.ln_1.register_forward_hook(get_activation(f\"resblock_{i}_ln_1\", activations_model2))\n",
    "    \n",
    "    # For the attention output, capture the output of the attention module.\n",
    "    block.attn.register_forward_hook(get_activation(f\"resblock_{i}_attn_out\", activations_model2))\n",
    "\n",
    "    # Second layer normalization.\n",
    "    block.ln_2.register_forward_hook(get_activation(f\"resblock_{i}_ln_2\", activations_model2))\n",
    "    \n",
    "    # MLP internal components.\n",
    "    block.mlp.c_fc.register_forward_hook(get_activation(f\"resblock_{i}_mlp_fc1\", activations_model2))\n",
    "    block.mlp.gelu.register_forward_hook(get_activation(f\"resblock_{i}_mlp_gelu\", activations_model2))\n",
    "    block.mlp.c_proj.register_forward_hook(get_activation(f\"resblock_{i}_mlp_fc2\", activations_model2))\n",
    "\n",
    "# For Model1 (CLIPEncoderLayer): capture internal submodule outputs.\n",
    "for i, layer in enumerate(encoder_layers_model1):\n",
    "    layer.layer_norm1.register_forward_hook(get_activation(f\"encoder_layer_{i}_ln1\", activations_model1))\n",
    "    layer.self_attn.out_proj.register_forward_hook(get_activation(f\"encoder_layer_{i}_attn_out\", activations_model1))\n",
    "    layer.layer_norm2.register_forward_hook(get_activation(f\"encoder_layer_{i}_ln2\", activations_model1))\n",
    "    layer.mlp.fc1.register_forward_hook(get_activation(f\"encoder_layer_{i}_mlp_fc1\", activations_model1))\n",
    "    layer.mlp.activation_fn.register_forward_hook(get_activation(f\"encoder_layer_{i}_mlp_act\", activations_model1))\n",
    "    layer.mlp.fc2.register_forward_hook(get_activation(f\"encoder_layer_{i}_mlp_fc2\", activations_model1))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Prepare an input for the forward pass.\n",
    "# Here we create a dummy input; replace with your actual image processing pipeline.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = 100*torch.randn(1, 3, 336, 336, device=device, dtype=torch.float16)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Run a forward pass for both models.\n",
    "model1.to(\"cuda\")\n",
    "model1.to\n",
    "output_model1 = model1(x)\n",
    "model1.to(\"cpu\")\n",
    "model2.to(\"cuda\")\n",
    "output_model2 = model2(x)\n",
    "model2.to(\"cpu\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Compare high-level activations.\n",
    "layer_mapping = {\n",
    "    \"patch_embedding\": \"conv1\",    # Model1 patch embedding vs. Model2 conv1\n",
    "    \"pre_layrnorm\": \"ln_pre\",\n",
    "    \"postlayernorm\": \"ln_post\"\n",
    "}\n",
    "\n",
    "print(model1)\n",
    "print(model2)\n",
    "print(\"=== High-Level Activations Comparison ===\")\n",
    "for name1, name2 in layer_mapping.items():\n",
    "    act1 = activations_model1.get(name1)\n",
    "    act2 = activations_model2.get(name2)\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2)\n",
    "        print(f\"Model1 {name1}: shape {act1.shape}, norm {torch.norm(act1)}\")\n",
    "        print(f\"Model2 {name2}: shape {act2.shape}, norm {torch.norm(act2)}\")\n",
    "        print(f\"Difference: {diff.item()}\\n\")\n",
    "    else:\n",
    "        print(f\"Missing activation for {name1} or {name2}\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Compare internal submodule outputs for each block.\n",
    "print(\"=== Internal Blocks Comparison ===\")\n",
    "for i in range(24):\n",
    "    print(f\"--- Block {i} ---\")\n",
    "    \n",
    "    # Compare first layer normalization (ln_1 vs. layer_norm1)\n",
    "    act1 = activations_model1.get(f\"encoder_layer_{i}_ln1\")\n",
    "    act2 = activations_model2.get(f\"resblock_{i}_ln_1\")\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2.to(dtype=torch.float32))\n",
    "        print(f\"LayerNorm1 difference: {diff.item()}\")\n",
    "    else:\n",
    "        print(\"Missing ln1 activation\")\n",
    "    \n",
    "    # Compare attention's output projection.\n",
    "    act1 = activations_model1.get(f\"encoder_layer_{i}_attn_out\")\n",
    "    act2 = activations_model2.get(f\"resblock_{i}_attn_out\")\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2.to(dtype=torch.float32))\n",
    "        print(f\"Attention Out Projection difference: {diff.item()}\")\n",
    "    else:\n",
    "        print(\"Missing attention out activation\")\n",
    "    \n",
    "    # Compare second layer normalization.\n",
    "    act1 = activations_model1.get(f\"encoder_layer_{i}_ln2\")\n",
    "    act2 = activations_model2.get(f\"resblock_{i}_ln_2\")\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2.to(dtype=torch.float32))\n",
    "        print(f\"LayerNorm2 difference: {diff.item()}\")\n",
    "    else:\n",
    "        print(\"Missing ln2 activation\")\n",
    "    \n",
    "    # Compare MLP first linear layer output.\n",
    "    act1 = activations_model1.get(f\"encoder_layer_{i}_mlp_fc1\")\n",
    "    act2 = activations_model2.get(f\"resblock_{i}_mlp_fc1\")\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2.to(dtype=torch.float32))\n",
    "        print(f\"MLP FC1 difference: {diff.item()}\")\n",
    "    else:\n",
    "        print(\"Missing MLP fc1 activation\")\n",
    "    \n",
    "    # Compare MLP activation (GELU/QuickGELU).\n",
    "    act1 = activations_model1.get(f\"encoder_layer_{i}_mlp_act\")\n",
    "    act2 = activations_model2.get(f\"resblock_{i}_mlp_gelu\")\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2.to(dtype=torch.float32))\n",
    "        print(f\"MLP Activation difference: {diff.item()}\")\n",
    "    else:\n",
    "        print(\"Missing MLP activation (GELU)\")\n",
    "    \n",
    "    # Compare MLP second linear layer output.\n",
    "    act1 = activations_model1.get(f\"encoder_layer_{i}_mlp_fc2\")\n",
    "    act2 = activations_model2.get(f\"resblock_{i}_mlp_fc2\")\n",
    "    if act1 is not None and act2 is not None:\n",
    "        diff = torch.norm(act1 - act2.to(dtype=torch.float32))\n",
    "        print(f\"MLP FC2 difference: {diff.item()}\\n\")\n",
    "    else:\n",
    "        print(\"Missing MLP fc2 activation\\n\")\n",
    "\n",
    "\n",
    "\"\"\" # ----------------------------------------------------------------------------\n",
    "# Now, compare parameters of the corresponding layers.\n",
    "def compare_module_parameters(module1, module2, tol=1e-5):\n",
    "    params1 = dict(module1.named_parameters())\n",
    "    params2 = dict(module2.named_parameters())\n",
    "    for name, p1 in params1.items():\n",
    "        p2 = params2.get(name)\n",
    "        if p2 is None:\n",
    "            print(f\"Parameter '{name}' not found in second module.\")\n",
    "        else:\n",
    "            if torch.allclose(p1.detach().cpu(), p2.detach().cpu(), atol=tol):\n",
    "                print(f\"Parameter '{name}' matches.\")\n",
    "            else:\n",
    "                print(f\"Parameter '{name}' differs.\")\n",
    "\n",
    "print(\"=== Parameters Comparison ===\")\n",
    "# Compare high-level layers.\n",
    "print(\"Comparing patch embedding parameters:\")\n",
    "compare_module_parameters(model1.vision_tower.vision_model.embeddings.patch_embedding, model2.conv1)\n",
    "\n",
    "print(\"\\nComparing pre-layer norm parameters:\")\n",
    "compare_module_parameters(model1.vision_tower.vision_model.pre_layrnorm, model2.ln_pre)\n",
    "\n",
    "print(\"\\nComparing post-layer norm parameters:\")\n",
    "compare_module_parameters(model1.vision_tower.vision_model.post_layernorm, model2.ln_post)\n",
    "\n",
    "# Compare parameters for each internal block.\n",
    "for i in range(24):\n",
    "    print(f\"\\n--- Block {i} Parameters Comparison ---\")\n",
    "    \n",
    "    # Compare first layer normalization parameters.\n",
    "    print(\"LayerNorm1 parameters:\")\n",
    "    compare_module_parameters(encoder_layers_model1[i].layer_norm1, resblocks_model2[i].ln_1)\n",
    "    \n",
    "    # Compare attention output projection parameters.\n",
    "    print(\"Attention Out Projection parameters:\")\n",
    "    compare_module_parameters(encoder_layers_model1[i].self_attn.out_proj, resblocks_model2[i].attn.out_proj)\n",
    "    \n",
    "    # Compare second layer normalization parameters.\n",
    "    print(\"LayerNorm2 parameters:\")\n",
    "    compare_module_parameters(encoder_layers_model1[i].layer_norm2, resblocks_model2[i].ln_2)\n",
    "    \n",
    "    # Compare MLP first linear layer parameters.\n",
    "    print(\"MLP FC1 parameters:\")\n",
    "    compare_module_parameters(encoder_layers_model1[i].mlp.fc1, resblocks_model2[i].mlp.c_fc)\n",
    "    \n",
    "    # Compare MLP second linear layer parameters.\n",
    "    print(\"MLP FC2 parameters:\")\n",
    "    compare_module_parameters(encoder_layers_model1[i].mlp.fc2, resblocks_model2[i].mlp.c_proj)\n",
    " \"\"\"\n",
    "# ----------------------------------------------------------------------------\n",
    "# Compare the final outputs of both models.\n",
    "print(\"=== Final Output Comparison ===\")\n",
    "print(f\"Model1 output: shape {output_model1.shape}, norm {torch.norm(output_model1)}\")\n",
    "print(f\"Model2 output: shape {output_model2.shape}, norm {torch.norm(output_model2)}\")\n",
    "# Here, we compare model1 output to the transpose of model2 output; adjust if needed.\n",
    "final_diff = torch.norm(output_model1 - output_model2.T.to(dtype=output_model1.dtype))\n",
    "print(f\"Final output difference: {final_diff.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Half but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# For post-layernorm, assume a transformer input shape, e.g., (1, 197, 768); adjust if needed.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m dummy_transformer_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m577\u001b[39m, \u001b[38;5;241m1024\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mcompare_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_layernorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_post\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdummy_transformer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPost-LayerNorm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Internal Transformer/Encoder Block Comparisons\u001b[39;00m\n\u001b[1;32m     50\u001b[0m encoder_layers_model1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mvision_model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayers\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mcompare_modules\u001b[0;34m(module1, module2, dummy_input, module_label)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Evaluate module1 on CUDA and move the output back to CPU.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     module1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodule1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m out1\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     24\u001b[0m     module1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Half but found Float"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume model1 and model2 are defined, loaded, and set to evaluation mode.\n",
    "# For example:\n",
    "#   model1 = ...  # a CLIP encoder-based model with: vision_tower.vision_model.encoder.layers (CLIPEncoderLayer)\n",
    "#   model2 = ...  # a VisionTransformer model with: transformer.resblocks (ResidualAttentionBlock)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# Set the device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Define a helper function to compare outputs of two modules on a fixed dummy input.\n",
    "# For each module, we move it to CUDA, run the dummy input, then move it back to CPU.\n",
    "def compare_modules(module1, module2, dummy_input, module_label=\"\"):\n",
    "    with torch.no_grad():\n",
    "        # Evaluate module1 on CUDA and move the output back to CPU.\n",
    "        module1.to(device)\n",
    "        out1 = module1(dummy_input)[0]\n",
    "        out1 = out1.cpu()\n",
    "        module1.to(\"cpu\")\n",
    "        \n",
    "        # Evaluate module2 on CUDA and move the output back to CPU.\n",
    "        module2.to(device)\n",
    "        out2 = module2(dummy_input)\n",
    "        out2 = out2.cpu()\n",
    "        module2.to(\"cpu\")\n",
    "        \n",
    "    diff = torch.norm(out1 - out2.to(dtype=out1.dtype))\n",
    "    print(f\"{module_label} difference: {diff.item():.6f}\")\n",
    "    print(f\"Module1 {module_label} output norm: {torch.norm(out1).item():.6f}\")\n",
    "    print(f\"Module2 {module_label} output norm: {torch.norm(out2).item():.6f}\\n\")\n",
    "    return out1, out2, diff\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# For post-layernorm, assume a transformer input shape, e.g., (1, 197, 768); adjust if needed.\n",
    "dummy_transformer_input = 100 * torch.randn(1, 577, 1024, device=device, dtype=torch.float16)\n",
    "\n",
    "compare_modules(model1.vision_tower.vision_model.post_layernorm,\n",
    "                model2.ln_post,\n",
    "                dummy_transformer_input,\n",
    "                \"Post-LayerNorm\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Internal Transformer/Encoder Block Comparisons\n",
    "encoder_layers_model1 = model1.vision_tower.vision_model.encoder.layers\n",
    "resblocks_model2 = model2.transformer.resblocks\n",
    "\n",
    "# Create a dummy input for transformer blocks (adjust shape as required).\n",
    "model1.to(device)\n",
    "dummy_block_input = 100 * torch.randn(1, 577, 1024, device=device, dtype=torch.float16)\n",
    "model1.to(\"cpu\")\n",
    "\n",
    "for i in range(len(encoder_layers_model1)):\n",
    "    print(f\"=== Comparing Block {i} as a whole ===\")\n",
    "\n",
    "    # Compare attention's output projection.\n",
    "    compare_modules(encoder_layers_model1[i].self_attn,\n",
    "                    resblocks_model2[i].attn,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} Attn Out\")\n",
    "    print(f\"--- Block {i} internal modules ---\")\n",
    "    # Compare first layer normalization.\n",
    "    compare_modules(encoder_layers_model1[i].layer_norm1,\n",
    "                    resblocks_model2[i].ln_1,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} LN1\")\n",
    "    \n",
    "    # Compare attention's output projection.\n",
    "    compare_modules(encoder_layers_model1[i].self_attn.out_proj,\n",
    "                    resblocks_model2[i].attn.out_proj,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} Attn Out\")\n",
    "    \n",
    "    # Compare second layer normalization.\n",
    "    compare_modules(encoder_layers_model1[i].layer_norm2,\n",
    "                    resblocks_model2[i].ln_2,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} LN2\")\n",
    "    \n",
    "    # Compare MLP first linear layer.\n",
    "    compare_modules(encoder_layers_model1[i].mlp.fc1,\n",
    "                    resblocks_model2[i].mlp.c_fc,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} MLP FC1\")\n",
    "    \n",
    "    # Compare MLP activation function.\n",
    "    compare_modules(encoder_layers_model1[i].mlp.activation_fn,\n",
    "                    resblocks_model2[i].mlp.gelu,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} MLP Activation\")\n",
    "    \n",
    "    # Compare MLP second linear layer.\n",
    "    compare_modules(encoder_layers_model1[i].mlp.fc2,\n",
    "                    resblocks_model2[i].mlp.c_proj,\n",
    "                    dummy_block_input,\n",
    "                    f\"Block {i} MLP FC2\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Final Output Comparison Using a Fixed Dummy Image.\n",
    "with torch.no_grad():\n",
    "    # Evaluate model1 on CUDA then move it back to CPU.\n",
    "    model1.to(device)\n",
    "    final_out1 = model1(dummy_image)\n",
    "    final_out1 = final_out1.cpu()\n",
    "    model1.to(\"cpu\")\n",
    "    \n",
    "    # Evaluate model2 on CUDA then move it back to CPU.\n",
    "    model2.to(device)\n",
    "    final_out2 = model2(dummy_image)\n",
    "    final_out2 = final_out2.cpu()\n",
    "    model2.to(\"cpu\")\n",
    "\n",
    "print(\"=== Final Output Comparison ===\")\n",
    "print(f\"Model1 final output: shape {final_out1.shape}, norm {torch.norm(final_out1).item():.6f}\")\n",
    "print(f\"Model2 final output: shape {final_out2.shape}, norm {torch.norm(final_out2).item():.6f}\")\n",
    "final_diff = torch.norm(final_out1 - final_out2.T.to(dtype=final_out1.dtype))\n",
    "print(f\"Final output difference: {final_diff.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here play around with LLava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "Describe the image focusing on main subjects. ignore background ASSISTANT:\n",
      "dict_keys([])\n",
      "dict_keys([])\n",
      "head\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'peinr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     91\u001b[0m     model_CLIP\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     spatial_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_CLIP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimages_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(spatial_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     98\u001b[0m     model_CLIP\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/model.py:263\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image, normalize, attn_method)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, attn_method: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 263\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/transformer.py:933\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x, attn_method)\u001b[0m\n\u001b[1;32m    931\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mln_pre_post\u001b[39m\u001b[38;5;124m\"\u001b[39m, ret\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre(x))\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# x = x.permute(1, 0, 2)  # NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# x = x.permute(1, 0, 2)  # LND -> NLD\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/transformer.py:788\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, attn_method)\u001b[0m\n\u001b[1;32m    786\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint(r, x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 788\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook\u001b[38;5;241m.\u001b[39mfinalize()\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/transformer.py:724\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, attn_mask, attn_method)\u001b[0m\n\u001b[1;32m    722\u001b[0m q_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m, ret\u001b[38;5;241m=\u001b[39mq_x)\n\u001b[1;32m    723\u001b[0m after_ln1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(q_x)\n\u001b[0;32m--> 724\u001b[0m after_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mafter_ln1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_method\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m after_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m, ret\u001b[38;5;241m=\u001b[39mafter_attn)\n\u001b[1;32m    728\u001b[0m x \u001b[38;5;241m=\u001b[39m q_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_1(after_attn)\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/transformer.py:714\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, attn_mask, method)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattention\u001b[39m(\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    709\u001b[0m     q_x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    710\u001b[0m     attn_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    711\u001b[0m     method: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    712\u001b[0m ):\n\u001b[1;32m    713\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mto(q_x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/transformer.py:664\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, x, attn_mask, method)\u001b[0m\n\u001b[1;32m    662\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_qkv(x, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 664\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_no_spatial\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    666\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_per_head_no_spatial(x, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask)\n",
      "File \u001b[0;32m/cluster/work/vogtlab/Group/vstrozzi/working-MT2024-active/utils/models/transformer.py:415\u001b[0m, in \u001b[0;36mMultiheadAttention.forward_clip\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_output should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(B\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Reshape and combine heads.\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m \u001b[43mpeinr\u001b[49m(attn_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    416\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    417\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, tgt_len, C)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peinr' is not defined"
     ]
    }
   ],
   "source": [
    "# Layer where to extract infos on patches\n",
    "select_layer = -2\n",
    "# Params\n",
    "prompt = \"Describe the image focusing on main subjects. ignore background\"\n",
    "image_file = \"images/catdog.png\"\n",
    "max_new_tokens = 512\n",
    "num_beams = 1 # numer of path of decision, less faster\n",
    "sep =  \",\"\n",
    "temperature = 0 # 0 lowest, det\n",
    "top_p = None\n",
    "images_embeds = True # If provided image embeds\n",
    "\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "# Making prompt in correct format\n",
    "if IMAGE_PLACEHOLDER in prompt:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "    else:\n",
    "        prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = image_token_se + \"\\n\" + prompt\n",
    "    else:\n",
    "        prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "# Derive necessary conv\n",
    "if \"llama-2\" in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"mistral\" in model_name.lower():\n",
    "    conv_mode = \"mistral_instruct\"\n",
    "elif \"v1.6-34b\" in model_name.lower():\n",
    "    conv_mode = \"chatml_direct\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "if conv_mode is not None and conv_mode != conv_mode:\n",
    "    print(\n",
    "        \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "            conv_mode, conv_mode, conv_mode\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    conv_mode = conv_mode\n",
    "\n",
    "# Load conversation mode standard template \n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "print(prompt)\n",
    "\n",
    "# Load images from online or local\n",
    "image_files = image_parser(image_file, sep)\n",
    "images = load_images(image_files)\n",
    "image_sizes = [x.size for x in images]\n",
    "# Convert images to format b, 3, h, w (h = w) with resizing or padding\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "\n",
    "model_CLIP.eval()\n",
    "vision_enc.eval()\n",
    "# Tokenize prompt\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "\n",
    "# Use CLIP Model\n",
    "if images_embeds:\n",
    "    model.to(\"cuda\")\n",
    "    output_pt = vision_enc(images_tensor.to(device))\n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    #spatial_features = torch.randn((1, 576, 1024))# if want to manually edit features, we want b, nr_spat (576), d (1024)\n",
    "    # Need to handle some memory movement :/\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "   # prs.reinit()  # Reinitialize any hooks if required\n",
    "    with torch.no_grad():\n",
    "        model_CLIP.to(\"cuda\")\n",
    "        spatial_features = model_CLIP.encode_image(\n",
    "                images_tensor.to(device), \n",
    "                attn_method='head',\n",
    "                normalize=False\n",
    "            )\n",
    "        print(spatial_features.shape)\n",
    "        model_CLIP.to(\"cpu\")\n",
    "\n",
    "        attentions, mlps = prs.finalize(spatial_features)  # attentions: [1, 12, 197, 16, 512], [b, l, n, h, d], mlps: [1, 13, 512], [b, l + 1, d]\n",
    "        attentions = einops.rearrange(attentions, \"b l n h d -> b l h n d\")\n",
    "        # Compute intermediate features from select_layer\n",
    "        print(attentions.shape)\n",
    "        print(mlps.shape)\n",
    "        for k in range(1, 24 - 2):\n",
    "            if k == 1:\n",
    "                spatial_features = attentions[:, k, :, 1:, :].sum(1) + mlps[:, k, 1:, :]\n",
    "            else:\n",
    "                spatial_features += spatial_features + attentions[:, k, :, 1:, :].sum(1) + mlps[:, k, 1:, :]\n",
    "\n",
    "            print(spatial_features.norm())\n",
    "        #spatial_features = (attentions[:, :-2, :, 1:, :].sum(1).sum(1) + mlps[:, :-1, 1:, :].sum(1))\n",
    "        print(spatial_features.shape)\n",
    "        print(spatial_features.norm())\n",
    "        print(\"MSE Loss:\", mse_loss(output_pt.to(\"cpu\"), spatial_features.to(\"cpu\")).item())\n",
    "\n",
    "        print(spatial_features.norm())\n",
    "        print(attentions.shape, mlps.shape)\n",
    "        \n",
    "    # Move model back to gpu\n",
    "    images_tensor = ct2 = output_pt#activations_model2.get(f\"resblock_{23}\")[:, 1:, :].to(device)\n",
    "    print(images_tensor.shape)\n",
    "\n",
    "\n",
    "# Generate an answer by using full model LLava\n",
    "model.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        images_embeds = images_embeds # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "\n",
    "    )\n",
    "\n",
    "# Print the output\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
