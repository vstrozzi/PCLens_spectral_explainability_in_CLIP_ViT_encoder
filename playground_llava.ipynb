{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import requests\n",
    "import copy\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.nn.functional import mse_loss\n",
    "import numpy as np\n",
    "import einops\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.models.prs_hook import hook_prs_logger\n",
    "from utils.scripts.utils_llava import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cuda'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = 10\n",
    "tot_samples_per_class = 50\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"imagenet\"\n",
    "cache_dir = \"../cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def image_parser(image_file, sep=\",\"):\n",
    "    out = image_file.split(sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLava Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT: On Biomedcluster change .config under model_path to point towards correct vision_tower clip path\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_name),\n",
    ")\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP model\n",
    "model_CLIP_name = 'ViT-L-14-336' \n",
    "pretrained = \"openai\"\n",
    "precision = \"fp16\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_CLIP, _, preprocess_clip = create_model_and_transforms(model_CLIP_name, pretrained=pretrained, precision=precision, cache_dir=\"../cache\")\n",
    "\n",
    "model_CLIP.eval()\n",
    "context_length = model_CLIP.context_length\n",
    "# Not needed anymore\n",
    "vocab_size = model_CLIP.vocab_size\n",
    "tokenizer_CLIP = get_tokenizer(model_CLIP_name)\n",
    "\n",
    "print(model_CLIP)\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model_CLIP.visual.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model_CLIP.visual.transformer.resblocks))\n",
    "# Hook necessary to have: no projection on shared space, no spatial tokens in output (i.e. contributuon of attention to tokens), and hidden outputs of all tokens\n",
    "prs = hook_prs_logger(model_CLIP, device, spatial=False, vision_projection=False, full_output=True) # This attach hook to get the residual stream\n",
    "\n",
    "final_embeddings_images = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_embeddings_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\")).to(device)\n",
    "final_embeddings_texts = torch.tensor(np.load(f\"output_dir/{dataset_text_name}_{model_CLIP_name}.npy\", mmap_mode=\"r\")).to(device)\n",
    "\n",
    "attns_hid_mean = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attns_mean_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # [l, n, h, d], attention values\n",
    "mlps_hid_mean = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlps_mean_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [l + 1, n, d], mlp values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here play around with LLava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ablate_head(attentions, mlps, select_layer, layers=None, heads=None, \n",
    "                     attentions_mean_abl=None, mlps_mean_abl=None, mean_ablate_mlps=False):\n",
    "\n",
    "    # Clone the input tensors to prevent in-place modifications from affecting future calls\n",
    "    attentions = attentions.clone()\n",
    "    mlps = mlps.clone()\n",
    "\n",
    "    # Compute the mean value over the selected layers, all heads, and tokens (excluding token index 0) \n",
    "    # or use predefined mean\n",
    "    attentions_mean_abl = (attentions[:, :(select_layer + 1), :, :, :].mean(dim=(0, 1, 2, 3)) \n",
    "                           if attentions_mean_abl is None \n",
    "                           else attentions_mean_abl.unsqueeze(0))\n",
    "    mlps_mean_abl = (mlps[:, :(select_layer + 1), :, :].mean(dim=(0, 1, 2)) \n",
    "                     if mlps_mean_abl is None \n",
    "                     else mlps_mean_abl.unsqueeze(0))\n",
    "\n",
    "    # Replace the attention values for specified layers and heads with the computed mean ablation value\n",
    "    if heads is not None and layers is not None:\n",
    "        for layer, head in zip(layers, heads):        \n",
    "            attentions[:, layer, :, head, :] = (attentions_mean_abl \n",
    "                if len(attentions_mean_abl.shape) < 2 \n",
    "                else attentions_mean_abl[:, layer, :, head, :])\n",
    "            # If required, mean ablate mlps\n",
    "            if mean_ablate_mlps:\n",
    "                mlps[:, layer, :, :] = (mlps_mean_abl[:, layer, :, :] \n",
    "                    if len(mlps_mean_abl.shape) >= 2 \n",
    "                    else mlps_mean_abl)\n",
    "\n",
    "    # Aggregate the modified attention tensor by summing over layers and heads,\n",
    "    # and add the corresponding summed MLP outputs \n",
    "    return attentions, mlps\n",
    "\n",
    "def llava_pred(attentions, mlps, select_layer):\n",
    "\n",
    "    return (attentions[:, :(select_layer + 1), :, :, :].sum(1).sum(2) +\n",
    "            mlps[:, :(select_layer + 1), :, :].sum(1))\n",
    "\n",
    "# Project \n",
    "def remove(a, b):\n",
    "    a = a.squeeze()\n",
    "    b = b.squeeze()\n",
    "    return (a - (torch.dot(a, b) / torch.dot(b, b)) * b).unsqueeze(0)\n",
    "\n",
    "def remove_patches(p, b):\n",
    "    p = p.squeeze()\n",
    "    for i in range(p.shape[0]):\n",
    "        p[i:i+1, :] = remove(p[i:i+1, :], b)\n",
    "\n",
    "    return p.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_infer(prompt, pil_image, images_embeds=False, mean_ablate=False, up_to_layer_ablate = 10, mean_ablate_mlps=False, attentions_mean_abl=None, mlps_mean_abl=None,): # If provided image embeds\n",
    "    # Layer where to extract infos on patches\n",
    "    select_layer = -2\n",
    "    max_new_tokens = 512\n",
    "    num_beams = 1 # numer of path of decision, less faster\n",
    "    sep =  \",\"\n",
    "    temperature = 0 # 0 lowest, det\n",
    "    top_p = None\n",
    "\n",
    "    ## Tokenization prompt\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    # Making prompt in correct format\n",
    "    if IMAGE_PLACEHOLDER in prompt:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "        else:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = image_token_se + \"\\n\" + prompt\n",
    "        else:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "    ## Convert model\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    if conv_mode is not None and conv_mode != conv_mode:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, conv_mode, conv_mode\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        conv_mode = conv_mode\n",
    "\n",
    "    ## Load conversation mode standard template \n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    # print(prompt)\n",
    "\n",
    "    ## Load images from online or local\n",
    "    \"\"\" image_files = image_parser(image_file, sep)\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images] \"\"\"\n",
    "    \n",
    "    image_sizes = [img.size]\n",
    "    \n",
    "    images_tensor = process_images(\n",
    "        [pil_image],\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "\n",
    "    ## Tokenize prompt\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    ## Use CLIP Model\n",
    "    if images_embeds:\n",
    "        prs.reinit()\n",
    "        model_CLIP.eval()\n",
    "        with torch.no_grad():\n",
    "            model.to(\"cpu\")\n",
    "            model_CLIP.to(\"cuda\")\n",
    "            # Get output as we want it\n",
    "            spatial_features = model_CLIP.encode_image(\n",
    "                    images_tensor.to(device), \n",
    "                    attn_method='head_no_spatial',\n",
    "                    normalize=False\n",
    "                )\n",
    "\n",
    "            model_CLIP.to(\"cpu\")\n",
    "\n",
    "            # Move directions\n",
    "            attentions, mlps = prs.finalize(spatial_features)  # attentions: [b, l, n, h, d], mlps: [b, l + 1, n, d]\n",
    "\n",
    "            # Compute spatial features required by our layer \n",
    "            if mean_ablate:\n",
    "                attentions, mlps = mean_ablate_head(attentions, mlps, select_layer,\n",
    "                layers = [y for x in range(0, 16) for y in range(0, up_to_layer_ablate)],\n",
    "                heads = [x for x in range(0, 16) for y in range(0, up_to_layer_ablate)],\n",
    "                attentions_mean_abl = attentions_mean_abl,\n",
    "                mlps_mean_abl = mlps_mean_abl,\n",
    "                mean_ablate_mlps = mean_ablate_mlps)\n",
    "            \n",
    "            images_tensor = llava_pred(attentions, mlps[:, :(select_layer + 1), :, :], select_layer)\n",
    "            \n",
    "        # images_tensor = remove_patches(images_tensor, invert_topic_emb)\n",
    "        # print(images_tensor.shape)\n",
    "        # Swap some features position \n",
    "        # images_tensor[:, :192, :] = images_tensor[:, 384:, :] \n",
    "        # images_tensor[:, 192:384, :] = images_tensor[:, 384:, :] \n",
    "        # images_tensor[:, 384:, :] = images_tensor[:, 384:, :] \n",
    "\n",
    "\n",
    "    ## Generate an answer by using full model LLava\n",
    "    model.to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor[:, 1:], # Skip CLS\n",
    "            image_sizes=image_sizes,\n",
    "            #do_sample= True if temperature > 0 else False,\n",
    "            #temperature=temperature,\n",
    "            #top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            images_embeds = images_embeds # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "\n",
    "        )\n",
    "\n",
    "    ## Print the output\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return outputs, images_tensor\n",
    "\n",
    "\n",
    "# Params\n",
    "prompt = \"You are a vision-language expert. Analyze the given image and classify it into one of the following categories:  \\\n",
    "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']. Answer with only the most appropriate category.\"\n",
    "image_file = \"images/catdog.png\"\n",
    "\n",
    "## Visualize image\n",
    "img = Image.open(image_file)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()\n",
    "\n",
    "llava_infer(prompt, img, images_embeds = True, mean_ablate = True, up_to_layer_ablate = 20, attentions_mean_abl=attns_hid_mean, mlps_mean_abl=mlps_hid_mean,  mean_ablate_mlps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image features a brown and white dog and a brown and black cat sitting together on a carpeted floor. They appear to be relaxed and comfortable in each other's company. The dog is positioned on the left side of the cat, with both animals facing the same direction.\n",
    "# In the background, there is a bookshelf with several books on it, adding a cozy and lived-in atmosphere to the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test zero-shot accuracy VLM on Cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_formatted(prompt, model, conv_templates):\n",
    "    ## Tokenization prompt\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    # Making prompt in correct format\n",
    "    if IMAGE_PLACEHOLDER in prompt:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "        else:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = image_token_se + \"\\n\" + prompt\n",
    "        else:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "    ## Convert model\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    if conv_mode is not None and conv_mode != conv_mode:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, conv_mode, conv_mode\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        conv_mode = conv_mode\n",
    "\n",
    "    ## Load conversation mode standard template \n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    ## Tokenize prompt\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add on mean ablation our loaded values of memory\n",
    "from utils.misc.visualization import visualization_preprocess\n",
    "from utils.datasets_constants.cifar_10_classes import cifar_10_classes\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.misc.misc import accuracy, accuracy_correct\n",
    "from utils.scripts.algorithms_text_explanations import *\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.misc.visualization import visualization_preprocess\n",
    "from utils.models.prs_hook import hook_prs_logger\n",
    "from utils.datasets_constants.imagenet_classes import imagenet_classes\n",
    "from utils.datasets_constants.cifar_10_classes import cifar_10_classes\n",
    "from utils.datasets_constants.cub_classes import cub_classes, waterbird_classes\n",
    "import os\n",
    "from utils.datasets.dataset_helpers import dataset_to_dataloader\n",
    "from utils.scripts.algorithms_text_explanations_funcs import *\n",
    "import tqdm\n",
    "from torchvision.transforms import ToPILImage\n",
    "import copy\n",
    "\n",
    "# Constants and fixed configuration\n",
    "seed = 1\n",
    "path = './datasets/'\n",
    "batch_size = 1  # ToDO: ONLY WORK WITH B DIM 1 NOW\n",
    "prompt = \"You are a vision-language expert. Analyze the given image and classify it into one of the following categories:  \\\n",
    "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']. Answer with only the most appropriate category in lower case.\"\n",
    "\n",
    "# Prepare dataset\n",
    "ds_ = CIFAR10(root=path, download=True, train=False, transform=preprocess_clip)\n",
    "\n",
    "dataloader = dataset_to_dataloader(\n",
    "    ds_,\n",
    "    samples_per_class=1,\n",
    "    tot_samples_per_class=1000,  # or whatever you prefer\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "\n",
    "classes_ = cifar_10_classes\n",
    "classifier_ = torch.tensor(np.load(f\"output_dir/CIFAR10_classifier_{model_CLIP_name}.npy\", mmap_mode=\"r\")).to(device, dtype=torch.float16) # embedding of the labels\n",
    "\n",
    "print(classes_)\n",
    "num_total_images = len(dataloader) * batch_size\n",
    "print(f\"We are using a dataset containing {num_total_images} images.\")\n",
    "\n",
    "# Metrics to measure \n",
    "tot_correct_llava = 0\n",
    "tot_correct_clip = 0\n",
    "log_it = 100\n",
    "count = 0\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize accumulators for our test loop:\n",
    "# For each 'lay' test value, we store the number of correct predictions\n",
    "test_lay_values = [1, 5, 10, 15, 20, 21, 22, 23]\n",
    "test_results_llava = {lay: 0 for lay in test_lay_values}\n",
    "test_results_clip = {lay: 0 for lay in test_lay_values}\n",
    "# -------------------------------\n",
    "\n",
    "# Layer where to extract infos on patches\n",
    "select_layer = -2\n",
    "max_new_tokens = 512\n",
    "num_beams = 1  # number of path of decision, less faster\n",
    "sep =  \",\"\n",
    "temperature = 0  # 0 lowest, det\n",
    "top_p = None\n",
    "\n",
    "# Extract necessary pprojection for clip\n",
    "ln_post = copy.deepcopy(model_CLIP.visual.ln_post).to(\"cuda\")\n",
    "proj = copy.deepcopy(model_CLIP.visual.proj).to(\"cuda\")\n",
    "print(f\"Running the test with ablation up to layer {test_lay_values}\")\n",
    "# Inference loop over images\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Get format of prompt\n",
    "input_ids = get_prompt_formatted(prompt, model, conv_templates)\n",
    "\n",
    "for i, (image, labels) in enumerate(tqdm.tqdm(dataloader)):\n",
    "    batch_size_here = image.shape[0]\n",
    "    count += batch_size_here\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ## Use CLIP Model\n",
    "        prs.reinit()\n",
    "        model_CLIP.eval()\n",
    "        with torch.no_grad():\n",
    "            model_CLIP.to(\"cuda\")\n",
    "            # Get output as we want it\n",
    "            spatial_features = model_CLIP.encode_image(\n",
    "                    image.to(device, dtype=torch.float16), \n",
    "                    attn_method='head_no_spatial',\n",
    "                    normalize=False\n",
    "                )\n",
    "\n",
    "            model_CLIP.to(\"cpu\")\n",
    "\n",
    "            # Move directions\n",
    "            attentions, mlps = prs.finalize(spatial_features)  # attentions: [b, l, n, h, d], mlps: [b, l + 1, n, d]\n",
    "        \n",
    "        ## HERE CAN FINALLY WORK and perform our tests on this image\n",
    "        # For each test value of 'lay', we recompute the ablated features and run both LLAVA and CLIP inference.\n",
    "        # The results are accumulated for later overall accuracy computation.\n",
    "        model.to(\"cuda\")\n",
    "        for lay_val in test_lay_values:\n",
    "            # Compute spatial features required by our layer with the current 'lay' value.\n",
    "            attentions_abl, mlps_abl =  mean_ablate_head(\n",
    "                attentions, mlps, select_layer,\n",
    "                layers = [y for x in range(0, 16) for y in range(0, lay_val)],\n",
    "                heads = [x for x in range(0, 16) for y in range(0, lay_val)],\n",
    "                attentions_mean_abl = attns_hid_mean,\n",
    "                mlps_mean_abl = mlps_hid_mean,\n",
    "                mean_ablate_mlps = True)\n",
    "\n",
    "            ## LLAVA prediction using test_images_tensor\n",
    "            with torch.inference_mode():\n",
    "                test_output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images= llava_pred(attentions_abl, mlps_abl, select_layer)[:, 1:],\n",
    "                    image_sizes=image_sizes,\n",
    "                    num_beams=num_beams,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    use_cache=True,\n",
    "                    images_embeds=True  # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "                )\n",
    "            test_out = tokenizer.batch_decode(test_output_ids, skip_special_tokens=True)[0].strip()\n",
    "            # Update LLAVA test counter if prediction is correct.\n",
    "            print(\"Correct sol is \", classes_[labels[0]].lower())\n",
    "            print(test_out.lower())\n",
    "            if test_out.lower() == classes_[labels[0]].lower():\n",
    "                test_results_llava[lay_val] += 1\n",
    "\n",
    "            ## CLIP prediction using test_images_tensor b, l, n, h, d\n",
    "            hidden_output_test = ln_post(attentions_abl[:, :, 0].sum(1).sum(1) + mlps_abl[:, :, 0].sum(1)) #test_images_tensor[:, 0, :].squeeze(0)\n",
    "            test_clip_out = hidden_output_test @ proj\n",
    "            # Update CLIP test counter if prediction is correct.\n",
    "            print((test_clip_out @ classifier_).squeeze())\n",
    "            print(classes_[torch.argmax((test_clip_out @ classifier_).squeeze())])\n",
    "            print(classes_[torch.argmax((spatial_features @ classifier_).squeeze())])\n",
    "\n",
    "            if torch.argmax(test_clip_out @ classifier_) == labels[0]:\n",
    "                test_results_clip[lay_val] += 1\n",
    "        model.to(\"cpu\")\n",
    "\n",
    "        # --- End of tests for different lay values for this image ---\n",
    "\n",
    "        if (i + 1) % log_it == 0:\n",
    "            print(f\"Tot accuracy LLAVA so far is {test_results_llava/count*100}\")\n",
    "            print(f\"Tot accuracy ClIP so far is {test_results_clip/count*100}\")\n",
    "\n",
    "print(f\"Final accuracy LLAVA is {test_results_llava/count*100}\")\n",
    "print(f\"Final accuracy CLIP is {tot_correct_clip/count*100}\")\n",
    "\n",
    "# After processing all images, compute the overall accuracy for each test 'lay' value.\n",
    "result_str = \"LLAVA and CLIP accuracies for different 'lay' values:\\n\"\n",
    "for lay_val in test_lay_values:\n",
    "    acc_llava = test_results_llava[lay_val] / num_total_images * 100\n",
    "    acc_clip = test_results_clip[lay_val] / num_total_images * 100\n",
    "    result_str += f\"lay = {lay_val}: LLAVA accuracy: {acc_llava:.2f}%, CLIP accuracy: {acc_clip:.2f}%\\n\"\n",
    "\n",
    "# Save the final test results into a text file.\n",
    "with open(\"test_results.txt\", \"w\") as f:\n",
    "    f.write(result_str)\n",
    "\n",
    "print(\"Test results saved to test_results.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project a CLIP text embedding into hidden space of ViT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = retrieve_proj_matrix(model_CLIP).to(device)\n",
    "\n",
    "ln_weight, ln_bias, ln_eps = retrieve_post_layer_norm_par(model_CLIP)\n",
    "ln_weight, ln_bias, ln_eps = ln_weight.to(device), ln_bias.to(device), ln_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CLIP.to(\"cuda\")\n",
    "\n",
    "# Get an image and a query text\n",
    "with torch.no_grad():\n",
    "    prs.reinit()\n",
    "    model_CLIP.eval()\n",
    "    # If querying by text, define a text prompt and encode it into an embedding\n",
    "    text_query = \"cat.\"\n",
    "    # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "    text_query_token = tokenizer_CLIP(text_query).to(device)  \n",
    "    # Encode the tokenized text into a normalized embedding\n",
    "    topic_emb = model_CLIP.encode_text(text_query_token, normalize=False)\n",
    "    # If querying by image, load and preprocess the image from disk\n",
    "    prs.reinit()  # Reinitialize any hooks if required\n",
    "    text_query = \"woman.png\"\n",
    "    image_pil = Image.open(f'images/{text_query}')\n",
    "    image = preprocess_clip(image_pil)[np.newaxis, :, :, :]  # Add batch dimension\n",
    "    if precision == \"fp16\":\n",
    "        image = image.to(dtype=torch.float16)\n",
    "        topic_emb = topic_emb.to(dtype=torch.float16)\n",
    "        \n",
    "\n",
    "    # Encode the image into a normalized embedding\n",
    "    image_emb = model_CLIP.encode_image(\n",
    "        image.to(device), \n",
    "        attn_method='head_no_spatial',\n",
    "        normalize=False\n",
    "    )\n",
    "    print(image_emb.shape)\n",
    "    print(topic_emb.shape)\n",
    "# Center text embed on image embed\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "if precision == \"fp16\":\n",
    "    mean_final_images = mean_final_images.to(dtype=torch.float16)\n",
    "    mean_final_texts = mean_final_texts.to(dtype=torch.float16)\n",
    "\n",
    "topic_emb = topic_emb - mean_final_texts + mean_final_images\n",
    "\n",
    "print(\"Normal\")\n",
    "print(topic_emb.norm())\n",
    "print(image_emb.norm())\n",
    "print(topic_emb @ image_emb.T)\n",
    "\n",
    "# Project \n",
    "def remove(a, b):\n",
    "    a = a.squeeze()\n",
    "    b = b.squeeze()\n",
    "    return (a - (torch.dot(a, b) / torch.dot(b, b)) * b).unsqueeze(0)\n",
    "# Fictious values\n",
    "mean = torch.tensor(0.15)\n",
    "std = torch.tensor(1)\n",
    "\n",
    "invert_topic_emb = invert_proj_layer_norm(topic_emb, P, ln_weight, ln_bias, std, mean, ln_eps)\n",
    "invert_image_emb = remove(invert_proj_layer_norm(image_emb, P, ln_weight, ln_bias, std, mean, ln_eps), invert_topic_emb)\n",
    "\n",
    "print(\"After proj back\")\n",
    "print(invert_topic_emb.norm())\n",
    "print(invert_image_emb.norm())\n",
    "print(invert_topic_emb @ invert_image_emb.T)\n",
    "\n",
    "# Go back and revaluate\n",
    "topic_emb_p = model_CLIP.visual.ln_post(invert_topic_emb) @ P\n",
    "image_emb_p = model_CLIP.visual.ln_post(invert_image_emb) @ P\n",
    "print(invert_topic_emb.shape)\n",
    "print(\"Normal proj back\")\n",
    "print(topic_emb_p.norm())\n",
    "print(image_emb_p.norm())\n",
    "print(torch.norm(topic_emb_p - image_emb_p))\n",
    "print(topic_emb_p @ image_emb_p.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
