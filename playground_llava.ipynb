{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import requests\n",
    "import copy\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.nn.functional import mse_loss\n",
    "import numpy as np\n",
    "import einops\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "\n",
    "from utils.scripts.utils_llava import *\n",
    "from utils.misc.misc import accuracy, accuracy_correct\n",
    "from utils.scripts.algorithms_text_explanations import *\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.misc.visualization import visualization_preprocess\n",
    "from utils.models.prs_hook import hook_prs_logger\n",
    "from utils.datasets_constants.imagenet_classes import imagenet_classes\n",
    "from utils.datasets_constants.cifar_10_classes import cifar_10_classes\n",
    "from utils.datasets_constants.cub_classes import cub_classes, waterbird_classes\n",
    "import os\n",
    "from utils.datasets.dataset_helpers import dataset_to_dataloader\n",
    "from utils.scripts.algorithms_text_explanations_funcs import *\n",
    "import tqdm\n",
    "from torchvision.transforms import ToPILImage\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cuda'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = 10\n",
    "tot_samples_per_class = 50\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"imagenet\"\n",
    "cache_dir = \"../cache\"\n",
    "path = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def image_parser(image_file, sep=\",\"):\n",
    "    out = image_file.split(sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLava Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT: On Biomedcluster change .config under model_path to point towards correct vision_tower clip path\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_name),\n",
    ")\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP model\n",
    "model_CLIP_name = 'ViT-L-14-336' \n",
    "pretrained = \"openai\"\n",
    "precision = \"fp16\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_CLIP, _, preprocess_clip = create_model_and_transforms(model_CLIP_name, pretrained=pretrained, precision=precision, cache_dir=\"../cache\")\n",
    "\n",
    "model_CLIP.eval()\n",
    "context_length = model_CLIP.context_length\n",
    "# Not needed anymore\n",
    "vocab_size = model_CLIP.vocab_size\n",
    "tokenizer_CLIP = get_tokenizer(model_CLIP_name)\n",
    "\n",
    "print(model_CLIP)\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model_CLIP.visual.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model_CLIP.visual.transformer.resblocks))\n",
    "# Hook necessary to have: no projection on shared space, no spatial tokens in output (i.e. contributuon of attention to tokens), and hidden outputs of all tokens\n",
    "prs = hook_prs_logger(model_CLIP, device, spatial=False, vision_projection=False, full_output=True) # This attach hook to get the residual stream\n",
    "\n",
    "final_embeddings_images = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_embeddings_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\")).to(device)\n",
    "final_embeddings_texts = torch.tensor(np.load(f\"output_dir/{dataset_text_name}_{model_CLIP_name}.npy\", mmap_mode=\"r\")).to(device, dtype=final_embeddings_images.dtype)\n",
    "\n",
    "attns_hid_mean = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attns_mean_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # [l, n, h, d], attention values\n",
    "mlps_hid_mean = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlps_mean_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [l + 1, n, d], mlp values\n",
    "\n",
    "if datataset_image_name == \"imagenet\":\n",
    "    ds_ = ImageNet(root=path+\"imagenet/\", split=\"val\", transform=visualization_preprocess)\n",
    "elif datataset_image_name == \"binary_waterbirds\":\n",
    "    ds_ = BinaryWaterbirds(root=path+\"waterbird_complete95_forest2water2/\", split=\"test\", transform=visualization_preprocess)\n",
    "elif datataset_image_name == \"CIFAR100\":\n",
    "    ds_ = CIFAR100(\n",
    "        root=path, download=True, train=False, transform=visualization_preprocess\n",
    "    )\n",
    "elif datataset_image_name == \"CIFAR10\":\n",
    "    ds_ = CIFAR10(\n",
    "        root=path, download=True, train=False, transform=visualization_preprocess\n",
    "    )\n",
    "else:\n",
    "    ds_ = ImageFolder(root=path, transform=visualization_preprocess)\n",
    "\n",
    "classes_ = {\n",
    "        'imagenet': imagenet_classes, \n",
    "        'CIFAR10': cifar_10_classes,\n",
    "        'waterbirds': cub_classes, \n",
    "        'binary_waterbirds': waterbird_classes, \n",
    "        'cub': cub_classes}[datataset_image_name]\n",
    "        \n",
    "# Depending\n",
    "ds_vis_ = dataset_subset(\n",
    "    ds_,\n",
    "    samples_per_class=subset_dim,\n",
    "    tot_samples_per_class=tot_samples_per_class,  # or whatever you prefer\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "with open( f\"utils/text_descriptions/{dataset_text_name}.txt\", \"r\") as f:\n",
    "    texts_str = np.array([i.replace(\"\\n\", \"\") for i in f.readlines()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here play around with LLava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ablate_head(attentions, mlps, select_layer, layers=[], heads=[], \n",
    "                     attentions_mean_abl=None, mlps_mean_abl=None, mean_ablate_mlps=False, mean_ablate_attns=True):\n",
    "\n",
    "    # Clone the input tensors to prevent in-place modifications from affecting future calls\n",
    "    attentions = attentions.clone()\n",
    "    mlps = mlps.clone()\n",
    "\n",
    "    # Compute the mean value over the selected layers, all heads, and tokens (excluding token index 0) \n",
    "    # or use predefined mean\n",
    "    attentions_mean_abl = attentions_mean_abl.unsqueeze(0)\n",
    "    mlps_mean_abl =  mlps_mean_abl.unsqueeze(0)\n",
    "\n",
    "    # Replace the attention values for specified layers and heads with the computed mean ablation value\n",
    "    if heads is not [] and layers is not []:\n",
    "        for layer, head in zip(layers, heads):     \n",
    "            if mean_ablate_attns:   \n",
    "                attentions[:, layer, :, head, :] = attentions_mean_abl[:, layer, :, head, :]\n",
    "            # If required, mean ablate mlps\n",
    "            if mean_ablate_mlps:\n",
    "                mlps[:, layer+1, :, :] = mlps_mean_abl[:, layer+1, :, :] \n",
    "    # Since MLPS has one more layer, mean ablate also nr. zero\n",
    "    layers_set = set(layers)\n",
    "    is_continuous_from_zero = layers_set == set(range(len(layers_set)))\n",
    "    if mean_ablate_mlps == True and is_continuous_from_zero:\n",
    "        mlps[:, 0, :, :] = mlps_mean_abl[:, 0, :, :] \n",
    "    # Aggregate the modified attention tensor by summing over layers and heads,\n",
    "    # and add the corresponding summed MLP outputs \n",
    "    return attentions, mlps\n",
    "\n",
    "def llava_pred(attentions, mlps, select_layer):\n",
    "\n",
    "    return (attentions[:, :(select_layer + 1), :, :, :].sum(1).sum(2) +\n",
    "            mlps[:, :(select_layer + 1), :, :].sum(1))\n",
    "\n",
    "# Project \n",
    "def remove(a, b):\n",
    "    a = a.squeeze()\n",
    "    b = b.squeeze()\n",
    "    return (a - (torch.dot(a, b) / torch.dot(b, b)) * b).unsqueeze(0)\n",
    "\n",
    "def remove_patches(p, b):\n",
    "    p = p.squeeze()\n",
    "    for i in range(p.shape[0]):\n",
    "        p[i:i+1, :] = remove(p[i:i+1, :], b)\n",
    "\n",
    "    return p.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_infer(prompt, pil_image, images_embeds=False, mean_ablate=False, up_to_layer_ablate = 10, from_layer_ablate = False, mean_ablate_mlps=False, mean_ablate_attns=True, attentions_mean_abl=None, mlps_mean_abl=None): # If provided image embeds\n",
    "    # Layer where to extract infos on patches\n",
    "    select_layer = -2\n",
    "    max_new_tokens = 512\n",
    "    num_beams = 1 # numer of path of decision, less faster\n",
    "    sep =  \",\"\n",
    "    temperature = 0 # 0 lowest, det\n",
    "    top_p = None\n",
    "\n",
    "    ## Tokenization prompt\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    # Making prompt in correct format\n",
    "    if IMAGE_PLACEHOLDER in prompt:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "        else:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = image_token_se + \"\\n\" + prompt\n",
    "        else:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "    ## Convert model\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    if conv_mode is not None and conv_mode != conv_mode:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, conv_mode, conv_mode\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        conv_mode = conv_mode\n",
    "\n",
    "    ## Load conversation mode standard template \n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    # print(prompt)\n",
    "\n",
    "    ## Load images from online or local\n",
    "    \"\"\" image_files = image_parser(image_file, sep)\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images] \"\"\"\n",
    "    \n",
    "    image_sizes = [img.size]\n",
    "    \n",
    "    images_tensor = process_images(\n",
    "        [pil_image],\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    ## Tokenize prompt\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    attentions = None\n",
    "    mlps = None\n",
    "    ## Use CLIP Model\n",
    "    if images_embeds:\n",
    "        prs.reinit()\n",
    "        model_CLIP.eval()\n",
    "        with torch.no_grad():\n",
    "            model.to(\"cpu\")\n",
    "            model_CLIP.to(\"cuda\")\n",
    "            # Get output as we want it\n",
    "            spatial_features = model_CLIP.encode_image(\n",
    "                    images_tensor.to(device), \n",
    "                    attn_method='head_no_spatial',\n",
    "                    normalize=False\n",
    "                )\n",
    "\n",
    "            model_CLIP.to(\"cpu\")\n",
    "\n",
    "            # Move directions\n",
    "            attentions, mlps = prs.finalize(spatial_features)  # attentions: [b, l, n, h, d], mlps: [b, l + 1, n, d]\n",
    "\n",
    "            # Compute spatial features required by our layer \n",
    "            if mean_ablate:\n",
    "                attentions, mlps = mean_ablate_head(attentions, mlps, select_layer,\n",
    "                layers = [y for x in range(0, 16) for y in range(0, up_to_layer_ablate)] if not from_layer_ablate else\n",
    "                         [y for x in range(0, 16) for y in range(up_to_layer_ablate, 24)],\n",
    "                heads = [x for x in range(0, 16) for y in range(0, up_to_layer_ablate)] if not from_layer_ablate else\n",
    "                        [x for x in range(0, 16) for y in range(up_to_layer_ablate, 24)],\n",
    "                attentions_mean_abl = attentions_mean_abl,\n",
    "                mlps_mean_abl = mlps_mean_abl,\n",
    "                mean_ablate_mlps = mean_ablate_mlps,\n",
    "                mean_ablate_attns = mean_ablate_attns)\n",
    "            \n",
    "    \n",
    "            images_tensor = llava_pred(attentions, mlps, select_layer)\n",
    "            images_tensor[:, 1:], # Skip CLS\n",
    "        # images_tensor = remove_patches(images_tensor, invert_topic_emb)\n",
    "        # print(images_tensor.shape)\n",
    "        # Swap some features position \n",
    "        # images_tensor[:, :192, :] = images_tensor[:, 384:, :] \n",
    "        # images_tensor[:, 192:384, :] = images_tensor[:, 384:, :] \n",
    "        # images_tensor[:, 384:, :] = images_tensor[:, 384:, :] \n",
    "\n",
    "\n",
    "    ## Generate an answer by using full model LLava\n",
    "    model.to(\"cuda\")\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            #do_sample= True if temperature > 0 else False,\n",
    "            #temperature=temperature,\n",
    "            #top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            images_embeds = images_embeds # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "\n",
    "        )\n",
    "\n",
    "    ## Print the output\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return outputs, attentions, mlps\n",
    "\n",
    "\n",
    "# Params\n",
    "prompt = \"You are a vision-language expert. Analyze the given image and classify it into one of the following categories:  \\\n",
    "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']. Answer with only the most appropriate category.\"\n",
    "image_file = \"images/catdog.png\"\n",
    "\n",
    "## Visualize image\n",
    "img = Image.open(image_file)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()\n",
    "\n",
    "llava_infer(prompt, img, images_embeds = True, mean_ablate = True, up_to_layer_ablate = 20, from_layer_ablate = False, attentions_mean_abl=attns_hid_mean, mlps_mean_abl=mlps_hid_mean,  mean_ablate_mlps=True, mean_ablate_attns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image features a brown and white dog and a brown and black cat sitting together on a carpeted floor. They appear to be relaxed and comfortable in each other's company. The dog is positioned on the left side of the cat, with both animals facing the same direction.\n",
    "# In the background, there is a bookshelf with several books on it, adding a cozy and lived-in atmosphere to the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative test of MLPS ablations of LLAVA on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_most_similar_texts_images_clip(clip_output, final_embeddings_images, final_embeddings_texts, ds_vis, classes, texts_str):\n",
    "    # Visualize ds Initialize arrays to store the top score\n",
    "    scores_array_images = np.empty(\n",
    "        final_embeddings_images.shape[0], \n",
    "        dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    "    )\n",
    "\n",
    "    scores_array_texts = np.empty(\n",
    "        final_embeddings_texts.shape[0], \n",
    "        dtype=[('score', 'f4'), ('score_vis', 'f4'), ('txt_index', 'i4')]\n",
    "    )\n",
    "\n",
    "    # Compute mean embeddings for centering\n",
    "    mean_final_images = torch.mean(final_embeddings_images,  axis=0)\n",
    "    mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "    # Create arrays of indexes for referencing images and texts.\n",
    "    indexes_images = np.arange(0, final_embeddings_images.shape[0], 1) \n",
    "    indexes_texts = np.arange(0, final_embeddings_texts.shape[0], 1)\n",
    "\n",
    "    # Get mean of data and texts\n",
    "    mean_final_images = torch.mean(final_embeddings_images, axis=0).to(device)\n",
    "\n",
    "    # Compute scores for images\n",
    "\n",
    "    scores_array_images[\"score_vis\"] = (final_embeddings_images @ clip_output.T).squeeze().cpu().numpy()\n",
    "    scores_array_texts[\"score_vis\"] = (final_embeddings_texts @ clip_output.T).squeeze().cpu().numpy()\n",
    "\n",
    "    clip_output /= clip_output.norm(dim=-1, keepdim=True)\n",
    "    final_embeddings_images /= final_embeddings_images.norm(dim=-1, keepdim=True)\n",
    "    final_embeddings_texts /= final_embeddings_texts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    scores_array_images[\"score\"] = (final_embeddings_images @ clip_output.T).squeeze().cpu().numpy()\n",
    "    scores_array_texts[\"score\"] = (final_embeddings_texts @ clip_output.T).squeeze().cpu().numpy()\n",
    "\n",
    "    scores_array_images[\"img_index\"] = indexes_images\n",
    "    scores_array_texts[\"txt_index\"] = indexes_texts\n",
    "\n",
    "\n",
    "    # Define the number of top and worst images to look at for each princ_comp\n",
    "    nr_top_imgs = 8  # Number of top elements\n",
    "    nr_worst_imgs = 0  # Number of worst elements\n",
    "    nr_cont_imgs = 0  # Length of continuous elements\n",
    "\n",
    "    dbs = create_dbs(scores_array_images, scores_array_texts, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "\n",
    "    # Hardcoded visualizations\n",
    "    nrs_dbs = [nr_top_imgs, nr_worst_imgs, nr_cont_imgs]\n",
    "    dbs_new = []\n",
    "    for i, db in enumerate(dbs):\n",
    "        if nrs_dbs[i] == 0:\n",
    "            continue\n",
    "        dbs_new.append(db)\n",
    "    # Visualize \n",
    "    visualize_dbs_no_data(dbs_new, ds_vis, texts_str, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt and images\n",
    "prompt = \"Describe me in details the following image.\"\n",
    "images_files = [\"images/catdog.png\", \"images/four_people.png\"]\n",
    "\n",
    "# Until which which layer ablate and if want to reverse (not until but to layer)\n",
    "up_to_layer_ablate = 0\n",
    "from_layer_ablate = True\n",
    "mean_ablate_mlps = True\n",
    "mean_ablate_attns = True\n",
    "\n",
    "# Extract necessary pprojection for clip\n",
    "ln_post = copy.deepcopy(model_CLIP.visual.ln_post).to(\"cuda\")\n",
    "proj = copy.deepcopy(model_CLIP.visual.proj).to(\"cuda\")\n",
    "# Main analysis\n",
    "for image_file in images_files:\n",
    "    print(f\"Displaying the image {image_file}\")\n",
    "    # Load as PIL\n",
    "    img = Image.open(image_file)\n",
    "    # Show it\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis ticks and labels\n",
    "    plt.show()\n",
    "\n",
    "    ## Call LLAVA on normal image without modifying\n",
    "    output, attns, mlps = llava_infer(prompt, img, images_embeds = True)\n",
    "    print(attns.sum(3).sum(1).norm())\n",
    "    print(mlps.sum(1).norm())\n",
    "    print(\"Original LLAVA output\")\n",
    "    print(output)\n",
    "    print()\n",
    "    # Call CLIP on normal image \n",
    "    hidden_output_test = ln_post(attns[:, :, 0].sum(1).sum(1) + mlps[:, :, 0].sum(1)) # only CLS token\n",
    "    test_clip_out = hidden_output_test @ proj\n",
    "    print(\"Original CLIP  output text and images\")\n",
    "    visualize_most_similar_texts_images_clip(test_clip_out.detach(), final_embeddings_images, final_embeddings_texts, ds_vis_, classes_, texts_str)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    ## Call LLAVA on mlps mean ablate image\n",
    "    output, attns, mlps = llava_infer(prompt, img, images_embeds = True, mean_ablate = True, up_to_layer_ablate = up_to_layer_ablate, from_layer_ablate= from_layer_ablate,attentions_mean_abl=attns_hid_mean, mlps_mean_abl=mlps_hid_mean, mean_ablate_mlps=mean_ablate_mlps, mean_ablate_attns=mean_ablate_attns)\n",
    "    print(attns.sum(3).sum(1).norm())\n",
    "    print(mlps.sum(1).norm())\n",
    "\n",
    "    print(\"MLPS mean ablated LLAVA output\")\n",
    "    print(output)\n",
    "    print()\n",
    "    # Call CLIP on mlps mean ablated image \n",
    "    hidden_output_test = ln_post(attns[:, :, 0].sum(1).sum(1) + mlps[:, :, 0].sum(1)) # only CLS token\n",
    "    test_clip_out = hidden_output_test @ proj\n",
    "    print(\"MLPS mean ablated CLIP output text and images\")\n",
    "    visualize_most_similar_texts_images_clip(test_clip_out.detach(), final_embeddings_images, final_embeddings_texts, ds_vis_, classes_, texts_str)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test zero-shot accuracy VLM on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_formatted(prompt, model, conv_templates):\n",
    "    ## Tokenization prompt\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    # Making prompt in correct format\n",
    "    if IMAGE_PLACEHOLDER in prompt:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "        else:\n",
    "            prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = image_token_se + \"\\n\" + prompt\n",
    "        else:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "    ## Convert model\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    if conv_mode is not None and conv_mode != conv_mode:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, conv_mode, conv_mode\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        conv_mode = conv_mode\n",
    "\n",
    "    ## Load conversation mode standard template \n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    ## Tokenize prompt\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and fixed configuration\n",
    "seed = 0\n",
    "subset_dim = 10\n",
    "tot_samples_per_class = 1000\n",
    "path = './datasets/'\n",
    "batch_size = 1  # ToDO: ONLY WORK WITH B DIM 1 NOW\n",
    "dataset_name = \"CIFAR10\"\n",
    "classes_ = {\n",
    "        'imagenet': imagenet_classes, \n",
    "        'CIFAR10': cifar_10_classes,\n",
    "        'waterbirds': cub_classes, \n",
    "        'binary_waterbirds': waterbird_classes, \n",
    "        'cub': cub_classes}[dataset_name]\n",
    "\n",
    "prompt = f\"You are a vision-language expert. Analyze the given image and classify it into one of the following categories:  \\\n",
    "{classes_}. Answer with only the most appropriate category in lower case.\"\n",
    "\n",
    "# Load dataset\n",
    "if dataset_name == \"imagenet\":\n",
    "    ds_ = ImageNet(root=path+\"imagenet/\", split=\"val\", transform=preprocess_clip)\n",
    "elif dataset_name == \"binary_waterbirds\":\n",
    "    ds_ = BinaryWaterbirds(root=path+\"waterbird_complete95_forest2water2/\", split=\"test\", transform=preprocess_clip)\n",
    "elif dataset_name == \"CIFAR100\":\n",
    "    ds_ = CIFAR100(\n",
    "        root=path, download=True, train=False, transform=preprocess_clip\n",
    "    )\n",
    "elif dataset_name == \"CIFAR10\":\n",
    "    ds_ = CIFAR10(\n",
    "        root=path, download=True, train=False, transform=preprocess_clip\n",
    "    )\n",
    "else:\n",
    "    ds_ = ImageFolder(root=path, transform=preprocess_clip)\n",
    "\n",
    "dataloader = dataset_to_dataloader(\n",
    "    ds_,\n",
    "    samples_per_class=subset_dim,\n",
    "    tot_samples_per_class=tot_samples_per_class,  # or whatever you prefer\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Load classifier\n",
    "classifier_ = torch.tensor(np.load(f\"output_dir/{dataset_name}_classifier_{model_CLIP_name}.npy\", mmap_mode=\"r\")).to(device, dtype=torch.float16) # embedding of the labels\n",
    "attns_hid_mean = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attns_mean_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # [l, n, h, d], attention values\n",
    "mlps_hid_mean = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlps_mean_{model_CLIP_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [l + 1, n, d], mlp values\n",
    "\n",
    "num_total_images = len(dataloader) * batch_size\n",
    "print(f\"We are using a dataset containing {num_total_images} images.\")\n",
    "\n",
    "# Metrics to measure \n",
    "tot_correct_llava = 0\n",
    "tot_correct_clip = 0\n",
    "log_it = 10\n",
    "count = 0\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize accumulators for our test loop:\n",
    "# For each 'lay' test value, we store the number of correct predictions\n",
    "test_lay_values = [0, 1, 5, 10, 15, 20, 21, 22, 23]\n",
    "mean_ablate_attns = False\n",
    "mean_ablate_mlps = True\n",
    "test_results_llava = {lay: 0. for lay in test_lay_values}\n",
    "test_results_clip = {lay: 0. for lay in test_lay_values}\n",
    "# -------------------------------\n",
    "\n",
    "# Layer where to extract infos on patches\n",
    "select_layer = -2\n",
    "nr_heads = 16\n",
    "max_new_tokens = 512\n",
    "num_beams = 1  # number of path of decision, less faster\n",
    "sep =  \",\"\n",
    "temperature = 0  # 0 lowest, det\n",
    "top_p = None\n",
    "\n",
    "# Extract necessary projection for clip\n",
    "ln_post = copy.deepcopy(model_CLIP.visual.ln_post).to(\"cuda\")\n",
    "proj = copy.deepcopy(model_CLIP.visual.proj).to(\"cuda\")\n",
    "print(f\"Running the test with ablation up to layer {test_lay_values} and mean ablation for attns {mean_ablate_attns} and mean ablation for mlps {mean_ablate_mlps}\")\n",
    "# Inference loop over images\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "model_CLIP.eval()\n",
    "\n",
    "# Get format of prompt\n",
    "input_ids = get_prompt_formatted(prompt, model, conv_templates)\n",
    "# Print the prompt\n",
    "print(f\"The prompt is \\n\\n {prompt}\")\n",
    "\n",
    "# Precompute indices\n",
    "precomputed_indices = {}\n",
    "for lay_val in test_lay_values:\n",
    "    layers = [y for _ in range(nr_heads) for y in range(lay_val)]\n",
    "    heads = [x for x in range(nr_heads) for _ in range(lay_val)]\n",
    "    precomputed_indices[lay_val] = (layers, heads)\n",
    "\n",
    "for i, (image, labels) in enumerate(tqdm.tqdm(dataloader)):\n",
    "    batch_size_here = image.shape[0]\n",
    "    count += batch_size_here\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image_sizes = [image.size]\n",
    "        ## Use CLIP Model\n",
    "        prs.reinit()\n",
    "        with torch.no_grad():\n",
    "            ### THIS IS THE BOTTLENECK OF COMPUTATION (hook)\n",
    "            model_CLIP.to(\"cuda\")\n",
    "            # Get output as we want it\n",
    "            spatial_features = model_CLIP.encode_image(\n",
    "                    image.to(device, dtype=torch.float16), \n",
    "                    attn_method='head_no_spatial',\n",
    "                    normalize=False\n",
    "                )\n",
    "            model_CLIP.to(\"cpu\")\n",
    "\n",
    "            # Retrieve computations directions\n",
    "            attentions, mlps = prs.finalize(spatial_features)  # attentions: [b, l, n, h, d], mlps: [b, l + 1, n, d]\n",
    "        \n",
    "        ## HERE CAN FINALLY WORK and perform our tests on this image\n",
    "        # For each test value of 'lay', we recompute the ablated features and run both LLAVA and CLIP inference.\n",
    "        # The results are accumulated for later overall accuracy computation.\n",
    "\n",
    "        model.to(\"cuda\")\n",
    "        for lay_val in test_lay_values:\n",
    "            layers, heads = precomputed_indices[lay_val]            # Compute spatial features required by our layer with the current 'lay' value.\n",
    "            attentions_abl, mlps_abl =  mean_ablate_head(\n",
    "                attentions, mlps, select_layer,\n",
    "                layers = layers,\n",
    "                heads =  heads,\n",
    "                attentions_mean_abl = attns_hid_mean,\n",
    "                mlps_mean_abl = mlps_hid_mean,\n",
    "                mean_ablate_mlps = mean_ablate_mlps,\n",
    "                mean_ablate_attns = mean_ablate_attns)\n",
    "\n",
    "            ## LLAVA prediction using test_images_tensor\n",
    "            with torch.inference_mode():\n",
    "                test_output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images= llava_pred(attentions_abl, mlps_abl, select_layer)[:, 1:], # all patches beside CLS token\n",
    "                    image_sizes=image_sizes,\n",
    "                    num_beams=num_beams,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    use_cache=True,\n",
    "                    images_embeds=True  # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "                )\n",
    "            test_out = tokenizer.batch_decode(test_output_ids, skip_special_tokens=True)[0].strip()\n",
    "            # Update LLAVA test counter if prediction is correct.\n",
    "            if test_out.lower() in classes_[labels[0]].lower():\n",
    "                test_results_llava[lay_val] += 1\n",
    "\n",
    "            ## CLIP prediction using test_images_tensor b, l, n, h, d\n",
    "            hidden_output_test = ln_post(attentions_abl[:, :, 0].sum(1).sum(1) + mlps_abl[:, :, 0].sum(1)) # only CLS token\n",
    "            test_clip_out = hidden_output_test @ proj\n",
    "            # Update CLIP test counter if prediction is correct.\n",
    "            if torch.argmax((test_clip_out @ classifier_).squeeze()) == labels[0]:\n",
    "                test_results_clip[lay_val] += 1\n",
    "\n",
    "        model.to(\"cpu\")\n",
    "\n",
    "        # --- End of tests for different lay values for this image ---\n",
    "        if (i + 1) % log_it == 0:\n",
    "            for lay_val in test_lay_values:\n",
    "                acc_llava = test_results_llava[lay_val] / (i + 1) * 100\n",
    "                acc_clip = test_results_clip[lay_val] / (i + 1) * 100\n",
    "                print(f\"lay = {lay_val}: LLAVA accuracy: {acc_llava:.2f}%, CLIP accuracy: {acc_clip:.2f}%\\n\")\n",
    "\n",
    "# After processing all images, compute the overall accuracy for each test 'lay' value.\n",
    "result_str = \"LLAVA and CLIP accuracies for different 'lay' values:\\n\"\n",
    "for lay_val in test_lay_values:\n",
    "    acc_llava = test_results_llava[lay_val] / num_total_images * 100\n",
    "    acc_clip = test_results_clip[lay_val] / num_total_images * 100\n",
    "    result_str += f\"lay = {lay_val}: LLAVA accuracy: {acc_llava:.2f}%, CLIP accuracy: {acc_clip:.2f}%\\n\"\n",
    "\n",
    "\n",
    "results_filename = f\"test_results_{dataset_name}_abl_mlps_{mean_ablate_mlps}_abl_attns_{mean_ablate_attns}_only_last.txt\"\n",
    "\n",
    "# Save the final test results into a text file with the dynamic filename.\n",
    "with open(results_filename, \"w\") as f:\n",
    "    f.write(result_str)\n",
    "\n",
    "print(f\"Test results saved to {results_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project a CLIP text embedding into hidden space of ViT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = retrieve_proj_matrix(model_CLIP).to(device)\n",
    "\n",
    "ln_weight, ln_bias, ln_eps = retrieve_post_layer_norm_par(model_CLIP)\n",
    "ln_weight, ln_bias, ln_eps = ln_weight.to(device), ln_bias.to(device), ln_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CLIP.to(\"cuda\")\n",
    "\n",
    "# Get an image and a query text\n",
    "with torch.no_grad():\n",
    "    prs.reinit()\n",
    "    model_CLIP.eval()\n",
    "    # If querying by text, define a text prompt and encode it into an embedding\n",
    "    text_query = \"cat.\"\n",
    "    # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "    text_query_token = tokenizer_CLIP(text_query).to(device)  \n",
    "    # Encode the tokenized text into a normalized embedding\n",
    "    topic_emb = model_CLIP.encode_text(text_query_token, normalize=False)\n",
    "    # If querying by image, load and preprocess the image from disk\n",
    "    prs.reinit()  # Reinitialize any hooks if required\n",
    "    text_query = \"woman.png\"\n",
    "    image_pil = Image.open(f'images/{text_query}')\n",
    "    image = preprocess_clip(image_pil)[np.newaxis, :, :, :]  # Add batch dimension\n",
    "    if precision == \"fp16\":\n",
    "        image = image.to(dtype=torch.float16)\n",
    "        topic_emb = topic_emb.to(dtype=torch.float16)\n",
    "        \n",
    "\n",
    "    # Encode the image into a normalized embedding\n",
    "    image_emb = model_CLIP.encode_image(\n",
    "        image.to(device), \n",
    "        attn_method='head_no_spatial',\n",
    "        normalize=False\n",
    "    )\n",
    "    print(image_emb.shape)\n",
    "    print(topic_emb.shape)\n",
    "# Center text embed on image embed\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "if precision == \"fp16\":\n",
    "    mean_final_images = mean_final_images.to(dtype=torch.float16)\n",
    "    mean_final_texts = mean_final_texts.to(dtype=torch.float16)\n",
    "\n",
    "topic_emb = topic_emb - mean_final_texts + mean_final_images\n",
    "\n",
    "print(\"Normal\")\n",
    "print(topic_emb.norm())\n",
    "print(image_emb.norm())\n",
    "print(topic_emb @ image_emb.T)\n",
    "\n",
    "# Project \n",
    "def remove(a, b):\n",
    "    a = a.squeeze()\n",
    "    b = b.squeeze()\n",
    "    return (a - (torch.dot(a, b) / torch.dot(b, b)) * b).unsqueeze(0)\n",
    "# Fictious values\n",
    "mean = torch.tensor(0.15)\n",
    "std = torch.tensor(1)\n",
    "\n",
    "invert_topic_emb = invert_proj_layer_norm(topic_emb, P, ln_weight, ln_bias, std, mean, ln_eps)\n",
    "invert_image_emb = remove(invert_proj_layer_norm(image_emb, P, ln_weight, ln_bias, std, mean, ln_eps), invert_topic_emb)\n",
    "\n",
    "print(\"After proj back\")\n",
    "print(invert_topic_emb.norm())\n",
    "print(invert_image_emb.norm())\n",
    "print(invert_topic_emb @ invert_image_emb.T)\n",
    "\n",
    "# Go back and revaluate\n",
    "topic_emb_p = model_CLIP.visual.ln_post(invert_topic_emb) @ P\n",
    "image_emb_p = model_CLIP.visual.ln_post(invert_image_emb) @ P\n",
    "print(invert_topic_emb.shape)\n",
    "print(\"Normal proj back\")\n",
    "print(topic_emb_p.norm())\n",
    "print(image_emb_p.norm())\n",
    "print(torch.norm(topic_emb_p - image_emb_p))\n",
    "print(topic_emb_p @ image_emb_p.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
