{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-26 12:34:31,613] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "import gc\n",
    "import requests\n",
    "import copy\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.nn.functional import mse_loss\n",
    "import numpy as np\n",
    "import einops\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.models.prs_hook import hook_prs_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def image_parser(image_file, sep=\",\"):\n",
    "    out = image_file.split(sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLava Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaConfig {\n",
      "  \"_name_or_path\": \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaLlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"freeze_mm_mlp_adapter\": false,\n",
      "  \"freeze_mm_vision_resampler\": false,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"image_aspect_ratio\": \"pad\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
      "  \"mm_resampler_type\": null,\n",
      "  \"mm_use_im_patch_token\": false,\n",
      "  \"mm_use_im_start_end\": false,\n",
      "  \"mm_vision_select_feature\": \"patch\",\n",
      "  \"mm_vision_select_layer\": -2,\n",
      "  \"mm_vision_tower\": \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/\",\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"tune_mm_mlp_adapter\": false,\n",
      "  \"tune_mm_vision_resampler\": false,\n",
      "  \"unfreeze_mm_vision_tower\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_mm_proj\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/MT/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 303,507,456\n"
     ]
    }
   ],
   "source": [
    "### IMPORT: On Biomedcluster change .config under model_path to point towards correct vision_tower clip path\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_name),\n",
    "    device_map=\"cpu\",  # HERE\n",
    ")\n",
    "\n",
    "vision_enc = model.get_vision_tower()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in vision_enc.parameters()]):,}\")\n",
    "\n",
    "model1 = vision_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local files\n",
      "Model parameters: 304,293,888\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "Len of res: 24\n",
      "VisionTransformer(\n",
      "  (patchnorm_pre_ln): Identity()\n",
      "  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "  (patch_dropout): Identity()\n",
      "  (ln_pre): LayerNorm()\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (gelu): QuickGELUActivation()\n",
      "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CLIP model\n",
    "model_CLIP_name = 'ViT-L-14-336' \n",
    "pretrained = \"hf\"\n",
    "precision = \"fp16\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_CLIP, _, preprocess_clip = create_model_and_transforms(model_CLIP_name, pretrained=pretrained, precision=precision, cache_dir=\"../cache\")\n",
    "\n",
    "model_CLIP.eval()\n",
    "context_length = model_CLIP.context_length\n",
    "# Not needed anymore\n",
    "vocab_size = model_CLIP.vocab_size\n",
    "# tokenizer_CLIP = get_tokenizer(model_CLIP_name)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model_CLIP.visual.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model_CLIP.visual.transformer.resblocks))\n",
    "# Replace with the one we want to use\n",
    "model2 = model_CLIP.visual\n",
    "prs = hook_prs_logger(model_CLIP, device, spatial=False, vision_projection=False, full_output=True) # This attach hook to get the residual stream\n",
    "print(model_CLIP.visual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here play around with LLava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "Describe the image focusing on main subjects. ignore background ASSISTANT:\n",
      "tensor(734.5000, device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([1, 576, 1024])\n",
      "MSE Loss with original output: 1.0771484375\n",
      "The image features a brown and white dog and a brown and black cat sitting together on a carpeted floor. The dog is positioned on the left side of the cat, both of them facing the same direction. The dog appears to be a bulldog, while the cat is a tabby cat. The scene creates a sense of companionship between the two animals, as they share the same space and seem to be enjoying each other's company.\n",
      "The image features a brown and white cat lying on the floor next to a brown and white dog. The cat is positioned on the left side of the dog, creating a sense of companionship between the two animals. The dog appears to be sitting or standing, while the cat is relaxed on the floor. The scene takes place in a room with a window, providing natural light to the space.\n"
     ]
    }
   ],
   "source": [
    "# Layer where to extract infos on patches\n",
    "select_layer = -2\n",
    "# Params\n",
    "prompt = \"Describe the image focusing on main subjects. ignore background\"\n",
    "image_file = \"images/catdog.png\"\n",
    "max_new_tokens = 512\n",
    "num_beams = 1 # numer of path of decision, less faster\n",
    "sep =  \",\"\n",
    "temperature = 0 # 0 lowest, det\n",
    "top_p = None\n",
    "images_embeds = True # If provided image embeds\n",
    "\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "# Making prompt in correct format\n",
    "if IMAGE_PLACEHOLDER in prompt:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "    else:\n",
    "        prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = image_token_se + \"\\n\" + prompt\n",
    "    else:\n",
    "        prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "# Derive necessary conv\n",
    "if \"llama-2\" in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"mistral\" in model_name.lower():\n",
    "    conv_mode = \"mistral_instruct\"\n",
    "elif \"v1.6-34b\" in model_name.lower():\n",
    "    conv_mode = \"chatml_direct\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "if conv_mode is not None and conv_mode != conv_mode:\n",
    "    print(\n",
    "        \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "            conv_mode, conv_mode, conv_mode\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    conv_mode = conv_mode\n",
    "\n",
    "# Load conversation mode standard template \n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "print(prompt)\n",
    "\n",
    "# Load images from online or local\n",
    "image_files = image_parser(image_file, sep)\n",
    "images = load_images(image_files)\n",
    "image_sizes = [x.size for x in images]\n",
    "# Convert images to format b, 3, h, w (h = w) with resizing or padding\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "\n",
    "model_CLIP.eval()\n",
    "vision_enc.eval()\n",
    "# Tokenize prompt\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "\n",
    "# Use CLIP Model\n",
    "if images_embeds:\n",
    "    prs.reinit()\n",
    "    model.to(\"cuda\")\n",
    "    output_pt = vision_enc(images_tensor.to(device))\n",
    "    print(output_pt.norm())\n",
    "    print(output_pt.shape)\n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    #spatial_features = torch.randn((1, 576, 1024))# if want to manually edit features, we want b, nr_spat (576), d (1024)\n",
    "    # Need to handle some memory movement :/\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "   # prs.reinit()  # Reinitialize any hooks if required\n",
    "    with torch.no_grad():\n",
    "        model_CLIP.to(\"cuda\")\n",
    "        spatial_features = model_CLIP.encode_image(\n",
    "                images_tensor.to(device), \n",
    "                attn_method='head_no_spatial',\n",
    "                normalize=False\n",
    "            )\n",
    "\n",
    "        model_CLIP.to(\"cpu\")\n",
    "\n",
    "        attentions, mlps = prs.finalize(spatial_features)  # attentions: [1, 12, 197, 16, 512], [b, l, n, h, d], mlps: [1, 13, 512], [b, l + 1, d]\n",
    "        attentions = einops.rearrange(attentions, \"b l n h d -> b l h n d\")\n",
    "\n",
    "        spatial_features = (attentions[:, :select_layer, :, 1:, :].sum(1).sum(1) + mlps[:, :(select_layer + 1), 1:, :].sum(1))\n",
    "        print(\"MSE Loss with original output:\", mse_loss(output_pt.to(\"cpu\"), spatial_features.to(\"cpu\")).item())\n",
    "    # Move model back to gpu\n",
    "    images_tensor = spatial_features \n",
    "\n",
    "\n",
    "# Generate an answer by using full model LLava\n",
    "model.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        images_embeds = images_embeds # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "\n",
    "    )\n",
    "\n",
    "# Print the original output\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(outputs)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=output_pt,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        images_embeds = images_embeds # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "\n",
    "    )\n",
    "\n",
    "# Print the output\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
