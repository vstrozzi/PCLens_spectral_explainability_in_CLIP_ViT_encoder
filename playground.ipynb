{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce479d0c-554a-42ee-b365-84a4d9ab81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "import json\n",
    "from utils.misc.misc import accuracy, accuracy_correct\n",
    "from utils.scripts.algorithms_text_explanations import *\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.misc.visualization import visualization_preprocess\n",
    "from utils.models.prs_hook import hook_prs_logger\n",
    "from utils.datasets_constants.imagenet_classes import imagenet_classes\n",
    "from utils.scripts.algorithms_text_explanations import svd_data_approx\n",
    "from utils.datasets.dataset_helpers import dataset_to_dataloader\n",
    "from torch.nn import functional as F\n",
    "from utils.scripts.algorithms_text_explanations_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee675770-3be8-40bf-8659-31e2d2a811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cpu'\n",
    "pretrained = 'laion2b_s34b_b79k' # 'laion2b_s32b_b79k'\n",
    "model_name = 'ViT-B-32' # 'ViT-H-14'\n",
    "seed = 0\n",
    "num_last_layers = 4\n",
    "subset_dim = 10\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"imagenet\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "batch_size = 16 # only needed for the nn search\n",
    "imagenet_path = './datasets/imagenet/' # only needed for the nn search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db3598-0d7d-47a4-b6c6-8d02f4902e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Model\n",
    "model, _, preprocess = create_model_and_transforms(model_name, pretrained=pretrained, cache_dir=\"../cache\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model.visual.transformer.resblocks))\n",
    "\n",
    "prs = hook_prs_logger(model, device, spatial=False) # This attach hook to get the residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f51611-710d-45b9-a797-85a958cc047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the chosen algorithm on a dataset to derive text explanations \n",
    "command = f\"python -m utils.scripts.compute_text_explanations --device {device} --model {model_name} --algorithm {algorithm} --seed {seed} --text_per_princ_comp 20 --num_of_last_layers {num_last_layers} --text_descriptions {dataset_text_name}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef211e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new created attention datasets\n",
    "attention_dataset = f\"output_dir/{datataset_image_name}_completeness_{dataset_text_name}_{model_name}_algo_{algorithm}_seed_{seed}.jsonl\"\n",
    "\n",
    "# Load necessary data\n",
    "attns_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attn_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], attention values\n",
    "mlps_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlp_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], mlp values\n",
    "classifier_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_classifier_{model_name}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], embedding of the labels\n",
    "labels_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_labels_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # Position of the labels in the cosndiered dataset\n",
    "final_embeddings_images = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_embeddings_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))\n",
    "final_embeddings_texts = torch.tensor(np.load(f\"output_dir/{dataset_text_name}_{model_name}.npy\", mmap_mode=\"r\"))\n",
    "with open( f\"utils/text_descriptions/{dataset_text_name}.txt\", \"r\") as f:\n",
    "    texts_str = np.array([i.replace(\"\\n\", \"\") for i in f.readlines()])\n",
    "# Get mean ablation\n",
    "no_heads_attentions_ = attns_.sum(axis=(2))  # Sum over heads dimension\n",
    "last_ = attns_.shape[1] - num_last_layers\n",
    "# Replace attention activations until 'last' layer with their average, while keeping later layers intact.\n",
    "current_mean_ablation_per_head_sum_ = torch.mean(no_heads_attentions_[:, :last_ + 1], axis=0).sum(0)\n",
    "\n",
    "# Save important stuff\n",
    "nr_layers_ = attns_.shape[1]\n",
    "nr_heads_ = attns_.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d1202",
   "metadata": {},
   "source": [
    "# Print the top Principal Components text-interpretation for each Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "min_princ_comp = 10\n",
    "\n",
    "# Read JSON lines from attention_dataset\n",
    "# This file contains data about layers, heads, and their principal components (PCs) with associated metrics.\n",
    "data = get_data(attention_dataset, -1)\n",
    "    \n",
    "# Print the data in a nice formatted table\n",
    "print_data(data, min_princ_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea667e63",
   "metadata": {},
   "source": [
    "# Strongest Principal Components per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top entries to retrieve\n",
    "top_k = 10\n",
    "min_heap = []\n",
    "\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Sort data entries in descending order of strength_abs of the princial component\n",
    "top_k_entries = top_data(sort_data_by(data, \"strength_abs\", descending=True), top_k=top_k)\n",
    "\n",
    "# Print the top_k entries in a nice formatted table\n",
    "print_used_heads(top_k_entries)\n",
    "print_data(top_k_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71330301",
   "metadata": {},
   "source": [
    "# Visualize singular values of a principal component (both text and images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d778ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info on data\n",
    "layer = 10\n",
    "head = 7\n",
    "princ_comp = 1\n",
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 0  # Length of continuous elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPT. Visualize textSpan\n",
    "attention_dataset_ts = f\"output_dir/{datataset_image_name}_completeness_{dataset_text_name}_{model_name}_algo_text_span_seed_{seed}.jsonl\"\n",
    "\n",
    "visualize_text_span(layer, head, attention_dataset_ts, top_k= top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7df3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_principal_component(layer, head, princ_comp, nr_top_imgs, nr_worst_imgs, nr_cont_imgs, attention_dataset, final_embeddings_images, final_embeddings_texts, seed, imagenet_path, texts_str, imagenet_classes, samples_per_class=subset_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCs strength\n",
    "data = get_data(attention_dataset)\n",
    "plot_pc_sv(data, layer, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad6081",
   "metadata": {},
   "source": [
    "# Test accuracy of reconstruction of text and images using only the final embedding and their projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e37a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top entries to retrieve\n",
    "top_k = 1\n",
    "min_heap = []\n",
    "image = preprocess(Image.open('images/woman.png'))[np.newaxis, :, :, :]  # Add batch dimension\n",
    "text_query = \"An image of a woman.\"\n",
    "\n",
    "# Encode the image\n",
    "prs.reinit()  # Reinitialize the residual stream hook\n",
    "\n",
    "# Encode the image with no gradient calculation\n",
    "with torch.no_grad():\n",
    "      image_emb = model.encode_image(\n",
    "        image.to(device),\n",
    "        attn_method='head_no_spatial',\n",
    "        normalize=True)\n",
    "\n",
    "      # Encode the text\n",
    "      text_query_token = tokenizer(text_query).to(device)  # Tokenize the text query\n",
    "      topic_emb = model.encode_text(text_query_token, normalize=True)  # Encode the text query\n",
    "\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0).to(device)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0).to(device)\n",
    "\n",
    "# Mean center the embeddings\n",
    "topic_emb -= mean_final_texts\n",
    "image_emb -= mean_final_images\n",
    "\n",
    "# Iterate through the attention dataset and reconstruct embeddings\n",
    "[topic_emb_rec, image_emb_rec], _ = reconstruct_embeddings(data, [topic_emb, image_emb], [\"text\", \"image\"], device=device)\n",
    "\n",
    "\n",
    "# Print norms to understand magnitude before normalization\n",
    "print(\"Norm of topic_emb_rec before normalization:\", topic_emb_rec.norm().item())\n",
    "print(\"Norm of image_emb_rec before normalization:\", image_emb_rec.norm().item())\n",
    "\n",
    "# Normalize the reconstructed embeddings so they lie on the unit sphere\n",
    "topic_emb_rec /= topic_emb_rec.norm(dim=-1, keepdim=True)\n",
    "image_emb_rec /= image_emb_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "topic_emb /= topic_emb.norm(dim=-1, keepdim=True)\n",
    "image_emb /= image_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Print similarities between original and reconstructed embeddings\n",
    "print(\"Cosine similarity between original topic_emb and reconstructed topic_emb_rec:\",\n",
    "      (topic_emb @ topic_emb_rec.T).item())\n",
    "\n",
    "print(\"Cosine similarity between original image_emb and reconstructed image_emb_rec:\",\n",
    "      (image_emb @ image_emb_rec.T).item())\n",
    "\n",
    "# Print cross-similarities to compare text-image embeddings before and after reconstruction\n",
    "print(\"Cosine similarity between original topic_emb and original image_emb:\",\n",
    "      (topic_emb @ image_emb.T).item())\n",
    "\n",
    "print(\"Cosine similarity between original topic_emb and reconstructed image_emb_rec:\",\n",
    "      (topic_emb @ image_emb_rec.T).item())\n",
    "\n",
    "print(\"Cosine similarity between reconstructed topic_emb_rec and original image_emb:\",\n",
    "      (topic_emb_rec @ image_emb.T).item())\n",
    "\n",
    "print(\"Cosine similarity between reconstructed topic_emb_rec and reconstructed image_emb_rec:\",\n",
    "      (topic_emb_rec @ image_emb_rec.T).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25a1a4",
   "metadata": {},
   "source": [
    "# Query a topic or image and NNs on that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c66cf",
   "metadata": {},
   "source": [
    "### Define the query and analyze each Principal Component and derive a strength metric for reconstruction of the query-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b8153-73a3-4f30-bc1d-eddef413df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode so that no gradients are computed\n",
    "model.eval()\n",
    "query_text = True\n",
    "\n",
    "# Retrieve an embedding\n",
    "with torch.no_grad():\n",
    "    if query_text:\n",
    "        # If querying by text, define a text prompt and encode it into an embedding\n",
    "        text_query = \"woman\"\n",
    "        # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "        text_query_token = tokenizer(text_query).to(device)  \n",
    "        # Encode the tokenized text into a normalized embedding\n",
    "        topic_emb = model.encode_text(text_query_token, normalize=True)\n",
    "    else:\n",
    "        # If querying by image, load and preprocess the image from disk\n",
    "        prs.reinit()  # Reinitialize any hooks if required\n",
    "        text_query = \"woman.png\"\n",
    "        image_pil = Image.open(f'images/{text_query}')\n",
    "        image = preprocess(image_pil)[np.newaxis, :, :, :]  # Add batch dimension\n",
    "        # Encode the image into a normalized embedding\n",
    "        topic_emb = model.encode_image(\n",
    "            image.to(device), \n",
    "            attn_method='head_no_spatial',\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "### Reconstruct embedding and find contributions from principal components\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0).to(device)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0).to(device)\n",
    "\n",
    "# Mean center the embeddings\n",
    "mean_final = mean_final_texts if query_text else mean_final_images\n",
    "\n",
    "# Mean center the embeddings\n",
    "topic_emb_cent = topic_emb - mean_final\n",
    "final_embeddings_texts_cent = final_embeddings_texts.to(device) - mean_final_texts\n",
    "# Recontruct embedding\n",
    "[topic_emb_rec_cent], data = reconstruct_embeddings(data, [topic_emb_cent], [\"text\" if query_text else \"image\"], return_princ_comp=True, plot=True, means=[mean_final], device=device)\n",
    "\n",
    "# Normalize the embeddings\n",
    "topic_emb_rec_cent_norm = topic_emb_rec_cent / topic_emb_rec_cent.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# The maximum reconstruction score is how close the reconstructed embedding is to the original,\n",
    "# adjusted by the baseline score. This gives a cosine similarity measure.\n",
    "topic_emb_cent_norm = topic_emb_cent / topic_emb_cent.norm(dim=-1, keepdim=True)\n",
    "max_reconstr_score = topic_emb_rec_cent_norm @ topic_emb_cent_norm.T\n",
    "# Print out the cosine similarity between the original and reconstructed embeddings\n",
    "print(f\"We have a max cosine similarity of: {(max_reconstr_score).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3bb4b",
   "metadata": {},
   "source": [
    "### Use the strength of the previous reconstruction to derive a good enough reconstruction of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant details from the top-k principal component entries based on the reconstruction of the query\n",
    "top_k = 50  # Maximum number of top entries to retrieve\n",
    "approx = 1.1  # Target approximation threshold for the reconstruction quality\n",
    "\n",
    "# Initialize a tensor to accumulate the reconstructed topic embedding from selected principal components\n",
    "topic_emb_rec_act = torch.zeros_like(topic_emb)\n",
    "\n",
    "### Extract relevant details from the top k entries\n",
    "data = sort_data_by(data, \"correlation_princ_comp_abs\", descending=True) \n",
    "\n",
    "top_k_entries = top_data(data, top_k)\n",
    "\n",
    "top_k_details = reconstruct_top_embedding(top_k_entries, topic_emb_cent, mean_final, \"text\" if query_text else \"image\", max_reconstr_score, top_k, approx, device=device)\n",
    "# Convert the collected principal component details into a DataFrame for easy processing\n",
    "print(f\"Currently querying the topic: {text_query}\")\n",
    "print_data(top_k_details, is_corr_present=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cea6d",
   "metadata": {},
   "source": [
    "### Prepare scores of images and texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE OTHERS PCs (uncomment here)\n",
    "#data = get_data(attention_dataset, -1, skip_final=True)\n",
    "#top_k_entries = get_remaining_pcs(data, top_k_entries)\n",
    "\n",
    "\n",
    "## For Reconstructed Embedding\n",
    "# Visualize ds\n",
    "ds_vis = create_dataset_imagenet(imagenet_path, visualization_preprocess, samples_per_class=subset_dim, tot_samples_per_class=50, seed=seed)\n",
    "# Initialize arrays to store the top and lowest scores based on similarity with original query\n",
    "scores_array_images = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "scores_array_texts = np.empty(\n",
    "    final_embeddings_texts.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('txt_index', 'i4')]\n",
    ")\n",
    "# Initialize arrays to store the top and lowest scores based on similarity with self reconstructed query\n",
    "scores_array_images_self = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "scores_array_texts_self = np.empty(\n",
    "    final_embeddings_texts.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('txt_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Create arrays of indexes for referencing images and texts.\n",
    "indexes_images = np.arange(0, final_embeddings_images.shape[0], 1) \n",
    "indexes_texts = np.arange(0, final_embeddings_texts.shape[0], 1) \n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0).to(device)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0).to(device)\n",
    "\n",
    "images_centered = final_embeddings_images.to(device) - mean_final_images\n",
    "texts_centered = final_embeddings_texts.to(device) - mean_final_texts\n",
    "\n",
    "# Recontruct embedding\n",
    "[texts_rec_cent, images_rec_cent], _ = reconstruct_embeddings(top_k_entries, [texts_centered, images_centered], [\"text\", \"image\"], return_princ_comp=False, device=device)\n",
    "\n",
    "texts_rec = texts_rec_cent\n",
    "images_rec = images_rec_cent\n",
    "# Compute the similarity scores between the reconstructed embeddings (images or texts) and the original query embedding.\n",
    "# The dot product gives a similarity measure, which we store in the scores arrays along with the index.\n",
    "# We do NOT normalize the score.\n",
    "# Compute scores for images\n",
    "\n",
    "scores_array_images[\"score\"] = (images_rec @ topic_emb_cent.T).squeeze().cpu().numpy()\n",
    "scores_array_images_self[\"score\"] = (torch.diag(images_rec @ images_centered.T)).squeeze().cpu().numpy()\n",
    "\n",
    "images_rec /= images_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "scores_array_images[\"score_vis\"] = (images_rec @ topic_emb_cent.T).squeeze().cpu().numpy()\n",
    "scores_array_images_self[\"score_vis\"] = (torch.diag(images_rec @ images_centered.T)).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_images[\"img_index\"] = indexes_images\n",
    "scores_array_images_self[\"img_index\"] = indexes_images\n",
    "\n",
    "# Compute scores for texts\n",
    "scores_array_texts[\"score\"] = (texts_rec @ topic_emb_cent.T).squeeze().cpu().numpy()\n",
    "scores_array_texts_self[\"score\"] = (torch.diag(texts_rec @ texts_centered.T)).squeeze().cpu().numpy()\n",
    "\n",
    "texts_rec /= texts_rec.norm(dim=-1, keepdim=True)\n",
    "scores_array_texts[\"score_vis\"] = (texts_rec @ topic_emb_cent.T).squeeze().cpu().numpy()\n",
    "scores_array_texts_self[\"score_vis\"] = (torch.diag(texts_rec @ texts_centered.T)).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_texts[\"txt_index\"] = indexes_texts\n",
    "scores_array_texts_self[\"txt_index\"] = indexes_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eccc6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For full CLIP Embedding\n",
    "# Scores array of real CLIP embeddings\n",
    "scores_array_images_full = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "scores_array_texts_full = np.empty(\n",
    "    final_embeddings_texts.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('txt_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Compute scores for images\n",
    "images = final_embeddings_images.to(device)\n",
    "scores_array_images_full[\"score\"] = (images @ topic_emb.T).squeeze().cpu().numpy()\n",
    "\n",
    "images /= images.norm(dim=-1, keepdim=True)\n",
    "scores_array_images_full[\"score_vis\"] = (images @ topic_emb.T).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_images_full[\"img_index\"] = indexes_images\n",
    "\n",
    "# Compute scores for texts\n",
    "texts = final_embeddings_texts.to(device)\n",
    "scores_array_texts_full[\"score\"] = (texts @ topic_emb.T).squeeze().cpu().numpy()\n",
    "\n",
    "texts /= texts.norm(dim=-1, keepdim=True)\n",
    "scores_array_texts_full[\"score_vis\"] = (texts @ topic_emb.T).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_texts_full[\"txt_index\"] = indexes_texts\n",
    "\n",
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 20  # Length of continuous elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae29f",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 0  # Length of continuous elements\n",
    "\n",
    "dbs = create_dbs(scores_array_images, scores_array_texts, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "# Hardcoded visualizations\n",
    "nrs_dbs = [nr_top_imgs, nr_worst_imgs, nr_cont_imgs]\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis, texts_str, imagenet_classes, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full Embeddings similarity\n",
    "dbs = create_dbs(scores_array_images_full, scores_array_texts_full, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis, texts_str, imagenet_classes, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "dbs = create_dbs(scores_array_images_self, scores_array_texts_self, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis, texts_str, imagenet_classes, text_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7efe8e",
   "metadata": {},
   "source": [
    "# Evaluate classification using reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a7422",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_layers = attns_.shape[1]\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "accs = []\n",
    "for layer_nr in range(nr_layers):\n",
    "    current_mean_ablation_per_head_sum = torch.mean(no_heads_attentions_[:, :layer_nr], axis=0).sum(0)\n",
    "    current_model = (current_mean_ablation_per_head_sum  + no_heads_attentions_[:, layer_nr + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "    acc, _ = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {nr_layers - layer_nr} to {nr_layers}\")\n",
    "    accs.append(acc)\n",
    "\n",
    "# Create an x-axis that has one increment for each element in acc\n",
    "x_values = range(len(accs))\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, accs, linestyle='-', label=model_name)\n",
    "\n",
    "# Labeling\n",
    "plt.xlabel(\"Accumulated mean-ablated layers\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Accumulated Mean-Ablated Layers\")\n",
    "\n",
    "# Add legend for the line\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f9d05",
   "metadata": {},
   "source": [
    "## Proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c800759",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_per_class = 400\n",
    "max_approx_per_class = 1\n",
    "class_embeddings = classifier_.T  # M x D\n",
    "\n",
    "# Print baseline accuracy\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "acc, _ = test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "\n",
    "# Reconstruct embeddings for each class label\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "images_centered = final_embeddings_images - mean_final_images.unsqueeze(0)\n",
    "classes_centered = class_embeddings - mean_final_texts.unsqueeze(0)\n",
    "\n",
    "# Initialize a (num_images x 2) array to track:\n",
    "#   [best_score_so_far, class_index_for_that_score]\n",
    "all_preds = torch.zeros((final_embeddings_images.shape[0], 2), dtype=torch.double)\n",
    "\n",
    "for text_idx in range(classes_centered.shape[0]):\n",
    "    # Perform query system on entry\n",
    "    class_i_centered = classes_centered[text_idx, :].unsqueeze(0)\n",
    "\n",
    "    data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "    [class_i_rec], data = reconstruct_embeddings(\n",
    "        data, \n",
    "        [class_i_centered], \n",
    "        [\"text\"], \n",
    "        return_princ_comp=True, \n",
    "        plot=False, \n",
    "        means=[mean_final_texts]\n",
    "    )\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    class_i_rec_norm = class_i_rec / class_i_rec.norm(dim=-1, keepdim=True)\n",
    "    class_i_rec_norm += mean_final_texts\n",
    "    class_i_rec_norm /= class_i_rec_norm.norm(dim=-1, keepdim=True)\n",
    "    # The maximum reconstruction score is how close the reconstructed embedding is\n",
    "    # to the original, adjusted by the baseline score. This is a cosine similarity measure.\n",
    "    class_i_centered_norm = class_i_centered / class_i_centered.norm(dim=-1, keepdim=True)\n",
    "    max_reconstr_score = class_i_rec_norm @ class_i_rec_norm.T\n",
    "\n",
    "    # Extract relevant details from the top k entries\n",
    "    data_pcs = sort_data_by(data, \"correlation_princ_comp_abs\", descending=True)\n",
    "    top_k_entries = top_data(data_pcs, pcs_per_class)\n",
    "\n",
    "    top_k_details = reconstruct_top_embedding(\n",
    "        top_k_entries,\n",
    "        class_i_centered,\n",
    "        mean_final_texts,\n",
    "        \"text\",\n",
    "        max_reconstr_score,\n",
    "        pcs_per_class,\n",
    "        max_approx_per_class,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    # Reconstruct final_embeddings_images\n",
    "    [images_rec_cent], _ = reconstruct_embeddings_proj(\n",
    "        top_k_entries, \n",
    "        [images_centered], \n",
    "        [\"image\"], \n",
    "        return_princ_comp=False\n",
    "    )\n",
    "\n",
    "    # Compute predictions for all images under the current text_idx\n",
    "    images_rec_cent = images_rec_cent / images_rec_cent.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    images_rec_cent += mean_final_images\n",
    "    # predictions = torch.diag(images_rec_cent @ final_embeddings_images.T)\n",
    "    predictions = images_rec_cent @ class_i_rec_norm.squeeze(0).T #class_embeddings[text_idx, :].T\n",
    "\n",
    "    # Update \"best so far\" scores in all_preds\n",
    "    best_vals_this_round = predictions\n",
    "    improved_mask = best_vals_this_round > all_preds[:, 0]\n",
    "\n",
    "    best_idxs_this_round = torch.full_like(all_preds[:, 1], fill_value=text_idx)\n",
    "    all_preds[improved_mask, 0] = best_vals_this_round[improved_mask].double()\n",
    "    all_preds[improved_mask, 1] = best_idxs_this_round[improved_mask].double()\n",
    "\n",
    "    # Optionally, check accuracy for the current text_idx predictions\n",
    "    acc, idxs = test_accuracy(predictions.unsqueeze(-1), labels_, label=f\"{imagenet_classes[text_idx]}\")\n",
    "    print_correct_elements(idxs, subset_dim)\n",
    "\n",
    "    # Build a fictitious one-hot matrix from all_preds\n",
    "    num_images = final_embeddings_images.shape[0]\n",
    "    num_classes = classifier_.shape[1]  # Typically M x D => M classes => classifier_.shape[1] is #classes\n",
    "\n",
    "    # Convert the best class index to a LongTensor\n",
    "    best_class_idxs = all_preds[:, 1].long()\n",
    "\n",
    "    # Create zero matrix [num_images, num_classes]\n",
    "    fictitious_preds = torch.zeros((num_images, num_classes), device=best_class_idxs.device)\n",
    "\n",
    "    # Fill 1.0 in the best predicted class for each image\n",
    "    fictitious_preds[torch.arange(num_images), best_class_idxs] = 1.0\n",
    "\n",
    "    # Test accuracy on these \"hard\" predictions\n",
    "    acc_best, idxs_best = test_accuracy(fictitious_preds, labels_, label=\"Best So Far (One-Hot)\")\n",
    "    sorted_output = print_correct_elements(idxs_best, subset_dim)\n",
    "\n",
    "    # Print overall accuracy so far\n",
    "    tot_sum = 0\n",
    "    for _, el_nr in sorted_output:\n",
    "        tot_sum += el_nr\n",
    "    print(f\"Tot accuracy so far is {tot_sum/((text_idx + 1) * subset_dim)}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35803f",
   "metadata": {},
   "source": [
    "## Test different accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of the tensors for debugging purposes:\n",
    "# attns_: attention activations\n",
    "# mlps_: MLP activations\n",
    "# classifier_: classifier weights\n",
    "# labels_: ground truth labels\n",
    "print(attns_.shape, mlps_.shape, classifier_.shape, labels_.shape)\n",
    "\n",
    "\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "# Test accuracy of mean centered data with mean centered text\n",
    "mean_centered_data = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1) - mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "mean_centered_classifier_ = classifier_ - mean_final_texts.unsqueeze(-1)\n",
    "mean_centered_classifier_ /= mean_centered_classifier_.norm(dim=-1, keepdim=True)\n",
    "\n",
    "test_accuracy(mean_centered_data @ mean_centered_classifier_, labels_, label=\"Mean centered data with mean centered text\")\n",
    "\n",
    "# Test accuracy of mean centered data with original text\n",
    "mean_centered_data = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1) - mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "mean_centered_data += mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "test_accuracy(mean_centered_data @ classifier_, labels_, label=\"Mean centered data with original (not mean centered) text\")\n",
    "\n",
    "# We now attempt a \"mean ablation\" approach for attention\n",
    "current_model = (current_mean_ablation_per_head_sum_\n",
    "                 + no_heads_attentions_[:, last_ + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "_, indexes_mean_ablate = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {last_} until layer {attns_.shape[1]}\")\n",
    "\n",
    "# We now attempt a \"mean ablation\" approach for attention\n",
    "current_model = (current_mean_ablation_per_head_sum_\n",
    "                 + no_heads_attentions_[:, last_ + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "current_model -= mean_final_images\n",
    "current_model /= current_model.norm(dim=-1, keepdim=True)\n",
    "_, indexes_mean_ablate = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {last_} until layer {attns_.shape[1]} with mean centered images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252fe7a",
   "metadata": {},
   "source": [
    "## Test different accuracies using reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45515051",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings_images_rec_embed = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns_not_mean_centered = torch.zeros_like(final_embeddings_images)\n",
    "image_emb_cent_embed = final_embeddings_images - mean_final_images\n",
    "\n",
    "final_embeddings_texts_rec_embed = torch.zeros_like(classifier_.T)\n",
    "texts_emb_cent_embed = (classifier_ - mean_final_texts.unsqueeze(-1)).T\n",
    "\n",
    "# Open the attention dataset to retrieve projection matrices and mean values\n",
    "with open(attention_dataset, \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        entry = json.loads(line)\n",
    "        # If this entry is the final embedding entry (head == -1), skip it.\n",
    "        if entry[\"head\"] == -1:\n",
    "            last_line = entry\n",
    "            continue\n",
    "\n",
    "        project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "        vh = torch.tensor(entry[\"vh\"])\n",
    "        # Reconstruct the image embeddings using final embeddings:\n",
    "        # Center them by subtracting mean attention values, project them through vh, \n",
    "        # apply project_matrix and vh again, then add mean values back.\n",
    "        final_embeddings_images_rec_embed += (image_emb_cent_embed) @ vh.T @ project_matrix @ vh\n",
    "        final_embeddings_texts_rec_embed += (texts_emb_cent_embed) @ vh.T @ project_matrix @ vh\n",
    "        # Reconstruct the image embeddings using attention activations:\n",
    "        # Similar process, but start from attns_ for the given layer/head.\n",
    "        image_emb_cent_attns = attns_[:, entry[\"layer\"], entry[\"head\"], :] - torch.tensor(entry[\"mean_values_att\"])\n",
    "        final_embeddings_images_rec_attns += (image_emb_cent_attns) @ vh.T @ project_matrix @ vh + torch.tensor(entry[\"mean_values_att\"])\n",
    "        final_embeddings_images_rec_attns_not_mean_centered += (image_emb_cent_attns) @ vh.T @ project_matrix @ vh\n",
    "\n",
    "final_embeddings_images_rec_embed_norm = final_embeddings_images_rec_embed/final_embeddings_images_rec_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "final_embeddings_texts_rec_embed_norm = final_embeddings_texts_rec_embed/final_embeddings_texts_rec_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "final_embeddings_images_rec_attns_not_mean_centered_norm = final_embeddings_images_rec_attns_not_mean_centered/final_embeddings_images_rec_attns_not_mean_centered.norm(dim=-1, keepdim=True)\n",
    "\n",
    "texts_emb_cent_embed /= texts_emb_cent_embed.norm(dim=-1, keepdim=True)\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed_norm + mean_final_images\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with final embeddings on only the last layers\")\n",
    "\n",
    "current_model = mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns_not_mean_centered\n",
    "_, indexes_approx_activ_only = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images with direct contribution of activation space\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the attention activations approach\n",
    "current_model = (mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns)\n",
    "_, indexes_approx_activ = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with attention activations\")\n",
    "\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed+ mean_final_images\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images without mean-ablation\")\n",
    "\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed+ mean_final_images\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images and texts without mean-ablation\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = (mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns)\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Original images and approximation of texts without mean-ablation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba44aa0",
   "metadata": {},
   "source": [
    "## Test Bias Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"../cache\"\n",
    "top_k = 30  # Maximum number of top entries to retrieve\n",
    "approx = 1.1  # Target approximation threshold for the reconstruction quality\n",
    "## Run the chosen algorithm on a dataset to derive text explanations \n",
    "command = f\"python -m utils.scripts.bias_removal_test \\\n",
    "    --device {device} --model {model_name} --pretrained {pretrained} --seed {seed} \\\n",
    "    --subset_dim {subset_dim} --dataset_text {dataset_text_name} --dataset {datataset_image_name} \\\n",
    "    --device {device} --top_k {top_k} --max_approx {approx} --cache_dir {cache_dir}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0c5f8",
   "metadata": {},
   "source": [
    "## Test bias removal and subset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we calculate scores for each principal component (PC) while ignoring query info.\n",
    "# We do this by reconstructing embeddings from the principal components alone, both from \n",
    "# the final embeddings perspective and the attention activations perspective.\n",
    "\n",
    "final_embeddings_images_rec_embed_topic = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns_topic = torch.zeros_like(final_embeddings_images)\n",
    "\n",
    "image_emb_cent_embed = final_embeddings_images - mean_final_images\n",
    "\n",
    "top_k_other_details = get_remaining_pcs(data, top_k_details)\n",
    "# Iterate through the top_k entries and reconstruct embeddings\n",
    "for entry in top_k_details:\n",
    "    # Reconstruct embeddings focusing on each principal component:\n",
    "    # 1. Start from the final embeddings, center them, and extract the component of interest.\n",
    "    vh = torch.tensor(entry[\"vh\"])\n",
    "    project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "    princ_comp = torch.tensor(entry[\"princ_comp\"])\n",
    "\n",
    "    projection_image_embed = image_emb_cent_embed @ vh.T\n",
    "    mask_images_embed = torch.zeros_like(projection_image_embed)\n",
    "    mask_images_embed[:, princ_comp] = projection_image_embed[:, princ_comp]\n",
    "    final_embeddings_images_rec_embed_topic += mask_images_embed @ project_matrix @ vh\n",
    "\n",
    "    # Repeat for attention-based activations:\n",
    "    mean_values_att = torch.tensor(entry[\"mean_values_att\"])\n",
    "    image_emb_cent_attns = attns_[:, entry[\"layer\"], entry[\"head\"], :] - mean_values_att\n",
    "    projection_images_attns = image_emb_cent_attns @ vh.T\n",
    "    mask_images_attns = torch.zeros_like(projection_images_attns)\n",
    "    mask_images_attns[:, princ_comp] = projection_images_attns[:, princ_comp]\n",
    "    final_embeddings_images_rec_attns_topic += mask_images_attns @ project_matrix @ vh\n",
    "\n",
    "\"\"\" # Mean ablate the other components\n",
    "for entry in top_k_other_details:\n",
    "    # Reconstruct embeddings focusing on each principal component:\n",
    "    # 1. Start from the final embeddings, center them, and extract the component of interest.\n",
    "    vh = torch.tensor(entry[\"vh\"])\n",
    "    project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "    princ_comp = torch.tensor(entry[\"princ_comp\"])\n",
    "    mean_values_att = torch.tensor(entry[\"mean_values_att\"]).unsqueeze(0)\n",
    "\n",
    "    projection_image_embed = mean_values_att @ vh.T\n",
    "    mask_images_embed = torch.zeros_like(projection_image_embed)\n",
    "    mask_images_embed[:, princ_comp] = projection_image_embed[:, princ_comp]\n",
    "    final_embeddings_images_rec_embed_topic += mask_images_embed @ project_matrix @ vh\n",
    "\n",
    "    # Repeat for attention-based activations:\n",
    "    image_emb_cent_attns = mean_values_att\n",
    "    projection_images_attns = image_emb_cent_attns @ vh.T\n",
    "    mask_images_attns = torch.zeros_like(projection_images_attns)\n",
    "    mask_images_attns[:, princ_comp] = projection_images_attns[:, princ_comp]\n",
    "    final_embeddings_images_rec_attns_topic += mask_images_attns @ project_matrix @ vh \"\"\"\n",
    "\n",
    "# Compute accuracy using the reconstruction from final embeddings, ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_embed - final_embeddings_images_rec_embed_topic\n",
    "_, indexes_approx_final_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Bias removal)\")\n",
    "print_diff_elements(indexes_approx_final, indexes_approx_final_rem, subset_dim)\n",
    "\n",
    "# Compute accuracy using the reconstruction from attention activations, also ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_attns_not_mean_centered - final_embeddings_images_rec_attns_topic\n",
    "_, indexs_approx_activ_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Bias Removal)\")\n",
    "print_diff_elements(indexes_approx_activ_only, indexs_approx_activ_rem, subset_dim)\n",
    "\n",
    "# Compute accuracy using the reconstruction from final embeddings, ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_embed_topic\n",
    "_, indexes_approx_final_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Subset)\")\n",
    "print_correct_elements(indexes_approx_final_rem, subset_dim)\n",
    "\n",
    "# Compute accuracy using the reconstruction from attention activations, also ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_attns_topic\n",
    "_, indexs_approx_activ_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Subset)\")\n",
    "print_correct_elements(indexs_approx_activ_rem, subset_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee21ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ddc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
