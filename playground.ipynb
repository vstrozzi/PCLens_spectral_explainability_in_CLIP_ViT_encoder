{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce479d0c-554a-42ee-b365-84a4d9ab81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "import matplotlib.lines as mlines\n",
    "import json\n",
    "from utils.misc.misc import accuracy, accuracy_correct\n",
    "from utils.scripts.algorithms_text_explanations import *\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.misc.visualization import visualization_preprocess\n",
    "from utils.models.prs_hook import hook_prs_logger\n",
    "from utils.datasets_constants.imagenet_classes import imagenet_classes\n",
    "from utils.datasets_constants.cifar_10_classes import cifar_10_classes\n",
    "from utils.datasets_constants.cub_classes import cub_classes, waterbird_classes\n",
    "import os\n",
    "from utils.scripts.algorithms_text_explanations import svd_data_approx\n",
    "from utils.datasets.dataset_helpers import dataset_to_dataloader\n",
    "from torch.nn import functional as F\n",
    "from utils.scripts.algorithms_text_explanations_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee675770-3be8-40bf-8659-31e2d2a811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cpu'\n",
    "model_name = 'ViT-B-32' # 'ViT-H-14'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = None\n",
    "tot_samples_per_class = None\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"binary_waterbirds\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "path = './datasets/'\n",
    "\n",
    "if model_name == \"ViT-H-14\":\n",
    "    pretrained = \"laion2B-s32B-b79K\"\n",
    "elif model_name == \"ViT-L-14\":\n",
    "    pretrained = \"laion2B-s32B-b82K\"\n",
    "elif model_name == \"ViT-B-16\":\n",
    "    pretrained = \"laion2B-s34B-b88K\"\n",
    "elif model_name == \"ViT-B-32\":\n",
    "    pretrained = \"laion2B-s34B-b79K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db3598-0d7d-47a4-b6c6-8d02f4902e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Model\n",
    "model, _, preprocess = create_model_and_transforms(model_name, pretrained=pretrained, cache_dir=\"../cache\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model.visual.transformer.resblocks))\n",
    "\n",
    "prs = hook_prs_logger(model, device, spatial=False) # This attach hook to get the residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f51611-710d-45b9-a797-85a958cc047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the chosen algorithm on a dataset to derive text explanations \n",
    "command = f\"python -m utils.scripts.compute_text_explanations --device {device} --model {model_name} --algorithm {algorithm} --seed {seed} --text_per_princ_comp 20 --num_of_last_layers {num_last_layers_} --text_descriptions {dataset_text_name}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef211e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new created attention datasets\n",
    "attention_dataset = f\"output_dir/{datataset_image_name}_completeness_{dataset_text_name}_{model_name}_algo_{algorithm}_seed_{seed}.jsonl\"\n",
    "\n",
    "# Load necessary data\n",
    "attns_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attn_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], attention values\n",
    "mlps_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlp_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], mlp values\n",
    "classifier_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_classifier_{model_name}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], embedding of the labels\n",
    "labels_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_labels_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # Position of the labels in the cosndiered dataset\n",
    "final_embeddings_images = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_embeddings_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))\n",
    "final_embeddings_texts = torch.tensor(np.load(f\"output_dir/{dataset_text_name}_{model_name}.npy\", mmap_mode=\"r\"))\n",
    "with open( f\"utils/text_descriptions/{dataset_text_name}.txt\", \"r\") as f:\n",
    "    texts_str = np.array([i.replace(\"\\n\", \"\") for i in f.readlines()])\n",
    "# Get mean ablation\n",
    "no_heads_attentions_ = attns_.sum(axis=(2))  # Sum over heads dimension\n",
    "last_ = attns_.shape[1] - num_last_layers_\n",
    "# Replace attention activations until 'last' layer with their average, while keeping later layers intact.\n",
    "current_mean_ablation_per_head_sum_ = torch.mean(no_heads_attentions_[:, :last_ + 1], axis=0).sum(0)\n",
    "\n",
    "# Save important stuff\n",
    "nr_layers_ = attns_.shape[1]\n",
    "nr_heads_ = attns_.shape[2]\n",
    "\n",
    "if datataset_image_name == \"imagenet\":\n",
    "    ds_ = ImageNet(root=path+\"imagenet/\", split=\"val\", transform=visualization_preprocess)\n",
    "elif datataset_image_name == \"binary_waterbirds\":\n",
    "    ds_ = BinaryWaterbirds(root=path+\"waterbird_complete95_forest2water2/\", split=\"test\", transform=visualization_preprocess)\n",
    "elif datataset_image_name == \"CIFAR100\":\n",
    "    ds_ = CIFAR100(\n",
    "        root=path, download=True, train=False, transform=visualization_preprocess\n",
    "    )\n",
    "elif datataset_image_name == \"CIFAR10\":\n",
    "    ds_ = CIFAR10(\n",
    "        root=path, download=True, train=False, transform=visualization_preprocess\n",
    "    )\n",
    "else:\n",
    "    ds_ = ImageFolder(root=path, transform=visualization_preprocess)\n",
    "\n",
    "classes_ = {\n",
    "        'imagenet': imagenet_classes, \n",
    "        'CIFAR10': cifar_10_classes,\n",
    "        'waterbirds': cub_classes, \n",
    "        'binary_waterbirds': waterbird_classes, \n",
    "        'cub': cub_classes}[datataset_image_name]\n",
    "# Depending\n",
    "ds_vis_ = dataset_subset(\n",
    "    ds_,\n",
    "    samples_per_class=subset_dim,\n",
    "    tot_samples_per_class=tot_samples_per_class,  # or whatever you prefer\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Print metadata accuracy if waterbird\n",
    "if classes_ == waterbird_classes:\n",
    "    root = \"datasets/waterbird_complete95_forest2water2/\"\n",
    "    df = pd.read_csv(root + \"metadata.csv\")\n",
    "    filtered_df = df[df['split'] == 2]\n",
    "\n",
    "    s = [(os.path.join(root, filtered_df.iloc[i]['img_filename']), filtered_df.iloc[i]['y'], filtered_df.iloc[i]['place']) for i in range(len(filtered_df))]\n",
    "    background_groups_ = list([x[2] for x in s])\n",
    "\n",
    "# Retrieve Rank\n",
    "data = get_data(attention_dataset, skip_final=True)\n",
    "mean_rank_ = 0\n",
    "for entry in data:\n",
    "    mean_rank_ += entry[\"rank\"]\n",
    "mean_rank_ /= len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d1202",
   "metadata": {},
   "source": [
    "# Print the top Principal Components text-interpretation for each Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "min_princ_comp = 10\n",
    "\n",
    "# Read JSON lines from attention_dataset\n",
    "# This file contains data about layers, heads, and their principal components (PCs) with associated metrics.\n",
    "data = get_data(attention_dataset, -1)\n",
    "    \n",
    "# Print the data in a nice formatted table\n",
    "print_data(data, min_princ_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea667e63",
   "metadata": {},
   "source": [
    "# Strongest Principal Components per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top entries to retrieve\n",
    "top_k = 10\n",
    "min_heap = []\n",
    "\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Sort data entries in descending order of strength_abs of the princial component\n",
    "top_k_entries = top_data(sort_data_by(data, \"strength_abs\", descending=True), top_k=top_k)\n",
    "\n",
    "# Print the top_k entries in a nice formatted table\n",
    "print_used_heads(top_k_entries)\n",
    "print_data(top_k_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71330301",
   "metadata": {},
   "source": [
    "# Visualize singular values of a principal component (both text and images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d778ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info on data\n",
    "layer = 11\n",
    "head = 11\n",
    "princ_comp = 0\n",
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 0  # Length of continuous elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e06794",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPT. Visualize textSpan\n",
    "attention_dataset_ts = f\"output_dir/{datataset_image_name}_completeness_{dataset_text_name}_{model_name}_algo_text_span_seed_{seed}.jsonl\"\n",
    "\n",
    "visualize_text_span(layer, head, attention_dataset_ts, top_k= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874902eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_principal_component(layer, head, princ_comp, nr_top_imgs, nr_worst_imgs, nr_cont_imgs, attention_dataset, final_embeddings_images, final_embeddings_texts, seed, path, texts_str, dataset=datataset_image_name, samples_per_class=subset_dim, tot_samples_per_class=tot_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCs strength\n",
    "data = get_data(attention_dataset)\n",
    "plot_pc_sv(data, layer, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25a1a4",
   "metadata": {},
   "source": [
    "# Query a topic or image and NNs on that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c66cf",
   "metadata": {},
   "source": [
    "### Define the query and analyze each Principal Component and derive a strength metric for reconstruction of the query-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b8153-73a3-4f30-bc1d-eddef413df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode so that no gradients are computed\n",
    "model.eval()\n",
    "query_text = True\n",
    "\n",
    "# Retrieve an embedding\n",
    "with torch.no_grad():\n",
    "    if query_text:\n",
    "        # If querying by text, define a text prompt and encode it into an embedding\n",
    "        text_query = \"An image of a woman\"\n",
    "        # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "        text_query_token = tokenizer(text_query).to(device)  \n",
    "        # Encode the tokenized text into a normalized embedding\n",
    "        topic_emb = model.encode_text(text_query_token, normalize=True)\n",
    "    else:\n",
    "        # If querying by image, load and preprocess the image from disk\n",
    "        prs.reinit()  # Reinitialize any hooks if required\n",
    "        text_query = \"woman.png\"\n",
    "        image_pil = Image.open(f'images/{text_query}')\n",
    "        image = preprocess(image_pil)[np.newaxis, :, :, :]  # Add batch dimension\n",
    "        # Encode the image into a normalized embedding\n",
    "        topic_emb = model.encode_image(\n",
    "            image.to(device), \n",
    "            attn_method='head_no_spatial',\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "### Reconstruct embedding and find contributions from principal components\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0).to(device)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0).to(device)\n",
    "\n",
    "# Mean center the embeddings\n",
    "mean_final = mean_final_texts if query_text else mean_final_images\n",
    "\n",
    "# Mean center the embeddings\n",
    "topic_emb_cent = topic_emb - mean_final\n",
    "final_embeddings_texts_cent = final_embeddings_texts.to(device) - mean_final_texts\n",
    "# Recontruct embedding\n",
    "[topic_emb_rec_cent], data = reconstruct_embeddings(data, [topic_emb_cent], [\"text\" if query_text else \"image\"], return_princ_comp=True, plot=True, means=[mean_final], device=device)\n",
    "\n",
    "# Normalize the embeddings\n",
    "topic_emb_rec_cent_norm = topic_emb_rec_cent / topic_emb_rec_cent.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# The maximum reconstruction score is how close the reconstructed embedding is to the original,\n",
    "# adjusted by the baseline score. This gives a cosine similarity measure.\n",
    "topic_emb_cent_norm = topic_emb_cent / topic_emb_cent.norm(dim=-1, keepdim=True)\n",
    "max_reconstr_score = topic_emb_rec_cent_norm @ topic_emb_cent_norm.T\n",
    "# Print out the cosine similarity between the original and reconstructed embeddings\n",
    "print(f\"We have a max cosine similarity of: {(max_reconstr_score).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3bb4b",
   "metadata": {},
   "source": [
    "### Use the strength of the previous reconstruction to derive a good enough reconstruction of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant details from the top-k principal component entries based on the reconstruction of the query\n",
    "top_k = 25  # Maximum number of top entries to retrieve\n",
    "approx = 1.1  # Target approximation threshold for the reconstruction quality\n",
    "\n",
    "# Initialize a tensor to accumulate the reconstructed topic embedding from selected principal components\n",
    "topic_emb_rec_act = torch.zeros_like(topic_emb)\n",
    "\n",
    "### Extract relevant details from the top k entries\n",
    "data = sort_data_by(data, \"correlation_princ_comp_abs\", descending=True) \n",
    "\n",
    "top_k_entries = top_data(data, top_k)\n",
    "\n",
    "top_k_details = reconstruct_top_embedding(top_k_entries, topic_emb_cent, mean_final, \"text\" if query_text else \"image\", max_reconstr_score, top_k, approx, device=device)\n",
    "# Convert the collected principal component details into a DataFrame for easy processing\n",
    "print(f\"Currently querying the topic: {text_query}\")\n",
    "print_data(top_k_details, is_corr_present=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cea6d",
   "metadata": {},
   "source": [
    "### Prepare scores of images and texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Reconstructed Embedding\n",
    "# Visualize ds\n",
    "# Initialize arrays to store the top and lowest scores based on similarity with original query\n",
    "scores_array_images = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Initialize arrays to store the top and lowest scores based on similarity with self reconstructed query\n",
    "scores_array_images_self = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Create arrays of indexes for referencing images and texts.\n",
    "indexes_images = np.arange(0, final_embeddings_images.shape[0], 1) \n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0).to(device)\n",
    "\n",
    "\n",
    "# Recontruct embedding\n",
    "images_rec = reconstruct_all_embeddings_mean_ablation_pcs(top_k_entries, mlps_, attns_, attns_, nr_layers_, nr_heads_, last_, ratio=-1, ablation=True, mean_ablate_all=True)\n",
    "\n",
    "\n",
    "# Compute the similarity scores between the reconstructed embeddings (images or texts) and the original query embedding.\n",
    "# The dot product gives a similarity measure, which we store in the scores arrays along with the index.\n",
    "# We do NOT normalize the score.\n",
    "# Compute scores for images\n",
    "\n",
    "scores_array_images[\"score\"] = (images_rec @ topic_emb.T).squeeze().cpu().numpy()\n",
    "scores_array_images_self[\"score\"] = (torch.diag(images_rec @ final_embeddings_images.T)).squeeze().cpu().numpy()\n",
    "\n",
    "images_rec /= images_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "scores_array_images[\"score_vis\"] = (images_rec @ topic_emb.T).squeeze().cpu().numpy()\n",
    "scores_array_images_self[\"score_vis\"] = (torch.diag(images_rec @ final_embeddings_images.T)).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_images[\"img_index\"] = indexes_images\n",
    "scores_array_images_self[\"img_index\"] = indexes_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For full CLIP Embedding\n",
    "# Scores array of real CLIP embeddings\n",
    "scores_array_images_full = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Compute scores for images\n",
    "images = final_embeddings_images.to(device)\n",
    "scores_array_images_full[\"score\"] = (images @ topic_emb.T).squeeze().cpu().numpy()\n",
    "\n",
    "images /= images.norm(dim=-1, keepdim=True)\n",
    "scores_array_images_full[\"score_vis\"] = (images @ topic_emb.T).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_images_full[\"img_index\"] = indexes_images\n",
    "\n",
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 20  # Length of continuous elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae29f",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 0  # Length of continuous elements\n",
    "\n",
    "dbs = create_dbs(scores_array_images, None, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "# Hardcoded visualizations\n",
    "nrs_dbs = [nr_top_imgs, nr_worst_imgs, nr_cont_imgs]\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis_, texts_str, classes_, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full Embeddings similarity\n",
    "dbs = create_dbs(scores_array_images_full, None, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis_, texts_str, classes_, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "dbs = create_dbs(scores_array_images_self, None, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis_, texts_str, classes_, text_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba18dc",
   "metadata": {},
   "source": [
    "### Opt, visualize using all NOT selected PCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1534854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get other PCs\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "top_k_other_details = get_remaining_pcs(data, top_k_entries)\n",
    "\n",
    "# Recontruct embedding\n",
    "\n",
    "images_rec = reconstruct_all_embeddings_mean_ablation_pcs(top_k_other_details, mlps_, attns_, attns_, nr_layers_, nr_heads_, last_, ratio=-1, mean_ablate_all=True)\n",
    "\n",
    "# Compute the similarity scores between the reconstructed embeddings (images or texts) and the original query embedding.\n",
    "# The dot product gives a similarity measure, which we store in the scores arrays along with the index.\n",
    "# We do NOT normalize the score.\n",
    "# Compute scores for images\n",
    "\n",
    "scores_array_images[\"score\"] = (images_rec @ topic_emb.T).squeeze().cpu().numpy()\n",
    "scores_array_images_self[\"score\"] = (torch.diag(images_rec @ final_embeddings_images.T)).squeeze().cpu().numpy()\n",
    "\n",
    "images_rec /= images_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "scores_array_images[\"score_vis\"] = (images_rec @ topic_emb.T).squeeze().cpu().numpy()\n",
    "scores_array_images_self[\"score_vis\"] = (torch.diag(images_rec @ final_embeddings_images.T)).squeeze().cpu().numpy()\n",
    "\n",
    "scores_array_images[\"img_index\"] = indexes_images\n",
    "scores_array_images_self[\"img_index\"] = indexes_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76197a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = create_dbs(scores_array_images, None, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "# Hardcoded visualizations\n",
    "nrs_dbs = [nr_top_imgs, nr_worst_imgs, nr_cont_imgs]\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis_, texts_str, classes_, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "dbs = create_dbs(scores_array_images_self, None, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "dbs_new = []\n",
    "for i, db in enumerate(dbs):\n",
    "    if nrs_dbs[i] == 0:\n",
    "        continue\n",
    "    dbs_new.append(db)\n",
    "visualize_dbs(top_k_details, dbs_new, ds_vis_, texts_str, classes_, text_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7efe8e",
   "metadata": {},
   "source": [
    "# Evaluate classification using reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a7422",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_layers = attns_.shape[1]\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "accs = []\n",
    "for layer_nr in range(nr_layers):\n",
    "    current_mean_ablation_per_head_sum = torch.mean(no_heads_attentions_[:, :layer_nr], axis=0).sum(0)\n",
    "    current_model = (current_mean_ablation_per_head_sum  + no_heads_attentions_[:, layer_nr + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "    acc, _ = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {nr_layers - layer_nr} to {nr_layers}\")\n",
    "    accs.append(acc)\n",
    "\n",
    "# Create an x-axis that has one increment for each element in acc\n",
    "x_values = range(len(accs))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, accs, linestyle='-', label=model_name)\n",
    "\n",
    "# Labeling\n",
    "plt.xlabel(\"Accumulated mean-ablated layers\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Accumulated Mean-Ablated Layers\")\n",
    "\n",
    "# Add legend for the line\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f9d05",
   "metadata": {},
   "source": [
    "## Proof of concept 1: Use aggregations of PCs of all labels at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40262dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_per_class_start = 1\n",
    "pcs_per_class_end = int(nr_heads_*num_last_layers_*mean_rank_)\n",
    "pcs_per_class_step = 10\n",
    "max_pcs_per_head = -1\n",
    "random = False\n",
    "class_embeddings = classifier_.T  # M x D\n",
    "\n",
    "\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "baseline_acc, idxs = test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "output = print_correct_elements(idxs, labels_, classes_)  \n",
    "\n",
    "if classes_ == waterbird_classes:\n",
    "    baseline_worst = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "\n",
    "# Using the knwoledge of which class are we predicting wrongly, give more or less weight to pcs per class\n",
    "# Reconstruct embeddings for each class label\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "classes_centered = class_embeddings - mean_final_texts.unsqueeze(0)\n",
    "\n",
    "# Initialize a (num_images x 2) array to track:\n",
    "#   [best_score_so_far, class_index_for_that_score]\n",
    "all_preds = torch.zeros((final_embeddings_images.shape[0], 2), dtype=torch.double)\n",
    "\n",
    "sorted_data = []\n",
    "for text_idx in range(classes_centered.shape[0]):\n",
    "    # Perform query system on entry\n",
    "    concept_i_centered = classes_centered[text_idx, :].unsqueeze(0)\n",
    "\n",
    "    data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "\n",
    "    _, data_abs = reconstruct_embeddings(\n",
    "        data, \n",
    "        [concept_i_centered], \n",
    "        [\"text\"], \n",
    "        return_princ_comp=True, \n",
    "        plot=False, \n",
    "        means=[mean_final_texts],\n",
    "    )\n",
    "\n",
    "    # Extract relevant details from the top k entries\n",
    "    data_pcs = sort_data_by(data_abs, \"correlation_princ_comp_abs\", descending=True)\n",
    "    # Derive nr_pcs_per_class\n",
    "    sorted_data.append(data_pcs)\n",
    "\n",
    "pcs_per_class_max_worst_acc = pcs_per_class_start\n",
    "max_worst_acc = 0\n",
    "\n",
    "worst_class_acc = []\n",
    "worst_class_nr_pcs = []\n",
    "\n",
    "total_accuracy = []  # Will store the average accuracy across all text_idx for each pcs_per_class\n",
    "\n",
    "for pcs_per_class in range(pcs_per_class_start, pcs_per_class_end, pcs_per_class_step):\n",
    "    entries = []\n",
    "    temp_accs = []  # temporary list to store accuracy for each text_idx\n",
    "\n",
    "    # Collect top_k_entries for each concept\n",
    "    for text_idx in range(classes_centered.shape[0]):\n",
    "        # Retrieve data\n",
    "        data_pcs = sorted_data[text_idx]\n",
    "        top_k_entries = top_data(data_pcs, pcs_per_class)\n",
    "        print(f\"Currently processing label {text_idx} with nr_pcs_per_class: {pcs_per_class}\")\n",
    "        entries += top_k_entries\n",
    "\n",
    "    # Remove duplicates\n",
    "    entries_set = []\n",
    "    entries_meta = []\n",
    "    for entry in entries:\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        princ_comp = entry[\"princ_comp\"]\n",
    "        if (layer, head, princ_comp) not in entries_meta:\n",
    "            entries_meta.append((layer, head, princ_comp))\n",
    "            entries_set.append(entry)\n",
    "\n",
    "    print(f\"Total number of unique entries: {len(entries_set)}\")\n",
    "\n",
    "    # If `random` is True, randomly pick PCs instead of the actual top_k\n",
    "    if random:\n",
    "        data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "        entries_set = random_pcs(data, pcs_per_class * len(classes_))\n",
    "\n",
    "    # Reconstruct final_embeddings_images\n",
    "    reconstructed_images = reconstruct_all_embeddings_mean_ablation_pcs(\n",
    "        entries_set,\n",
    "        mlps_,\n",
    "        attns_,\n",
    "        attns_,\n",
    "        nr_layers_,\n",
    "        nr_heads_,\n",
    "        num_last_layers_,\n",
    "        ratio=-1,\n",
    "        mean_ablate_all=False\n",
    "    )\n",
    "\n",
    "    reconstructed_images /= reconstructed_images.norm(dim=-1, keepdim=True)\n",
    "    predictions = reconstructed_images @ classifier_\n",
    "\n",
    "    # Evaluate across all text_idx\n",
    "    # (If you have different labels per text_idx, adapt accordingly.)\n",
    "    # For simplicity, assume `test_accuracy` returns (acc, idxs) for the entire set.\n",
    "    acc, idxs = test_accuracy(predictions, labels_, label=\"All classes combined\")\n",
    "    total_accuracy.append(acc)\n",
    "\n",
    "    # If you need to evaluate `acc` separately for each text_idx, do so in the loop above.\n",
    "    # Then store the average or final result below.\n",
    "\n",
    "    # You might also be printing correctness here:\n",
    "    print_correct_elements(idxs, labels_, classes_)\n",
    "\n",
    "    # Worst-class accuracy for Waterbirds, if applicable\n",
    "    if classes_ == waterbird_classes:\n",
    "        curr_worst_acc = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "        if curr_worst_acc > max_worst_acc:\n",
    "            max_worst_acc = curr_worst_acc\n",
    "            pcs_per_class_max_worst_acc = pcs_per_class\n",
    "\n",
    "        worst_class_acc.append(curr_worst_acc)\n",
    "        worst_class_nr_pcs.append(pcs_per_class)\n",
    "\n",
    "# ---------------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# ---------------------------------------\n",
    "\n",
    "# Suppose these are already known/computed:\n",
    "max_worst_acc = max(worst_class_acc)\n",
    "pcs_per_class_max_worst_acc = worst_class_nr_pcs[np.argmax(worst_class_acc)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the main data\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, worst_class_acc, color='blue', \n",
    "    linestyle='-', \n",
    "    label='Worst-Class Accuracy'\n",
    ")\n",
    "\n",
    "# Baseline line\n",
    "plt.axhline(\n",
    "    y=baseline_worst, \n",
    "    color='gray', \n",
    "    linestyle='--', \n",
    "    linewidth=2, \n",
    "    label=f'Baseline Worst-Class Acc = {baseline_worst:.2f}' \n",
    ")\n",
    "\n",
    "# Horizontal line for max\n",
    "plt.axhline(\n",
    "    y=max_worst_acc, \n",
    "    color='blue', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Vertical line for max\n",
    "plt.axvline(\n",
    "    x=pcs_per_class_max_worst_acc, \n",
    "    color='blue', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Create a single custom legend entry for both max lines\n",
    "max_line_legend = mlines.Line2D(\n",
    "    [], [], \n",
    "    color='blue', \n",
    "    linestyle=':', \n",
    "    linewidth=2,\n",
    "    label=f'Max Worst-Class Acc = {max_worst_acc:.2f} at PCs={pcs_per_class_max_worst_acc}\\n(Worst total accuracy is {total_accuracy[np.argmax(worst_class_acc)]:.2f})'\n",
    ")\n",
    "\n",
    "# Collect existing handles and labels from the current axes\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Append the custom max-line legend entry\n",
    "handles.append(max_line_legend)\n",
    "labels.append(max_line_legend.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Worst Accuracy')\n",
    "plt.title('Worst-Class Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles, labels)  # Use your updated handles and labels\n",
    "plt.tight_layout()  # Ensures elements fit within the figure\n",
    "plt.savefig(f\"plt_1_worst_{model_name}.pdf\", bbox_inches='tight', format='pdf')  # Corrected format argument\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max worst accuracy of {max_worst_acc:.2f} found at {pcs_per_class_max_worst_acc} PCs/class.\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# PLOTTING SECTION for TOTAL ACCURACY\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Suppose these are already known/computed:\n",
    "max_total_acc = max(total_accuracy)\n",
    "pcs_for_max_total_acc = worst_class_nr_pcs[np.argmax(total_accuracy)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the main data\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, total_accuracy, \n",
    "    linestyle='-', color='orange', \n",
    "    label='Total Accuracy'\n",
    ")\n",
    "\n",
    "# Baseline line\n",
    "plt.axhline(\n",
    "    y=baseline_acc, \n",
    "    color='gray', \n",
    "    linestyle='--', \n",
    "    linewidth=2, \n",
    "    label=f'Baseline Total Acc = {baseline_acc:.2f}'\n",
    ")\n",
    "\n",
    "# Horizontal line for the max total accuracy\n",
    "plt.axhline(\n",
    "    y=max_total_acc, \n",
    "    color='orange', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Vertical line for the max total accuracy\n",
    "plt.axvline(\n",
    "    x=pcs_for_max_total_acc, \n",
    "    color='orange', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Create a single custom legend entry for both max lines\n",
    "max_line_legend_2 = mlines.Line2D(\n",
    "    [], [], \n",
    "    color='orange', \n",
    "    linestyle=':', \n",
    "    linewidth=2,\n",
    "    label=f'Max Total Acc = {max_total_acc:.2f} at PCs={pcs_for_max_total_acc}\\n(Worst class accuracy is {worst_class_acc[np.argmax(total_accuracy)]:.2f})'\n",
    ")\n",
    "\n",
    "# Collect existing handles/labels\n",
    "handles2, labels2 = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Append the custom line\n",
    "handles2.append(max_line_legend_2)\n",
    "labels2.append(max_line_legend_2.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Total Accuracy (Avg)')\n",
    "plt.title('Total Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles2, labels2)\n",
    "plt.tight_layout()  # Ensures elements fit within the figure\n",
    "plt.savefig(f\"plt_1_acc_{model_name}.pdf\", bbox_inches='tight', format='pdf')  # Corrected format argument\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max total accuracy of {max_total_acc:.2f} found at {pcs_for_max_total_acc}\")# ---------------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# ---------------------------------------\n",
    "# -------------------------------\n",
    "# COMPUTE PARETO OPTIMAL CANDIDATE\n",
    "# -------------------------------\n",
    "# A candidate is one where both total accuracy and worst-class accuracy exceed their baselines.\n",
    "# Among these, we select the one with the highest worst-class accuracy.\n",
    "# If there is a tie, we pick the candidate with the higher total accuracy.\n",
    "total_accuracy_arr = np.array(total_accuracy)\n",
    "worst_class_acc_arr = np.array(worst_class_acc)\n",
    "worst_class_nr_pcs_arr = np.array(worst_class_nr_pcs)\n",
    "\n",
    "candidates_mask = (total_accuracy_arr > baseline_acc) & (worst_class_acc_arr > baseline_worst)\n",
    "if np.any(candidates_mask):\n",
    "    candidate_indices = np.where(candidates_mask)[0]\n",
    "    best_idx = candidate_indices[0]\n",
    "    for idx in candidate_indices:\n",
    "        if worst_class_acc_arr[idx] > worst_class_acc_arr[best_idx]:\n",
    "            best_idx = idx\n",
    "        elif worst_class_acc_arr[idx] == worst_class_acc_arr[best_idx] and total_accuracy_arr[idx] > total_accuracy_arr[best_idx]:\n",
    "            best_idx = idx\n",
    "    pareto_pcs = worst_class_nr_pcs_arr[best_idx]\n",
    "    pareto_worst = worst_class_acc_arr[best_idx]\n",
    "    pareto_total = total_accuracy_arr[best_idx]\n",
    "else:\n",
    "    pareto_pcs = None\n",
    "\n",
    "# -------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# -------------------------------\n",
    "# (Only applicable if classes_ == waterbird_classes)\n",
    "if classes_ == waterbird_classes:\n",
    "    # Recompute max values for worst-class accuracy for clarity\n",
    "    max_worst_acc = max(worst_class_acc)\n",
    "    pcs_per_class_max_worst_acc = worst_class_nr_pcs[np.argmax(worst_class_acc)]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Plot the worst-class accuracy curve\n",
    "    plt.plot(\n",
    "        worst_class_nr_pcs, worst_class_acc,\n",
    "        color='blue',\n",
    "        linestyle='-',\n",
    "        label='Worst-Class Accuracy'\n",
    "    )\n",
    "    # Plot baseline worst-class accuracy\n",
    "    plt.axhline(\n",
    "        y=baseline_worst,\n",
    "        color='gray',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label=f'Baseline Worst-Class Acc = {baseline_worst:.2f}'\n",
    "    )\n",
    "    # Highlight the maximum worst-class accuracy with horizontal and vertical lines\n",
    "    plt.axhline(\n",
    "        y=max_worst_acc,\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=pcs_per_class_max_worst_acc,\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2\n",
    "    )\n",
    "    # --- Add Vertical Line for Pareto Optimal Solution (if found) ---\n",
    "    if pareto_pcs is not None:\n",
    "        plt.axvline(\n",
    "            x=pareto_pcs,\n",
    "            color='green',\n",
    "            linestyle='--',\n",
    "            linewidth=2\n",
    "        )\n",
    "        pareto_line_legend = mlines.Line2D(\n",
    "            [],\n",
    "            [],\n",
    "            color='green',\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=f'Pareto Optimal: Worst Acc = {pareto_worst:.2f}, Total Acc = {pareto_total:.2f} at PCs={pareto_pcs}'\n",
    "        )\n",
    "    else:\n",
    "        pareto_line_legend = None\n",
    "\n",
    "    # Custom legend entry for max worst-class accuracy\n",
    "    max_line_legend = mlines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2,\n",
    "        label=f'Max Worst-Class Acc = {max_worst_acc:.2f} at PCs={pcs_per_class_max_worst_acc}\\n(Worst total accuracy is {total_accuracy[np.argmax(worst_class_acc)]:.2f})'\n",
    "    )\n",
    "\n",
    "    # Get current legend handles and append our custom entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    handles.append(max_line_legend)\n",
    "    labels.append(max_line_legend.get_label())\n",
    "    if pareto_line_legend is not None:\n",
    "        handles.append(pareto_line_legend)\n",
    "        labels.append(pareto_line_legend.get_label())\n",
    "\n",
    "    plt.xlabel('Number of PCs per Class')\n",
    "    plt.ylabel('Worst Accuracy')\n",
    "    plt.title('Worst-Class Accuracy vs. Number of PCs per Class')\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plt_1worst_{model_name}.pdf\", bbox_inches='tight', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Max worst accuracy of {max_worst_acc:.2f} found at {pcs_per_class_max_worst_acc} PCs/class.\")\n",
    "    if pareto_pcs is not None:\n",
    "        print(f\"Pareto optimal solution at {pareto_pcs} PCs/class with Worst Acc = {pareto_worst:.2f} and Total Acc = {pareto_total:.2f}.\")\n",
    "\n",
    "# -------------------------------\n",
    "# PLOTTING SECTION for TOTAL ACCURACY\n",
    "# -------------------------------\n",
    "# Compute max total accuracy details\n",
    "max_total_acc = max(total_accuracy)\n",
    "pcs_for_max_total_acc = worst_class_nr_pcs[np.argmax(total_accuracy)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Plot the total accuracy curve\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, total_accuracy,\n",
    "    linestyle='-', color='orange',\n",
    "    label='Total Accuracy'\n",
    ")\n",
    "# Plot baseline total accuracy\n",
    "plt.axhline(\n",
    "    y=baseline_acc,\n",
    "    color='gray',\n",
    "    linestyle='--',\n",
    "    linewidth=2,\n",
    "    label=f'Baseline Total Acc = {baseline_acc:.2f}'\n",
    ")\n",
    "# Highlight the maximum total accuracy with horizontal and vertical lines\n",
    "plt.axhline(\n",
    "    y=max_total_acc,\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.axvline(\n",
    "    x=pcs_for_max_total_acc,\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2\n",
    ")\n",
    "# --- Add Vertical Line for Pareto Optimal Solution (if found) ---\n",
    "if pareto_pcs is not None:\n",
    "    plt.axvline(\n",
    "        x=pareto_pcs,\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2\n",
    "    )\n",
    "    pareto_line_legend_2 = mlines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label=f'Pareto Optimal: Worst Acc = {pareto_worst:.2f}, Total Acc = {pareto_total:.2f} at PCs={pareto_pcs}'\n",
    "    )\n",
    "else:\n",
    "    pareto_line_legend_2 = None\n",
    "\n",
    "# Custom legend entry for max total accuracy\n",
    "max_line_legend_2 = mlines.Line2D(\n",
    "    [],\n",
    "    [],\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2,\n",
    "    label=f'Max Total Acc = {max_total_acc:.2f} at PCs={pcs_for_max_total_acc}\\n(Worst class accuracy is {worst_class_acc[np.argmax(total_accuracy)]:.2f})'\n",
    ")\n",
    "\n",
    "handles2, labels2 = plt.gca().get_legend_handles_labels()\n",
    "handles2.append(max_line_legend_2)\n",
    "labels2.append(max_line_legend_2.get_label())\n",
    "if pareto_line_legend_2 is not None:\n",
    "    handles2.append(pareto_line_legend_2)\n",
    "    labels2.append(pareto_line_legend_2.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Total Accuracy (Avg)')\n",
    "plt.title('Total Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles2, labels2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plt_1_acc_{model_name}.pdf\", bbox_inches='tight', format='pdf')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max total accuracy of {max_total_acc:.2f} found at {pcs_for_max_total_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9729043",
   "metadata": {},
   "source": [
    "## Proof of concept 2: Remove concepts from the model and use other PCs to reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_per_class_start = 1\n",
    "pcs_per_class_end = int(nr_heads_*num_last_layers_*mean_rank_)\n",
    "pcs_per_class_step = 10\n",
    "max_pcs_per_head = -1\n",
    "random = False\n",
    "class_embeddings = classifier_.T  # M x D\n",
    "concepts_to_remove = [\"water background\", \"land background\"]\n",
    "\n",
    "# Derive embedding:\n",
    "for k, concept in enumerate(concepts_to_remove):\n",
    "    # Retrieve an embedding\n",
    "    with torch.no_grad():\n",
    "        # If querying by text, define a text prompt and encode it into an embedding\n",
    "        # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "        text_query_token = tokenizer(concept).to(device)  \n",
    "        # Encode the tokenized text into a normalized embedding\n",
    "        topic_emb = model.encode_text(text_query_token, normalize=True)\n",
    "        if k == 0:\n",
    "            concepts_emb = torch.zeros(len(concepts_to_remove), topic_emb.shape[-1], device=device)\n",
    "        concepts_emb[k] = topic_emb\n",
    "\n",
    "# Print baseline accuracy\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "baseline_acc, idxs = test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "if classes_ == waterbird_classes:\n",
    "    baseline_worst = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "# Reconstruct embeddings for each class label\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "concepts_centered = concepts_emb - mean_final_texts.unsqueeze(0)\n",
    "\n",
    "# Initialize a (num_images x 2) array to track:\n",
    "#   [best_score_so_far, class_index_for_that_score]\n",
    "all_preds = torch.zeros((final_embeddings_images.shape[0], 2), dtype=torch.double)\n",
    "\n",
    "sorted_data = []\n",
    "for text_idx in range(concepts_centered.shape[0]):\n",
    "    # Perform query system on entry\n",
    "    concept_i_centered = concepts_centered[text_idx, :].unsqueeze(0)\n",
    "\n",
    "    data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "\n",
    "    _, data_abs = reconstruct_embeddings(\n",
    "        data, \n",
    "        [concept_i_centered], \n",
    "        [\"text\"], \n",
    "        return_princ_comp=True, \n",
    "        plot=False, \n",
    "        means=[mean_final_texts],\n",
    "    )\n",
    "\n",
    "    # Extract relevant details from the top k entries\n",
    "    data_pcs = sort_data_by(data_abs, \"correlation_princ_comp_abs\", descending=True)\n",
    "    # Derive nr_pcs_per_class\n",
    "    sorted_data.append(data_pcs)\n",
    "\n",
    "pcs_per_class_max_worst_acc = pcs_per_class_start\n",
    "max_worst_acc = 0\n",
    "\n",
    "worst_class_acc = []\n",
    "worst_class_nr_pcs = []\n",
    "\n",
    "total_accuracy = []  # Will store the average accuracy across all text_idx for each pcs_per_class\n",
    "\n",
    "for pcs_per_class in range(pcs_per_class_start, pcs_per_class_end, pcs_per_class_step):\n",
    "    entries = []\n",
    "    temp_accs = []  # temporary list to store accuracy for each text_idx\n",
    "\n",
    "    # Collect top_k_entries for each concept\n",
    "    for text_idx in range(concepts_centered.shape[0]):\n",
    "        # Retrieve data\n",
    "        data_pcs = sorted_data[text_idx]\n",
    "        top_k_entries = top_data(data_pcs, pcs_per_class)\n",
    "        print(f\"Currently processing label: {concepts_to_remove[text_idx]} with nr_pcs_per_class: {pcs_per_class}\")\n",
    "        entries += top_k_entries\n",
    "\n",
    "    # Remove duplicates\n",
    "    entries_set = []\n",
    "    entries_meta = []\n",
    "    for entry in entries:\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        princ_comp = entry[\"princ_comp\"]\n",
    "        if (layer, head, princ_comp) not in entries_meta:\n",
    "            entries_meta.append((layer, head, princ_comp))\n",
    "            entries_set.append(entry)\n",
    "\n",
    "    print(f\"Total number of unique entries: {len(entries_set)}\")\n",
    "\n",
    "    # Extract other components\n",
    "    top_k_other_details = get_remaining_pcs(data, entries_set)\n",
    "\n",
    "    # If `random` is True, randomly pick PCs instead of the actual top_k\n",
    "    if random:\n",
    "        data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "        top_k_other_details = random_pcs(data, pcs_per_class * len(classes_))\n",
    "\n",
    "    # Reconstruct final_embeddings_images\n",
    "    reconstructed_images = reconstruct_all_embeddings_mean_ablation_pcs(\n",
    "        top_k_other_details,\n",
    "        mlps_,\n",
    "        attns_,\n",
    "        attns_,\n",
    "        nr_layers_,\n",
    "        nr_heads_,\n",
    "        num_last_layers_,\n",
    "        ratio=-1,\n",
    "        mean_ablate_all=True\n",
    "    )\n",
    "\n",
    "    reconstructed_images /= reconstructed_images.norm(dim=-1, keepdim=True)\n",
    "    predictions = reconstructed_images @ classifier_\n",
    "\n",
    "    # Evaluate across all text_idx\n",
    "    # (If you have different labels per text_idx, adapt accordingly.)\n",
    "    # For simplicity, assume `test_accuracy` returns (acc, idxs) for the entire set.\n",
    "    acc, idxs = test_accuracy(predictions, labels_, label=\"All classes combined\")\n",
    "    total_accuracy.append(acc)\n",
    "\n",
    "    # If you need to evaluate `acc` separately for each text_idx, do so in the loop above.\n",
    "    # Then store the average or final result below.\n",
    "\n",
    "    # You might also be printing correctness here:\n",
    "    print_correct_elements(idxs, labels_, classes_)\n",
    "\n",
    "    # Worst-class accuracy for Waterbirds, if applicable\n",
    "    if classes_ == waterbird_classes:\n",
    "        curr_worst_acc = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "        if curr_worst_acc > max_worst_acc:\n",
    "            max_worst_acc = curr_worst_acc\n",
    "            pcs_per_class_max_worst_acc = pcs_per_class\n",
    "\n",
    "        worst_class_acc.append(curr_worst_acc)\n",
    "        worst_class_nr_pcs.append(pcs_per_class)\n",
    "\n",
    "# ---------------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# ---------------------------------------\n",
    "# -------------------------------\n",
    "# COMPUTE PARETO OPTIMAL CANDIDATE\n",
    "# -------------------------------\n",
    "# A candidate is one where both total accuracy and worst-class accuracy exceed their baselines.\n",
    "# Among these, we select the one with the highest worst-class accuracy.\n",
    "# If there is a tie, we pick the candidate with the higher total accuracy.\n",
    "total_accuracy_arr = np.array(total_accuracy)\n",
    "worst_class_acc_arr = np.array(worst_class_acc)\n",
    "worst_class_nr_pcs_arr = np.array(worst_class_nr_pcs)\n",
    "\n",
    "candidates_mask = (total_accuracy_arr > baseline_acc) & (worst_class_acc_arr > baseline_worst)\n",
    "if np.any(candidates_mask):\n",
    "    candidate_indices = np.where(candidates_mask)[0]\n",
    "    best_idx = candidate_indices[0]\n",
    "    for idx in candidate_indices:\n",
    "        if worst_class_acc_arr[idx] > worst_class_acc_arr[best_idx]:\n",
    "            best_idx = idx\n",
    "        elif worst_class_acc_arr[idx] == worst_class_acc_arr[best_idx] and total_accuracy_arr[idx] > total_accuracy_arr[best_idx]:\n",
    "            best_idx = idx\n",
    "    pareto_pcs = worst_class_nr_pcs_arr[best_idx]\n",
    "    pareto_worst = worst_class_acc_arr[best_idx]\n",
    "    pareto_total = total_accuracy_arr[best_idx]\n",
    "else:\n",
    "    pareto_pcs = None\n",
    "\n",
    "# -------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# -------------------------------\n",
    "# (Only applicable if classes_ == waterbird_classes)\n",
    "if classes_ == waterbird_classes:\n",
    "    # Recompute max values for worst-class accuracy for clarity\n",
    "    max_worst_acc = max(worst_class_acc)\n",
    "    pcs_per_class_max_worst_acc = worst_class_nr_pcs[np.argmax(worst_class_acc)]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Plot the worst-class accuracy curve\n",
    "    plt.plot(\n",
    "        worst_class_nr_pcs, worst_class_acc,\n",
    "        color='blue',\n",
    "        linestyle='-',\n",
    "        label='Worst-Class Accuracy'\n",
    "    )\n",
    "    # Plot baseline worst-class accuracy\n",
    "    plt.axhline(\n",
    "        y=baseline_worst,\n",
    "        color='gray',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label=f'Baseline Worst-Class Acc = {baseline_worst:.2f}'\n",
    "    )\n",
    "    # Highlight the maximum worst-class accuracy with horizontal and vertical lines\n",
    "    plt.axhline(\n",
    "        y=max_worst_acc,\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=pcs_per_class_max_worst_acc,\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2\n",
    "    )\n",
    "    # --- Add Vertical Line for Pareto Optimal Solution (if found) ---\n",
    "    if pareto_pcs is not None:\n",
    "        plt.axvline(\n",
    "            x=pareto_pcs,\n",
    "            color='green',\n",
    "            linestyle='--',\n",
    "            linewidth=2\n",
    "        )\n",
    "        pareto_line_legend = mlines.Line2D(\n",
    "            [],\n",
    "            [],\n",
    "            color='green',\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=f'Pareto Optimal: Worst Acc = {pareto_worst:.2f}, Total Acc = {pareto_total:.2f} at PCs={pareto_pcs}'\n",
    "        )\n",
    "    else:\n",
    "        pareto_line_legend = None\n",
    "\n",
    "    # Custom legend entry for max worst-class accuracy\n",
    "    max_line_legend = mlines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2,\n",
    "        label=f'Max Worst-Class Acc = {max_worst_acc:.2f} at PCs={pcs_per_class_max_worst_acc}\\n(Worst total accuracy is {total_accuracy[np.argmax(worst_class_acc)]:.2f})'\n",
    "    )\n",
    "\n",
    "    # Get current legend handles and append our custom entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    handles.append(max_line_legend)\n",
    "    labels.append(max_line_legend.get_label())\n",
    "    if pareto_line_legend is not None:\n",
    "        handles.append(pareto_line_legend)\n",
    "        labels.append(pareto_line_legend.get_label())\n",
    "\n",
    "    plt.xlabel('Number of PCs per Class')\n",
    "    plt.ylabel('Worst Accuracy')\n",
    "    plt.title('Worst-Class Accuracy vs. Number of PCs per Class')\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plt_2_worst_{model_name}.pdf\", bbox_inches='tight', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Max worst accuracy of {max_worst_acc:.2f} found at {pcs_per_class_max_worst_acc} PCs/class.\")\n",
    "    if pareto_pcs is not None:\n",
    "        print(f\"Pareto optimal solution at {pareto_pcs} PCs/class with Worst Acc = {pareto_worst:.2f} and Total Acc = {pareto_total:.2f}.\")\n",
    "\n",
    "# -------------------------------\n",
    "# PLOTTING SECTION for TOTAL ACCURACY\n",
    "# -------------------------------\n",
    "# Compute max total accuracy details\n",
    "max_total_acc = max(total_accuracy)\n",
    "pcs_for_max_total_acc = worst_class_nr_pcs[np.argmax(total_accuracy)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Plot the total accuracy curve\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, total_accuracy,\n",
    "    linestyle='-', color='orange',\n",
    "    label='Total Accuracy'\n",
    ")\n",
    "# Plot baseline total accuracy\n",
    "plt.axhline(\n",
    "    y=baseline_acc,\n",
    "    color='gray',\n",
    "    linestyle='--',\n",
    "    linewidth=2,\n",
    "    label=f'Baseline Total Acc = {baseline_acc:.2f}'\n",
    ")\n",
    "# Highlight the maximum total accuracy with horizontal and vertical lines\n",
    "plt.axhline(\n",
    "    y=max_total_acc,\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.axvline(\n",
    "    x=pcs_for_max_total_acc,\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2\n",
    ")\n",
    "# --- Add Vertical Line for Pareto Optimal Solution (if found) ---\n",
    "if pareto_pcs is not None:\n",
    "    plt.axvline(\n",
    "        x=pareto_pcs,\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2\n",
    "    )\n",
    "    pareto_line_legend_2 = mlines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label=f'Pareto Optimal: Worst Acc = {pareto_worst:.2f}, Total Acc = {pareto_total:.2f} at PCs={pareto_pcs}'\n",
    "    )\n",
    "else:\n",
    "    pareto_line_legend_2 = None\n",
    "\n",
    "# Custom legend entry for max total accuracy\n",
    "max_line_legend_2 = mlines.Line2D(\n",
    "    [],\n",
    "    [],\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2,\n",
    "    label=f'Max Total Acc = {max_total_acc:.2f} at PCs={pcs_for_max_total_acc}\\n(Worst class accuracy is {worst_class_acc[np.argmax(total_accuracy)]:.2f})'\n",
    ")\n",
    "\n",
    "handles2, labels2 = plt.gca().get_legend_handles_labels()\n",
    "handles2.append(max_line_legend_2)\n",
    "labels2.append(max_line_legend_2.get_label())\n",
    "if pareto_line_legend_2 is not None:\n",
    "    handles2.append(pareto_line_legend_2)\n",
    "    labels2.append(pareto_line_legend_2.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Total Accuracy (Avg)')\n",
    "plt.title('Total Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles2, labels2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plt_2_acc_{model_name}.pdf\", bbox_inches='tight', format='pdf')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max total accuracy of {max_total_acc:.2f} found at {pcs_for_max_total_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab504d4d",
   "metadata": {},
   "source": [
    "## Proof of concept 3: Use as main components of the model what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_per_class_start = 1\n",
    "pcs_per_class_end = int(nr_heads_*num_last_layers_*mean_rank_)\n",
    "pcs_per_class_step = 10\n",
    "max_pcs_per_head = -1\n",
    "random = False\n",
    "class_embeddings = classifier_.T  # M x D\n",
    "concepts_to_add = [\"feet shape\", \"beak shape\"]\n",
    "\n",
    "# Derive embedding:\n",
    "for k, concept in enumerate(concepts_to_add):\n",
    "    # Retrieve an embedding\n",
    "    with torch.no_grad():\n",
    "        # If querying by text, define a text prompt and encode it into an embedding\n",
    "        # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "        text_query_token = tokenizer(concept).to(device)  \n",
    "        # Encode the tokenized text into a normalized embedding\n",
    "        topic_emb = model.encode_text(text_query_token, normalize=True)\n",
    "        if k == 0:\n",
    "            concepts_emb = torch.zeros(len(concepts_to_add), topic_emb.shape[-1], device=device)\n",
    "        concepts_emb[k] = topic_emb\n",
    "\n",
    "# Print baseline accuracy\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "baseline_acc, idxs = test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "if classes_ == waterbird_classes:\n",
    "    baseline_worst = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "# Reconstruct embeddings for each class label\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "concepts_centered = concepts_emb - mean_final_texts.unsqueeze(0)\n",
    "\n",
    "# Initialize a (num_images x 2) array to track:\n",
    "#   [best_score_so_far, class_index_for_that_score]\n",
    "all_preds = torch.zeros((final_embeddings_images.shape[0], 2), dtype=torch.double)\n",
    "\n",
    "sorted_data = []\n",
    "for text_idx in range(concepts_centered.shape[0]):\n",
    "    # Perform query system on entry\n",
    "    concept_i_centered = concepts_centered[text_idx, :].unsqueeze(0)\n",
    "\n",
    "    data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "\n",
    "    _, data_abs = reconstruct_embeddings(\n",
    "        data, \n",
    "        [concept_i_centered], \n",
    "        [\"text\"], \n",
    "        return_princ_comp=True, \n",
    "        plot=False, \n",
    "        means=[mean_final_texts],\n",
    "    )\n",
    "\n",
    "    # Extract relevant details from the top k entries\n",
    "    data_pcs = sort_data_by(data_abs, \"correlation_princ_comp_abs\", descending=True)\n",
    "    # Derive nr_pcs_per_class\n",
    "    sorted_data.append(data_pcs)\n",
    "\n",
    "pcs_per_class_max_worst_acc = pcs_per_class_start\n",
    "max_worst_acc = 0\n",
    "\n",
    "worst_class_acc = []\n",
    "worst_class_nr_pcs = []\n",
    "\n",
    "total_accuracy = []  # Will store the average accuracy across all text_idx for each pcs_per_class\n",
    "\n",
    "for pcs_per_class in range(pcs_per_class_start, pcs_per_class_end, pcs_per_class_step):\n",
    "    entries = []\n",
    "    temp_accs = []  # temporary list to store accuracy for each text_idx\n",
    "\n",
    "    # Collect top_k_entries for each concept\n",
    "    for text_idx in range(concepts_centered.shape[0]):\n",
    "        # Retrieve data\n",
    "        data_pcs = sorted_data[text_idx]\n",
    "        top_k_entries = top_data(data_pcs, pcs_per_class)\n",
    "        print(f\"Currently processing label: {concepts_to_add[text_idx]} with nr_pcs_per_class: {pcs_per_class}\")\n",
    "        entries += top_k_entries\n",
    "\n",
    "    # Remove duplicates\n",
    "    entries_set = []\n",
    "    entries_meta = []\n",
    "    for entry in entries:\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        princ_comp = entry[\"princ_comp\"]\n",
    "        if (layer, head, princ_comp) not in entries_meta:\n",
    "            entries_meta.append((layer, head, princ_comp))\n",
    "            entries_set.append(entry)\n",
    "\n",
    "    print(f\"Total number of unique entries: {len(entries_set)}\")\n",
    "\n",
    "    # If `random` is True, randomly pick PCs instead of the actual top_k\n",
    "    if random:\n",
    "        data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "        entries_set = random_pcs(data, pcs_per_class * len(classes_))\n",
    "\n",
    "    # Reconstruct final_embeddings_images\n",
    "    reconstructed_images = reconstruct_all_embeddings_mean_ablation_pcs(\n",
    "        entries_set,\n",
    "        mlps_,\n",
    "        attns_,\n",
    "        attns_,\n",
    "        nr_layers_,\n",
    "        nr_heads_,\n",
    "        num_last_layers_,\n",
    "        ratio=-1,\n",
    "        mean_ablate_all=True\n",
    "    )\n",
    "\n",
    "    reconstructed_images /= reconstructed_images.norm(dim=-1, keepdim=True)\n",
    "    predictions = reconstructed_images @ classifier_\n",
    "\n",
    "    # Evaluate across all text_idx\n",
    "    # (If you have different labels per text_idx, adapt accordingly.)\n",
    "    # For simplicity, assume `test_accuracy` returns (acc, idxs) for the entire set.\n",
    "    acc, idxs = test_accuracy(predictions, labels_, label=\"All classes combined\")\n",
    "    total_accuracy.append(acc)\n",
    "\n",
    "    # If you need to evaluate `acc` separately for each text_idx, do so in the loop above.\n",
    "    # Then store the average or final result below.\n",
    "\n",
    "    # You might also be printing correctness here:\n",
    "    print_correct_elements(idxs, labels_, classes_)\n",
    "\n",
    "    # Worst-class accuracy for Waterbirds, if applicable\n",
    "    if classes_ == waterbird_classes:\n",
    "        curr_worst_acc = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "        if curr_worst_acc > max_worst_acc:\n",
    "            max_worst_acc = curr_worst_acc\n",
    "            pcs_per_class_max_worst_acc = pcs_per_class\n",
    "\n",
    "        worst_class_acc.append(curr_worst_acc)\n",
    "        worst_class_nr_pcs.append(pcs_per_class)\n",
    "\n",
    "# ---------------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# ---------------------------------------\n",
    "# -------------------------------\n",
    "# COMPUTE PARETO OPTIMAL CANDIDATE\n",
    "# -------------------------------\n",
    "# A candidate is one where both total accuracy and worst-class accuracy exceed their baselines.\n",
    "# Among these, we select the one with the highest worst-class accuracy.\n",
    "# If there is a tie, we pick the candidate with the higher total accuracy.\n",
    "total_accuracy_arr = np.array(total_accuracy)\n",
    "worst_class_acc_arr = np.array(worst_class_acc)\n",
    "worst_class_nr_pcs_arr = np.array(worst_class_nr_pcs)\n",
    "\n",
    "candidates_mask = (total_accuracy_arr > baseline_acc) & (worst_class_acc_arr > baseline_worst)\n",
    "if np.any(candidates_mask):\n",
    "    candidate_indices = np.where(candidates_mask)[0]\n",
    "    best_idx = candidate_indices[0]\n",
    "    for idx in candidate_indices:\n",
    "        if worst_class_acc_arr[idx] > worst_class_acc_arr[best_idx]:\n",
    "            best_idx = idx\n",
    "        elif worst_class_acc_arr[idx] == worst_class_acc_arr[best_idx] and total_accuracy_arr[idx] > total_accuracy_arr[best_idx]:\n",
    "            best_idx = idx\n",
    "    pareto_pcs = worst_class_nr_pcs_arr[best_idx]\n",
    "    pareto_worst = worst_class_acc_arr[best_idx]\n",
    "    pareto_total = total_accuracy_arr[best_idx]\n",
    "else:\n",
    "    pareto_pcs = None\n",
    "\n",
    "# -------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# -------------------------------\n",
    "# (Only applicable if classes_ == waterbird_classes)\n",
    "if classes_ == waterbird_classes:\n",
    "    # Recompute max values for worst-class accuracy for clarity\n",
    "    max_worst_acc = max(worst_class_acc)\n",
    "    pcs_per_class_max_worst_acc = worst_class_nr_pcs[np.argmax(worst_class_acc)]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Plot the worst-class accuracy curve\n",
    "    plt.plot(\n",
    "        worst_class_nr_pcs, worst_class_acc,\n",
    "        color='blue',\n",
    "        linestyle='-',\n",
    "        label='Worst-Class Accuracy'\n",
    "    )\n",
    "    # Plot baseline worst-class accuracy\n",
    "    plt.axhline(\n",
    "        y=baseline_worst,\n",
    "        color='gray',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label=f'Baseline Worst-Class Acc = {baseline_worst:.2f}'\n",
    "    )\n",
    "    # Highlight the maximum worst-class accuracy with horizontal and vertical lines\n",
    "    plt.axhline(\n",
    "        y=max_worst_acc,\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=pcs_per_class_max_worst_acc,\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2\n",
    "    )\n",
    "    # --- Add Vertical Line for Pareto Optimal Solution (if found) ---\n",
    "    if pareto_pcs is not None:\n",
    "        plt.axvline(\n",
    "            x=pareto_pcs,\n",
    "            color='green',\n",
    "            linestyle='--',\n",
    "            linewidth=2\n",
    "        )\n",
    "        pareto_line_legend = mlines.Line2D(\n",
    "            [],\n",
    "            [],\n",
    "            color='green',\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=f'Pareto Optimal: Worst Acc = {pareto_worst:.2f}, Total Acc = {pareto_total:.2f} at PCs={pareto_pcs}'\n",
    "        )\n",
    "    else:\n",
    "        pareto_line_legend = None\n",
    "\n",
    "    # Custom legend entry for max worst-class accuracy\n",
    "    max_line_legend = mlines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color='blue',\n",
    "        linestyle=':',\n",
    "        linewidth=2,\n",
    "        label=f'Max Worst-Class Acc = {max_worst_acc:.2f} at PCs={pcs_per_class_max_worst_acc}\\n(Worst total accuracy is {total_accuracy[np.argmax(worst_class_acc)]:.2f})'\n",
    "    )\n",
    "\n",
    "    # Get current legend handles and append our custom entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    handles.append(max_line_legend)\n",
    "    labels.append(max_line_legend.get_label())\n",
    "    if pareto_line_legend is not None:\n",
    "        handles.append(pareto_line_legend)\n",
    "        labels.append(pareto_line_legend.get_label())\n",
    "\n",
    "    plt.xlabel('Number of PCs per Class')\n",
    "    plt.ylabel('Worst Accuracy')\n",
    "    plt.title('Worst-Class Accuracy vs. Number of PCs per Class')\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plt_3_worst_{model_name}.pdf\", bbox_inches='tight', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Max worst accuracy of {max_worst_acc:.2f} found at {pcs_per_class_max_worst_acc} PCs/class.\")\n",
    "    if pareto_pcs is not None:\n",
    "        print(f\"Pareto optimal solution at {pareto_pcs} PCs/class with Worst Acc = {pareto_worst:.2f} and Total Acc = {pareto_total:.2f}.\")\n",
    "\n",
    "# -------------------------------\n",
    "# PLOTTING SECTION for TOTAL ACCURACY\n",
    "# -------------------------------\n",
    "# Compute max total accuracy details\n",
    "max_total_acc = max(total_accuracy)\n",
    "pcs_for_max_total_acc = worst_class_nr_pcs[np.argmax(total_accuracy)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Plot the total accuracy curve\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, total_accuracy,\n",
    "    linestyle='-', color='orange',\n",
    "    label='Total Accuracy'\n",
    ")\n",
    "# Plot baseline total accuracy\n",
    "plt.axhline(\n",
    "    y=baseline_acc,\n",
    "    color='gray',\n",
    "    linestyle='--',\n",
    "    linewidth=2,\n",
    "    label=f'Baseline Total Acc = {baseline_acc:.2f}'\n",
    ")\n",
    "# Highlight the maximum total accuracy with horizontal and vertical lines\n",
    "plt.axhline(\n",
    "    y=max_total_acc,\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.axvline(\n",
    "    x=pcs_for_max_total_acc,\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2\n",
    ")\n",
    "# --- Add Vertical Line for Pareto Optimal Solution (if found) ---\n",
    "if pareto_pcs is not None:\n",
    "    plt.axvline(\n",
    "        x=pareto_pcs,\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2\n",
    "    )\n",
    "    pareto_line_legend_2 = mlines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label=f'Pareto Optimal: Worst Acc = {pareto_worst:.2f}, Total Acc = {pareto_total:.2f} at PCs={pareto_pcs}'\n",
    "    )\n",
    "else:\n",
    "    pareto_line_legend_2 = None\n",
    "\n",
    "# Custom legend entry for max total accuracy\n",
    "max_line_legend_2 = mlines.Line2D(\n",
    "    [],\n",
    "    [],\n",
    "    color='orange',\n",
    "    linestyle=':',\n",
    "    linewidth=2,\n",
    "    label=f'Max Total Acc = {max_total_acc:.2f} at PCs={pcs_for_max_total_acc}\\n(Worst class accuracy is {worst_class_acc[np.argmax(total_accuracy)]:.2f})'\n",
    ")\n",
    "\n",
    "handles2, labels2 = plt.gca().get_legend_handles_labels()\n",
    "handles2.append(max_line_legend_2)\n",
    "labels2.append(max_line_legend_2.get_label())\n",
    "if pareto_line_legend_2 is not None:\n",
    "    handles2.append(pareto_line_legend_2)\n",
    "    labels2.append(pareto_line_legend_2.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Total Accuracy (Avg)')\n",
    "plt.title('Total Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles2, labels2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plt_3_acc_{model_name}.pdf\", bbox_inches='tight', format='pdf')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max total accuracy of {max_total_acc:.2f} found at {pcs_for_max_total_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06473b86",
   "metadata": {},
   "source": [
    "## Proof of concept 4: Compare cosine of reconstruction using PCs of one class vs. all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c800759",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_per_class_start = 1\n",
    "pcs_per_class_end = int(nr_heads_*num_last_layers_*mean_rank_)\n",
    "pcs_per_class_step = 10\n",
    "max_pcs_per_head = -1\n",
    "random = False\n",
    "class_embeddings = classifier_.T  # M x D\n",
    "\n",
    "# Print baseline accuracy\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "baseline_acc, idxs = test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "if classes_ == waterbird_classes:\n",
    "    baseline_worst = test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "# Reconstruct embeddings for each class label\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "classes_centered = class_embeddings - mean_final_texts.unsqueeze(0)\n",
    "\n",
    "# Initialize a (num_images x 2) array to track:\n",
    "#   [best_score_so_far, class_index_for_that_score]\n",
    "\n",
    "sorted_data = []\n",
    "for text_idx in range(classes_centered.shape[0]):\n",
    "    # Perform query system on entry\n",
    "    concept_i_centered = classes_centered[text_idx, :].unsqueeze(0)\n",
    "\n",
    "    data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "\n",
    "    _, data_abs = reconstruct_embeddings(\n",
    "        data, \n",
    "        [concept_i_centered], \n",
    "        [\"text\"], \n",
    "        return_princ_comp=True, \n",
    "        plot=False, \n",
    "        means=[mean_final_texts],\n",
    "    )\n",
    "\n",
    "    # Extract relevant details from the top k entries\n",
    "    data_pcs = sort_data_by(data_abs, \"correlation_princ_comp_abs\", descending=True)\n",
    "    # Derive nr_pcs_per_class\n",
    "    sorted_data.append(data_pcs)\n",
    "\n",
    "pcs_per_class_max_worst_acc = pcs_per_class_start\n",
    "max_worst_acc = 0\n",
    "\n",
    "worst_class_acc = []\n",
    "worst_class_nr_pcs = []\n",
    "\n",
    "total_accuracy = []  # Will store the average accuracy across all text_idx for each pcs_per_class\n",
    "\n",
    "for pcs_per_class in range(pcs_per_class_start, pcs_per_class_end, pcs_per_class_step):\n",
    "    entries = []\n",
    "    temp_accs = []  # temporary list to store accuracy for each text_idx\n",
    "    all_preds = torch.zeros((final_embeddings_images.shape[0], 2), dtype=torch.double)\n",
    "\n",
    "    for text_idx in range(classes_centered.shape[0]):\n",
    "        # Retrieve data\n",
    "        data_pcs = sorted_data[text_idx]\n",
    "        top_k_entries = top_data(data_pcs, pcs_per_class)\n",
    "        # If `random` is True, randomly pick PCs instead of the actual top_k\n",
    "        if random:\n",
    "            data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "            top_k_entries = random_pcs(data, pcs_per_class * len(classes_))\n",
    "            \n",
    "        # Reconstruct final_embeddings_images\n",
    "        reconstructed_images = reconstruct_all_embeddings_mean_ablation_pcs(\n",
    "            top_k_entries,\n",
    "            mlps_,\n",
    "            attns_, \n",
    "            attns_,\n",
    "            nr_layers_,\n",
    "            nr_heads_,\n",
    "            num_last_layers_,\n",
    "            ratio=-1,\n",
    "            mean_ablate_all=False\n",
    "        )\n",
    "\n",
    "        reconstructed_images /= reconstructed_images.norm(dim=-1, keepdim=True)\n",
    "        predictions = reconstructed_images @ class_embeddings[text_idx, :].T #class_embeddings[text_idx, :].T\n",
    "        # Update \"best so far\" scores in all_preds\n",
    "        best_vals_this_round = predictions\n",
    "        improved_mask = best_vals_this_round > all_preds[:, 0]\n",
    "\n",
    "        best_idxs_this_round = torch.full_like(all_preds[:, 1], fill_value=text_idx)\n",
    "        all_preds[improved_mask, 0] = best_vals_this_round[improved_mask].double()\n",
    "        all_preds[improved_mask, 1] = best_idxs_this_round[improved_mask].double()\n",
    "\n",
    "        # Optionally, check accuracy for the current text_idx predictions\n",
    "        acc, idxs = test_accuracy(predictions.unsqueeze(-1), labels_, label=f\"{classes_[text_idx]}\")\n",
    "        print_correct_elements(idxs, labels_, classes_)\n",
    "\n",
    "        # Build a fictitious one-hot matrix from all_preds\n",
    "        num_images = final_embeddings_images.shape[0]\n",
    "        num_classes = classifier_.shape[1]  # Typically M x D => M classes => classifier_.shape[1] is #classes\n",
    "\n",
    "        # Convert the best class index to a LongTensor\n",
    "        best_class_idxs = all_preds[:, 1].long()\n",
    "\n",
    "        # Create zero matrix [num_images, num_classes]\n",
    "        fictitious_preds = torch.zeros((num_images, num_classes), device=best_class_idxs.device)\n",
    "\n",
    "        # Fill 1.0 in the best predicted class for each image\n",
    "        fictitious_preds[torch.arange(num_images), best_class_idxs] = 1.0\n",
    "\n",
    "        # Test accuracy on these \"hard\" predictions\n",
    "        acc_best, idxs_best = test_accuracy(fictitious_preds, labels_, label=\"Best So Far (One-Hot)\")\n",
    "        if text_idx == len(classes_centered) - 1:\n",
    "            total_accuracy.append(acc_best)\n",
    "\n",
    "        sorted_output = print_correct_elements(idxs_best, labels_, classes_)\n",
    "        if classes_ == waterbird_classes:\n",
    "            curr_worst_acc = test_waterbird_preds(idxs_best, labels_, background_groups_)\n",
    "            if text_idx == len(classes_centered) - 1:\n",
    "                if curr_worst_acc > max_worst_acc:\n",
    "                    max_worst_acc = curr_worst_acc\n",
    "                    pcs_per_class_max_worst_acc = pcs_per_class\n",
    "\n",
    "                worst_class_acc.append(curr_worst_acc)\n",
    "                worst_class_nr_pcs.append(pcs_per_class)\n",
    "\n",
    "        # Print overall accuracy so far\n",
    "        tot_sum = 0\n",
    "        for _, el_nr in sorted_output:\n",
    "            tot_sum += el_nr\n",
    "        if subset_dim == None:\n",
    "            print(f\"Tot accuracy so far is {tot_sum/len(labels_)}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Tot accuracy so far is {tot_sum/((text_idx + 1) * subset_dim)}\")    \n",
    "# ---------------------------------------\n",
    "# PLOTTING SECTION for WORST-CLASS ACCURACY\n",
    "# ---------------------------------------\n",
    "\n",
    "# Suppose these are already known/computed:\n",
    "max_worst_acc = max(worst_class_acc)\n",
    "pcs_per_class_max_worst_acc = worst_class_nr_pcs[np.argmax(worst_class_acc)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the main data\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, worst_class_acc, color='blue', \n",
    "    linestyle='-', \n",
    "    label='Worst-Class Accuracy'\n",
    ")\n",
    "\n",
    "# Baseline line\n",
    "plt.axhline(\n",
    "    y=baseline_worst, \n",
    "    color='gray', \n",
    "    linestyle='--', \n",
    "    linewidth=2, \n",
    "    label=f'Baseline Worst-Class Acc = {baseline_worst:.2f}' \n",
    ")\n",
    "\n",
    "# Horizontal line for max\n",
    "plt.axhline(\n",
    "    y=max_worst_acc, \n",
    "    color='blue', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Vertical line for max\n",
    "plt.axvline(\n",
    "    x=pcs_per_class_max_worst_acc, \n",
    "    color='blue', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Create a single custom legend entry for both max lines\n",
    "max_line_legend = mlines.Line2D(\n",
    "    [], [], \n",
    "    color='blue', \n",
    "    linestyle=':', \n",
    "    linewidth=2,\n",
    "    label=f'Max Worst-Class Acc = {max_worst_acc:.2f} at PCs={pcs_per_class_max_worst_acc}\\n(Worst total accuracy is {total_accuracy[np.argmax(worst_class_acc)]:.2f})'\n",
    ")\n",
    "\n",
    "# Collect existing handles and labels from the current axes\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Append the custom max-line legend entry\n",
    "handles.append(max_line_legend)\n",
    "labels.append(max_line_legend.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Worst Accuracy')\n",
    "plt.title('Worst-Class Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles, labels)  # Use your updated handles and labels\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max worst accuracy of {max_worst_acc:.2f} found at {pcs_per_class_max_worst_acc} PCs/class.\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# PLOTTING SECTION for TOTAL ACCURACY\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Suppose these are already known/computed:\n",
    "max_total_acc = max(total_accuracy)\n",
    "pcs_for_max_total_acc = worst_class_nr_pcs[np.argmax(total_accuracy)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the main data\n",
    "plt.plot(\n",
    "    worst_class_nr_pcs, total_accuracy, \n",
    "    linestyle='-', color='orange', \n",
    "    label='Total Accuracy'\n",
    ")\n",
    "\n",
    "# Baseline line\n",
    "plt.axhline(\n",
    "    y=baseline_acc, \n",
    "    color='gray', \n",
    "    linestyle='--', \n",
    "    linewidth=2, \n",
    "    label=f'Baseline Total Acc = {baseline_acc:.2f}'\n",
    ")\n",
    "\n",
    "# Horizontal line for the max total accuracy\n",
    "plt.axhline(\n",
    "    y=max_total_acc, \n",
    "    color='orange', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Vertical line for the max total accuracy\n",
    "plt.axvline(\n",
    "    x=pcs_for_max_total_acc, \n",
    "    color='orange', \n",
    "    linestyle=':', \n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Create a single custom legend entry for both max lines\n",
    "max_line_legend_2 = mlines.Line2D(\n",
    "    [], [], \n",
    "    color='orange', \n",
    "    linestyle=':', \n",
    "    linewidth=2,\n",
    "    label=f'Max Total Acc = {max_total_acc:.2f} at PCs={pcs_for_max_total_acc}\\n(Worst class accuracy is {worst_class_acc[np.argmax(total_accuracy)]:.2f})'\n",
    ")\n",
    "\n",
    "# Collect existing handles/labels\n",
    "handles2, labels2 = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Append the custom line\n",
    "handles2.append(max_line_legend_2)\n",
    "labels2.append(max_line_legend_2.get_label())\n",
    "\n",
    "plt.xlabel('Number of PCs per Class')\n",
    "plt.ylabel('Total Accuracy (Avg)')\n",
    "plt.title('Total Accuracy vs. Number of PCs per Class')\n",
    "plt.grid(True)\n",
    "plt.legend(handles2, labels2)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max total accuracy of {max_total_acc:.2f} found at {pcs_for_max_total_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5605a2",
   "metadata": {},
   "source": [
    "## Comparison Textspan with waterbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56467a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive heads textspan\n",
    "if model_name == \"ViT-H-14\":\n",
    "    to_mean_ablate_setting = [\n",
    "        {\"layer\": 31, \"head\": 12},\n",
    "        {\"layer\": 30, \"head\": 11},\n",
    "        {\"layer\": 29, \"head\": 4},\n",
    "    ]\n",
    "    to_mean_ablate_geo = [\n",
    "        {\"layer\": 31, \"head\": 8},\n",
    "        {\"layer\": 30, \"head\": 15},\n",
    "        {\"layer\": 30, \"head\": 12},\n",
    "        {\"layer\": 30, \"head\": 6},\n",
    "        {\"layer\": 29, \"head\": 14},\n",
    "        {\"layer\": 29, \"head\": 8},\n",
    "    ]\n",
    "\n",
    "elif model_name == \"ViT-L-14\":\n",
    "    to_mean_ablate_geo = [\n",
    "        {\"layer\": 21, \"head\": 1},\n",
    "        {\"layer\": 22, \"head\": 12},\n",
    "        {\"layer\": 22, \"head\": 13},\n",
    "        {\"layer\": 21, \"head\": 11},\n",
    "        {\"layer\": 21, \"head\": 14},\n",
    "        {\"layer\": 23, \"head\": 6},\n",
    "    ]\n",
    "    to_mean_ablate_setting = [\n",
    "        {\"layer\": 21, \"head\": 3},\n",
    "        {\"layer\": 21, \"head\": 6},\n",
    "        {\"layer\": 21, \"head\": 8},\n",
    "        {\"layer\": 21, \"head\": 13},\n",
    "        {\"layer\": 22, \"head\": 2},\n",
    "        {\"layer\": 22, \"head\": 12},\n",
    "        {\"layer\": 22, \"head\": 15},\n",
    "        {\"layer\": 23, \"head\": 1},\n",
    "        {\"layer\": 23, \"head\": 3},\n",
    "        {\"layer\": 23, \"head\": 5},\n",
    "    ]\n",
    "\n",
    "elif model_name == \"ViT-B-16\":\n",
    "    to_mean_ablate_setting = [\n",
    "        {\"layer\": 11, \"head\": 3},\n",
    "        {\"layer\": 10, \"head\": 11},\n",
    "        {\"layer\": 10, \"head\": 10},\n",
    "        {\"layer\": 9, \"head\": 8},\n",
    "        {\"layer\": 9, \"head\": 6},\n",
    "    ]\n",
    "    to_mean_ablate_geo = [\n",
    "        {\"layer\": 11, \"head\": 6},\n",
    "        {\"layer\": 11, \"head\": 0},\n",
    "    ]\n",
    "\n",
    "elif model_name == \"ViT-B-32\":\n",
    "    to_mean_ablate_setting = [\n",
    "        {\"layer\": 11, \"head\":5},\n",
    "        {\"layer\": 10, \"head\": 5},\n",
    "        {\"layer\": 10, \"head\": 3},\n",
    "        {\"layer\": 9, \"head\": 1},\n",
    "    ]\n",
    "    to_mean_ablate_geo = [\n",
    "        {\"layer\": 11, \"head\": 9},\n",
    "        {\"layer\": 11, \"head\": 5},\n",
    "    ]\n",
    "to_mean_ablate_geo_heads = to_mean_ablate_setting + to_mean_ablate_geo\n",
    "all_heads = [{\"layer\": l, \"head\": h} for l in range(nr_layers_ - num_last_layers_, nr_layers_) for h in range(nr_heads_)]\n",
    "for h_1 in to_mean_ablate_geo_heads:\n",
    "    for c, h_2 in enumerate(all_heads):\n",
    "        if h_1 == h_2:\n",
    "            all_heads = all_heads[:c] + all_heads[c+1:]\n",
    "            break\n",
    "\n",
    "reconstructed_images = reconstruct_all_embeddings_mean_ablation_heads(all_heads, mlps_, attns_, final_embeddings_images,nr_layers_, nr_heads_, num_last_layers_)\n",
    "reconstructed_images /= reconstructed_images.norm(dim=-1, keepdim=True)\n",
    "predictions = reconstructed_images @ classifier_ #class_embeddings[text_idx, :].T\n",
    "\n",
    "# Optionally, check accuracy for the current text_idx predictions\n",
    "acc, idxs = test_accuracy(predictions, labels_, label=f\"Textspan\")\n",
    "print_correct_elements(idxs, labels_, classes_)    \n",
    "if classes_ == waterbird_classes:\n",
    "    test_waterbird_preds(idxs, labels_, background_groups_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35803f",
   "metadata": {},
   "source": [
    "## Test different accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of the tensors for debugging purposes:\n",
    "# attns_: attention activations\n",
    "# mlps_: MLP activations\n",
    "# classifier_: classifier weights\n",
    "# labels_: ground truth labels\n",
    "print(attns_.shape, mlps_.shape, classifier_.shape, labels_.shape)\n",
    "\n",
    "\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "# Test accuracy of mean centered data with mean centered text\n",
    "mean_centered_data = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1) - mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "mean_centered_classifier_ = classifier_ - mean_final_texts.unsqueeze(-1)\n",
    "mean_centered_classifier_ /= mean_centered_classifier_.norm(dim=-1, keepdim=True)\n",
    "\n",
    "test_accuracy(mean_centered_data @ mean_centered_classifier_, labels_, label=\"Mean centered data with mean centered text\")\n",
    "\n",
    "# Test accuracy of mean centered data with original text\n",
    "mean_centered_data = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1) - mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "mean_centered_data += mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "test_accuracy(mean_centered_data @ classifier_, labels_, label=\"Mean centered data with original (not mean centered) text\")\n",
    "\n",
    "# We now attempt a \"mean ablation\" approach for attention\n",
    "current_model = (current_mean_ablation_per_head_sum_\n",
    "                 + no_heads_attentions_[:, last_ + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "_, indexes_mean_ablate = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {last_} until layer {attns_.shape[1]}\")\n",
    "\n",
    "# We now attempt a \"mean ablation\" approach for attention\n",
    "current_model = (current_mean_ablation_per_head_sum_\n",
    "                 + no_heads_attentions_[:, last_ + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "current_model -= mean_final_images\n",
    "current_model /= current_model.norm(dim=-1, keepdim=True)\n",
    "_, indexes_mean_ablate = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {last_} until layer {attns_.shape[1]} with mean centered images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252fe7a",
   "metadata": {},
   "source": [
    "## Test different accuracies using reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45515051",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings_images_rec_embed = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns_not_mean_centered = torch.zeros_like(final_embeddings_images)\n",
    "image_emb_cent_embed = final_embeddings_images - mean_final_images\n",
    "\n",
    "final_embeddings_texts_rec_embed = torch.zeros_like(classifier_.T)\n",
    "texts_emb_cent_embed = (classifier_ - mean_final_texts.unsqueeze(-1)).T\n",
    "\n",
    "# Open the attention dataset to retrieve projection matrices and mean values\n",
    "with open(attention_dataset, \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        entry = json.loads(line)\n",
    "        # If this entry is the final embedding entry (head == -1), skip it.\n",
    "        if entry[\"head\"] == -1:\n",
    "            last_line = entry\n",
    "            continue\n",
    "\n",
    "        project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "        vh = torch.tensor(entry[\"vh\"])\n",
    "        # Reconstruct the image embeddings using final embeddings:\n",
    "        # Center them by subtracting mean attention values, project them through vh, \n",
    "        # apply project_matrix and vh again, then add mean values back.\n",
    "        final_embeddings_images_rec_embed += (image_emb_cent_embed) @ vh.T @ project_matrix @ vh\n",
    "        final_embeddings_texts_rec_embed += (texts_emb_cent_embed) @ vh.T @ project_matrix @ vh\n",
    "        # Reconstruct the image embeddings using attention activations:\n",
    "        # Similar process, but start from attns_ for the given layer/head.\n",
    "        image_emb_cent_attns = attns_[:, entry[\"layer\"], entry[\"head\"], :] - torch.tensor(entry[\"mean_values_att\"])\n",
    "        final_embeddings_images_rec_attns += (image_emb_cent_attns) @ vh.T @ project_matrix @ vh + torch.tensor(entry[\"mean_values_att\"])\n",
    "        final_embeddings_images_rec_attns_not_mean_centered += (image_emb_cent_attns) @ vh.T @ project_matrix @ vh\n",
    "\n",
    "final_embeddings_images_rec_embed_norm = final_embeddings_images_rec_embed/final_embeddings_images_rec_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "final_embeddings_texts_rec_embed_norm = final_embeddings_texts_rec_embed/final_embeddings_texts_rec_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "final_embeddings_images_rec_attns_not_mean_centered_norm = final_embeddings_images_rec_attns_not_mean_centered/final_embeddings_images_rec_attns_not_mean_centered.norm(dim=-1, keepdim=True)\n",
    "\n",
    "texts_emb_cent_embed /= texts_emb_cent_embed.norm(dim=-1, keepdim=True)\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed_norm + mean_final_images\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with final embeddings on only the last layers\")\n",
    "\n",
    "current_model = mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns_not_mean_centered\n",
    "_, indexes_approx_activ_only = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images with direct contribution of activation space\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the attention activations approach\n",
    "current_model = (mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns)\n",
    "_, indexes_approx_activ = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with attention activations\")\n",
    "\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed+ mean_final_images\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images without mean-ablation\")\n",
    "\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed+ mean_final_images\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images and texts without mean-ablation\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = (mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns)\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Original images and approximation of texts without mean-ablation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba44aa0",
   "metadata": {},
   "source": [
    "## Test Bias Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"../cache\"\n",
    "top_k = 30  # Maximum number of top entries to retrieve\n",
    "approx = 1.1  # Target approximation threshold for the reconstruction quality\n",
    "## Run the chosen algorithm on a dataset to derive text explanations \n",
    "command = f\"python -m utils.scripts.bias_removal_test \\\n",
    "    --device {device} --model {model_name} --pretrained {pretrained} --seed {seed} \\\n",
    "    --subset_dim {subset_dim} --dataset_text {dataset_text_name} --dataset {datataset_image_name} \\\n",
    "    --device {device} --top_k {top_k} --max_approx {approx} --cache_dir {cache_dir}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0c5f8",
   "metadata": {},
   "source": [
    "## Test bias removal and subset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we calculate scores for each principal component (PC) while ignoring query info.\n",
    "# We do this by reconstructing embeddings from the principal components alone, both from \n",
    "# the final embeddings perspective and the attention activations perspective.\n",
    "\n",
    "final_embeddings_images_rec_embed_topic = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns_topic = torch.zeros_like(final_embeddings_images)\n",
    "\n",
    "image_emb_cent_embed = final_embeddings_images - mean_final_images\n",
    "\n",
    "top_k_other_details = get_remaining_pcs(data, top_k_details)\n",
    "\n",
    "# Iterate through the top_k entries and reconstruct embeddings\n",
    "for entry in top_k_details:\n",
    "    # Reconstruct embeddings focusing on each principal component:\n",
    "    # 1. Start from the final embeddings, center them, and extract the component of interest.\n",
    "    vh = torch.tensor(entry[\"vh\"])\n",
    "    project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "    princ_comp = torch.tensor(entry[\"princ_comp\"])\n",
    "\n",
    "    projection_image_embed = image_emb_cent_embed @ vh.T\n",
    "    mask_images_embed = torch.zeros_like(projection_image_embed)\n",
    "    mask_images_embed[:, princ_comp] = projection_image_embed[:, princ_comp]\n",
    "    final_embeddings_images_rec_embed_topic += mask_images_embed @ project_matrix @ vh\n",
    "\n",
    "    # Repeat for attention-based activations:\n",
    "    mean_values_att = torch.tensor(entry[\"mean_values_att\"])\n",
    "    image_emb_cent_attns = attns_[:, entry[\"layer\"], entry[\"head\"], :] - mean_values_att\n",
    "    projection_images_attns = image_emb_cent_attns @ vh.T\n",
    "    mask_images_attns = torch.zeros_like(projection_images_attns)\n",
    "    mask_images_attns[:, princ_comp] = projection_images_attns[:, princ_comp]\n",
    "    final_embeddings_images_rec_attns_topic += mask_images_attns @ project_matrix @ vh\n",
    "\n",
    "\n",
    "# Compute accuracy using the reconstruction from final embeddings, ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_embed - final_embeddings_images_rec_embed_topic\n",
    "_, indexes_approx_final_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Bias removal)\")\n",
    "print_diff_elements(indexes_approx_final, indexes_approx_final_rem, subset_dim)\n",
    "\n",
    "# Compute accuracy using the reconstruction from attention activations, also ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_attns_not_mean_centered - final_embeddings_images_rec_attns_topic\n",
    "_, indexs_approx_activ_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Bias Removal)\")\n",
    "print_diff_elements(indexes_approx_activ_only, indexs_approx_activ_rem, subset_dim)\n",
    "\n",
    "# Compute accuracy using the reconstruction from final embeddings, ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_embed_topic\n",
    "_, indexes_approx_final_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Subset)\")\n",
    "print_correct_elements(indexes_approx_final_rem, labels_, classes_)\n",
    "\n",
    "# Compute accuracy using the reconstruction from attention activations, also ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_attns_topic\n",
    "_, indexs_approx_activ_rem = test_accuracy(current_model @ texts_emb_cent_embed.T, labels_, label=f\"Approximation with current topic final embeddings (Subset)\")\n",
    "print_correct_elements(indexs_approx_activ_rem, labels_, classes_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
