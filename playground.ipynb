{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce479d0c-554a-42ee-b365-84a4d9ab81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "import json\n",
    "from utils.misc.misc import accuracy, accuracy_correct\n",
    "from utils.scripts.algorithms_text_explanations import *\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.misc.visualization import visualization_preprocess\n",
    "from utils.models.prs_hook import hook_prs_logger\n",
    "from utils.datasets_constants.imagenet_classes import imagenet_classes\n",
    "from utils.scripts.algorithms_text_explanations import svd_data_approx\n",
    "from utils.datasets.dataset_helpers import dataset_to_dataloader\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from collections import defaultdict\n",
    "from utils.scripts.algorithms_text_explanations_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee675770-3be8-40bf-8659-31e2d2a811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cpu'\n",
    "pretrained = 'laion2b_s34b_b79k' # 'laion2b_s32b_b79k'\n",
    "model_name = 'ViT-B-32' # 'ViT-H-14'\n",
    "seed = 12\n",
    "num_last_layers = 4\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_bias_clean\"\n",
    "datataset_image_name = \"imagenet\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "batch_size = 16 # only needed for the nn search\n",
    "imagenet_path = './datasets/imagenet/' # only needed for the nn search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db3598-0d7d-47a4-b6c6-8d02f4902e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Model\n",
    "model, _, preprocess = create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model.visual.transformer.resblocks))\n",
    "\n",
    "prs = hook_prs_logger(model, device, spatial=False) # This attach hook to get the residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f51611-710d-45b9-a797-85a958cc047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the chosen algorithm on a dataset to derive text explanations \n",
    "command = f\"python -m utils.scripts.compute_text_explanations --device {device} --model {model_name} --algorithm {algorithm} --seed {seed} --text_per_princ_comp 20 --num_of_last_layers {num_last_layers} --text_descriptions {dataset_text_name}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef211e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new created attention datasets\n",
    "attention_dataset = f\"output_dir/{datataset_image_name}_completeness_{dataset_text_name}_{model_name}_algo_{algorithm}_seed_{seed}.jsonl\"\n",
    "\n",
    "# Load necessary data\n",
    "attns_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attn_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], attention values\n",
    "mlps_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlp_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], mlp values\n",
    "classifier_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_classifier_{model_name}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], embedding of the labels\n",
    "labels_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_labels_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # Position of the labels in the cosndiered dataset\n",
    "final_embeddings_images = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_embeddings_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))\n",
    "final_embeddings_texts = torch.tensor(np.load(f\"output_dir/{dataset_text_name}_{model_name}.npy\", mmap_mode=\"r\"))\n",
    "with open( f\"utils/text_descriptions/{dataset_text_name}.txt\", \"r\") as f:\n",
    "    texts_str = np.array([i.replace(\"\\n\", \"\") for i in f.readlines()])\n",
    "# Get mean ablation\n",
    "no_heads_attentions_ = attns_.sum(axis=(2))  # Sum over heads dimension\n",
    "last_ = attns_.shape[1] - num_last_layers\n",
    "# Replace attention activations until 'last' layer with their average, while keeping later layers intact.\n",
    "current_mean_ablation_per_head_sum_ = torch.mean(no_heads_attentions_[:, :last_ + 1], axis=0).sum(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d1202",
   "metadata": {},
   "source": [
    "# Print the top Principal Components text-interpretation for each Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "min_princ_comp = 5\n",
    "\n",
    "# Read JSON lines from attention_dataset\n",
    "# This file contains data about layers, heads, and their principal components (PCs) with associated metrics.\n",
    "data = get_data(attention_dataset, -1)\n",
    "    \n",
    "# Print the data in a nice formatted table\n",
    "print_data(data, min_princ_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea667e63",
   "metadata": {},
   "source": [
    "# Strongest Principal Components per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top entries to retrieve\n",
    "top_k = 5\n",
    "min_heap = []\n",
    "\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Sort data entries in descending order of strength_abs of the princial component\n",
    "top_k_entries = top_data(sort_data_by(data, \"strength_abs\", descending=True), top_k=top_k)\n",
    "\n",
    "# Print the top_k entries in a nice formatted table\n",
    "print_data(top_k_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71330301",
   "metadata": {},
   "source": [
    "# Visualize singular values of a principal component (both text and images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7df3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info on data\n",
    "layer = 10\n",
    "head = 0\n",
    "princ_comp = 0\n",
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 20  # Length of continuous elements\n",
    "\n",
    "visualize_principal_component(layer, head, princ_comp, nr_top_imgs, nr_worst_imgs, nr_cont_imgs, attention_dataset, final_embeddings_images, final_embeddings_texts, seed, imagenet_path, texts_str, imagenet_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad6081",
   "metadata": {},
   "source": [
    "# Test accuracy of reconstruction of text and images using only the final embedding and their projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e37a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top entries to retrieve\n",
    "top_k = 20\n",
    "min_heap = []\n",
    "image = preprocess(Image.open('images/woman.png'))[np.newaxis, :, :, :]  # Add batch dimension\n",
    "text_query = \"lallalasjiuuoaosoaonfnfnfn akskakskksak kaskksaksaksaksakksa.\"\n",
    "\n",
    "# Encode the image\n",
    "prs.reinit()  # Reinitialize the residual stream hook\n",
    "\n",
    "# Encode the image with no gradient calculation\n",
    "with torch.no_grad():\n",
    "      image_emb = model.encode_image(\n",
    "        image.to(device),\n",
    "        attn_method='head_no_spatial',\n",
    "        normalize=True)\n",
    "\n",
    "      # Encode the text\n",
    "      text_query_token = tokenizer(text_query).to(device)  # Tokenize the text query\n",
    "      topic_emb = model.encode_text(text_query_token, normalize=True)  # Encode the text query\n",
    "\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "# Mean center the embeddings\n",
    "topic_emb -= mean_final_texts\n",
    "image_emb -= mean_final_images\n",
    "\n",
    "# Iterate through the attention dataset and reconstruct embeddings\n",
    "[topic_emb_rec, image_emb_rec], _ = reconstruct_embeddings(data, [topic_emb, image_emb], [\"text\", \"image\"])\n",
    "\n",
    "\n",
    "# Print norms to understand magnitude before normalization\n",
    "print(\"Norm of topic_emb_rec before normalization:\", topic_emb_rec.norm().item())\n",
    "print(\"Norm of image_emb_rec before normalization:\", image_emb_rec.norm().item())\n",
    "\n",
    "# Normalize the reconstructed embeddings so they lie on the unit sphere\n",
    "topic_emb_rec /= topic_emb_rec.norm(dim=-1, keepdim=True)\n",
    "image_emb_rec /= image_emb_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "topic_emb_rec += mean_final_texts\n",
    "image_emb_rec += mean_final_images\n",
    "\n",
    "topic_emb_rec /= topic_emb_rec.norm(dim=-1, keepdim=True)\n",
    "image_emb_rec /= image_emb_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "topic_emb += mean_final_texts\n",
    "image_emb += mean_final_images\n",
    "\n",
    "\n",
    "# Print similarities between original and reconstructed embeddings\n",
    "print(\"Cosine similarity between original topic_emb and reconstructed topic_emb_rec:\",\n",
    "      (topic_emb @ topic_emb_rec.T).item())\n",
    "\n",
    "print(\"Cosine similarity between original image_emb and reconstructed image_emb_rec:\",\n",
    "      (image_emb @ image_emb_rec.T).item())\n",
    "\n",
    "# Print cross-similarities to compare text-image embeddings before and after reconstruction\n",
    "print(\"Cosine similarity between original topic_emb and original image_emb:\",\n",
    "      (topic_emb @ image_emb.T).item())\n",
    "\n",
    "print(\"Cosine similarity between original topic_emb and reconstructed image_emb_rec:\",\n",
    "      (topic_emb @ image_emb_rec.T).item())\n",
    "\n",
    "print(\"Cosine similarity between reconstructed topic_emb_rec and original image_emb:\",\n",
    "      (topic_emb_rec @ image_emb.T).item())\n",
    "\n",
    "print(\"Cosine similarity between reconstructed topic_emb_rec and reconstructed image_emb_rec:\",\n",
    "      (topic_emb_rec @ image_emb_rec.T).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25a1a4",
   "metadata": {},
   "source": [
    "# Query a topic or image and NNs on that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c66cf",
   "metadata": {},
   "source": [
    "### Define the query and analyze each Principal Component and derive a strength metric for reconstruction of the query-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b8153-73a3-4f30-bc1d-eddef413df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode so that no gradients are computed\n",
    "model.eval()\n",
    "query_text = True\n",
    "\n",
    "# Retrieve an embedding\n",
    "with torch.no_grad():\n",
    "    if query_text:\n",
    "        # If querying by text, define a text prompt and encode it into an embedding\n",
    "        text_query = \"An image of guillotine\"\n",
    "        # Tokenize the text query and move it to the device (GPU/CPU)\n",
    "        text_query_token = tokenizer(text_query).to(device)  \n",
    "        # Encode the tokenized text into a normalized embedding\n",
    "        topic_emb = model.encode_text(text_query_token, normalize=True)\n",
    "    else:\n",
    "        # If querying by image, load and preprocess the image from disk\n",
    "        prs.reinit()  # Reinitialize any hooks if required\n",
    "        text_query = \"woman.png\"\n",
    "        image_pil = Image.open(f'images/{text_query}')\n",
    "        image = preprocess(image_pil)[np.newaxis, :, :, :]  # Add batch dimension\n",
    "        # Encode the image into a normalized embedding\n",
    "        topic_emb = model.encode_image(\n",
    "            image.to(device), \n",
    "            attn_method='head_no_spatial',\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "### Reconstruct embedding and find contributions from principal components\n",
    "# Retrieve data\n",
    "data = get_data(attention_dataset, -1, skip_final=True)\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "# Mean center the embeddings\n",
    "mean_final = mean_final_texts if query_text else mean_final_images\n",
    "\n",
    "# Mean center the embeddings\n",
    "topic_emb_cent = topic_emb - mean_final\n",
    "final_embeddings_texts_cent = final_embeddings_texts - mean_final_texts\n",
    "# Recontruct embedding\n",
    "[topic_emb_rec_cent], data = reconstruct_embeddings(data, [topic_emb_cent], [\"text\" if query_text else \"image\"], return_princ_comp=True, plot=True, means=[mean_final])\n",
    "\n",
    "# Normalize the embeddings\n",
    "topic_emb_rec_cent /= topic_emb_rec_cent.norm(dim=-1, keepdim=True)\n",
    "topic_emb_rec = topic_emb_rec_cent + mean_final\n",
    "topic_emb_rec /= topic_emb_rec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# The maximum reconstruction score is how close the reconstructed embedding is to the original,\n",
    "# adjusted by the baseline score. This gives a cosine similarity measure.\n",
    "max_reconstr_score = topic_emb_rec @ topic_emb.T\n",
    "# Print out the cosine similarity between the original and reconstructed embeddings\n",
    "print(f\"We have a max cosine similarity of: {(max_reconstr_score).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3bb4b",
   "metadata": {},
   "source": [
    "### Use the strength of the previous reconstruction to derive a good enough reconstruction of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant details from the top-k principal component entries based on the reconstruction of the query\n",
    "top_k = 1  # Maximum number of top entries to retrieve\n",
    "approx = 0.9  # Target approximation threshold for the reconstruction quality\n",
    "\n",
    "# Initialize a tensor to accumulate the reconstructed topic embedding from selected principal components\n",
    "topic_emb_rec_act = torch.zeros_like(topic_emb)\n",
    "\n",
    "### Extract relevant details from the top k entries\n",
    "data = sort_data_by(data, \"correlation_princ_comp_abs\", descending=True) \n",
    "\n",
    "top_k_entries = top_data(data, top_k)\n",
    "\n",
    "top_k_details = reconstruct_top_embedding(top_k_entries, topic_emb_cent, mean_final, \"text\" if query_text else \"image\", max_reconstr_score, top_k, approx)\n",
    "# Convert the collected principal component details into a DataFrame for easy processing\n",
    "print(f\"Currently querying the topic: {text_query}\")\n",
    "print_data(top_k_details, is_corr_present=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cea6d",
   "metadata": {},
   "source": [
    "### Prepare scores of images and texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4215d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ds\n",
    "ds_vis = create_dataset_imagenet(imagenet_path, visualization_preprocess, samples_per_class=3, tot_samples_per_class=50, seed=seed)\n",
    "# Initialize arrays to store the top and lowest scores for each search head.\n",
    "scores_array_images = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "scores_array_texts = np.empty(\n",
    "    final_embeddings_texts.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('txt_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Create arrays of indexes for referencing images and texts.\n",
    "indexes_images = np.arange(0, final_embeddings_images.shape[0], 1) \n",
    "indexes_texts = np.arange(0, final_embeddings_texts.shape[0], 1) \n",
    "\n",
    "\n",
    "# Get mean of data and texts\n",
    "mean_final_images = torch.mean(final_embeddings_images, axis=0)\n",
    "mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "\n",
    "images_centered = final_embeddings_images - mean_final_images\n",
    "texts_centered = final_embeddings_texts - mean_final_texts\n",
    "\n",
    "# Recontruct embedding\n",
    "[texts_rec_cent, images_rec_cent], _ = reconstruct_embeddings(top_k_details, [texts_centered, images_centered], [\"text\", \"image\"], return_princ_comp=False)\n",
    "\n",
    "texts_rec = texts_rec_cent + mean_final_texts\n",
    "images_rec = images_rec_cent + mean_final_images\n",
    "# Compute the similarity scores between the reconstructed embeddings (images or texts) and the original query embedding.\n",
    "# The dot product gives a similarity measure, which we store in the scores arrays along with the index.\n",
    "# We do NOT normalize the score.\n",
    "# Compute scores for images\n",
    "scores_array_images[\"score\"] = (images_rec @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "images_rec /= images_rec.norm(dim=-1, keepdim=True)\n",
    "scores_array_images[\"score_vis\"] = (images_rec @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "scores_array_images[\"img_index\"] = indexes_images\n",
    "\n",
    "# Compute scores for texts\n",
    "scores_array_texts[\"score\"] = (texts_rec @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "texts_rec /= texts_rec.norm(dim=-1, keepdim=True)\n",
    "scores_array_texts[\"score_vis\"] = (texts_rec @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "scores_array_texts[\"txt_index\"] = indexes_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eccc6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores array of real CLIP embeddings\n",
    "scores_array_images_full = np.empty(\n",
    "    final_embeddings_images.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('img_index', 'i4')]\n",
    ")\n",
    "scores_array_texts_full = np.empty(\n",
    "    final_embeddings_texts.shape[0], \n",
    "    dtype=[('score', 'f4'), ('score_vis', 'f4'), ('txt_index', 'i4')]\n",
    ")\n",
    "\n",
    "# Compute scores for images\n",
    "images = final_embeddings_images\n",
    "scores_array_images_full[\"score\"] = (images @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "images /= images.norm(dim=-1, keepdim=True)\n",
    "scores_array_images_full[\"score_vis\"] = (images @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "scores_array_images_full[\"img_index\"] = indexes_images\n",
    "\n",
    "# Compute scores for texts\n",
    "texts = final_embeddings_texts\n",
    "scores_array_texts_full[\"score\"] = (texts @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "texts /= texts.norm(dim=-1, keepdim=True)\n",
    "scores_array_texts_full[\"score_vis\"] = (texts @ topic_emb.T).squeeze().numpy()\n",
    "\n",
    "scores_array_texts_full[\"txt_index\"] = indexes_texts\n",
    "\n",
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 20  # Length of continuous elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae29f",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top and worst images to look at for each princ_comp\n",
    "nr_top_imgs = 20  # Number of top elements\n",
    "nr_worst_imgs = 20  # Number of worst elements\n",
    "nr_cont_imgs = 20  # Length of continuous elements\n",
    "\n",
    "dbs = create_dbs(scores_array_images, scores_array_texts, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "\n",
    "visualize_dbs(top_k_details, dbs, ds_vis, texts_str, imagenet_classes, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full Embeddings similarity\n",
    "dbs = create_dbs(scores_array_images_full, scores_array_texts_full, nr_top_imgs, nr_worst_imgs, nr_cont_imgs)\n",
    "\n",
    "visualize_dbs(top_k_details, dbs, ds_vis, texts_str, imagenet_classes, text_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7efe8e",
   "metadata": {},
   "source": [
    "# Evaluate classification using reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fc495c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(prediction, labels, label=\"Classifier\"):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model's predictions.\n",
    "    \"\"\"\n",
    "    accuracy, indexes = accuracy_correct(prediction, labels)\n",
    "    accuracy_pred = accuracy[0] * 100\n",
    "    print(f\"For the approach {label}, the accuracy is: {accuracy_pred:3f}%\")\n",
    "    return accuracy_pred, indexes[0]\n",
    "\n",
    "def print_diff_elements(indexes_1, indexes_2):\n",
    "    # TODO: Hardcoded for ImageNet\n",
    "    # Retrieve the labels of the dataset. \n",
    "    # This is hardcoded for ImageNet where nr_classes is the number of classes (usually 1000).\n",
    "    nr_samples = torch.arange(1000)\n",
    "    classes_indexes = nr_samples.repeat_interleave(3)\n",
    "    class_labels = np.array([imagenet_classes[i] for i in classes_indexes])\n",
    "    # Determine which elements differ between the two reconstructions\n",
    "    wrong_elements = np.array(~(indexes_1 == indexes_2))\n",
    "    \n",
    "    print(f\"Number of elements with different results between the two reconstruction methods: {len(class_labels[wrong_elements])}\")\n",
    "    # Track occurrences\n",
    "    label_count = defaultdict(int)\n",
    "    output_set = set()\n",
    "\n",
    "    # Iterate through mask and labels\n",
    "    for idx, is_wrong in enumerate(wrong_elements):\n",
    "        if is_wrong:\n",
    "            label = class_labels[idx]\n",
    "            label_count[label] += 1\n",
    "\n",
    "    # Sort the set by nr_of_prev_occurrences in descending order\n",
    "    sorted_output = sorted(label_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"The different elements labels are: {sorted_output}\")\n",
    "\n",
    "def print_wrong_elements(indexes_1, text=\"wrong\"):\n",
    "    # TODO: Hardcoded for ImageNet\n",
    "    # Retrieve the labels of the dataset. \n",
    "    # This is hardcoded for ImageNet where nr_classes is the number of classes (usually 1000).\n",
    "    nr_samples = torch.arange(1000)\n",
    "    classes_indexes = nr_samples.repeat_interleave(3)\n",
    "    class_labels = np.array([imagenet_classes[i] for i in classes_indexes])\n",
    "    \n",
    "\n",
    "    print(f\"Number of elements with {text} results between the two reconstruction methods: {len(class_labels[indexes_1])}\")\n",
    "    # Track occurrences\n",
    "    label_count = defaultdict(int)\n",
    "\n",
    "    # Iterate through mask and labels\n",
    "    for idx, is_correct in enumerate(indexes_1):\n",
    "        if not is_correct:\n",
    "            label = class_labels[idx]\n",
    "            label_count[label] += 1\n",
    "\n",
    "    # Sort the set by nr_of_prev_occurrences in descending order\n",
    "    sorted_output = sorted(label_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"The {text} elements labels are: {sorted_output}\")\n",
    "\n",
    "def print_correct_elements(indexes_1, text=\"correct\"):\n",
    "    print_wrong_elements(~indexes_1, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35803f",
   "metadata": {},
   "source": [
    "## Test different accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of the tensors for debugging purposes:\n",
    "# attns_: attention activations\n",
    "# mlps_: MLP activations\n",
    "# classifier_: classifier weights\n",
    "# labels_: ground truth labels\n",
    "print(attns_.shape, mlps_.shape, classifier_.shape, labels_.shape)\n",
    "\n",
    "\n",
    "# Baseline accuracy computation:\n",
    "baseline = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1)\n",
    "test_accuracy(baseline @ classifier_, labels_, label=\"Baseline\")\n",
    "\n",
    "# Test accuracy of mean centered data with mean centered text\n",
    "mean_centered_data = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1) - mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "mean_centered_classifier_ = classifier_ - mean_final_texts.unsqueeze(-1)\n",
    "mean_centered_classifier_ /= mean_centered_classifier_.norm(dim=-1, keepdim=True)\n",
    "\n",
    "test_accuracy(mean_centered_data @ mean_centered_classifier_, labels_, label=\"Mean centered data with mean centered text\")\n",
    "\n",
    "# Test accuracy of mean centered data with original text\n",
    "mean_centered_data = attns_.sum(axis=(1, 2)) + mlps_.sum(axis=1) - mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "mean_centered_data += mean_final_images\n",
    "mean_centered_data /= mean_centered_data.norm(dim=-1, keepdim=True)\n",
    "test_accuracy(mean_centered_data @ classifier_, labels_, label=\"Mean centered data with original (not mean centered) text\")\n",
    "\n",
    "# We now attempt a \"mean ablation\" approach for attention\n",
    "current_model = (current_mean_ablation_per_head_sum_\n",
    "                 + no_heads_attentions_[:, last_ + 1:].sum(1)) + mlps_.sum(axis=1) \n",
    "_, indexes_mean_ablate = test_accuracy(current_model @ classifier_, labels_, label=f\"Mean ablation from layer {last_} until layer {attns_.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252fe7a",
   "metadata": {},
   "source": [
    "## Test different accuracies using reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45515051",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings_images_rec_embed = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns = torch.zeros_like(final_embeddings_images)\n",
    "image_emb_cent_embed = final_embeddings_images - mean_final_images\n",
    "\n",
    "final_embeddings_texts_rec_embed = torch.zeros_like(classifier_.T)\n",
    "texts_emb_cent_embed = (classifier_ - mean_final_texts.unsqueeze(-1)).T\n",
    "\n",
    "# Open the attention dataset to retrieve projection matrices and mean values\n",
    "with open(attention_dataset, \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        entry = json.loads(line)\n",
    "        # If this entry is the final embedding entry (head == -1), skip it.\n",
    "        if entry[\"head\"] == -1:\n",
    "            last_line = entry\n",
    "            continue\n",
    "\n",
    "        project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "        vh = torch.tensor(entry[\"vh\"])\n",
    "        # Reconstruct the image embeddings using final embeddings:\n",
    "        # Center them by subtracting mean attention values, project them through vh, \n",
    "        # apply project_matrix and vh again, then add mean values back.\n",
    "        final_embeddings_images_rec_embed += (image_emb_cent_embed) @ vh.T @ project_matrix @ vh\n",
    "        final_embeddings_texts_rec_embed += (texts_emb_cent_embed) @ vh.T @ project_matrix @ vh\n",
    "        # Reconstruct the image embeddings using attention activations:\n",
    "        # Similar process, but start from attns_ for the given layer/head.\n",
    "        image_emb_cent_attns = attns_[:, entry[\"layer\"], entry[\"head\"], :] - torch.tensor(entry[\"mean_values_att\"])\n",
    "        final_embeddings_images_rec_attns += (image_emb_cent_attns) @ vh.T @ project_matrix @ vh + torch.tensor(entry[\"mean_values_att\"]) \n",
    "\n",
    "final_embeddings_images_rec_embed /= final_embeddings_images_rec_embed.norm(dim=-1, keepdim=True)\n",
    "final_embeddings_images_rec_embed += mean_final_images \n",
    "final_embeddings_images_rec_embed /= final_embeddings_images_rec_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "final_embeddings_texts_rec_embed /= final_embeddings_texts_rec_embed.norm(dim=-1, keepdim=True)\n",
    "final_embeddings_texts_rec_embed += mean_final_texts\n",
    "final_embeddings_texts_rec_embed /= final_embeddings_texts_rec_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with final embeddings on only the last layers\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the attention activations approach\n",
    "current_model = (mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns)\n",
    "_, indexes_approx_activ = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with attention activations\")\n",
    "\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed\n",
    "_, indexes_approx_final = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation of images without mean-ablation\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = final_embeddings_images_rec_embed\n",
    "_, indexes_approx_final = test_accuracy(current_model @ final_embeddings_texts_rec_embed.T, labels_, label=f\"Approximation of images and texts without mean-ablation\")\n",
    "\n",
    "# Evaluate accuracy using the reconstructed embeddings from the final embedding approach\n",
    "current_model = (mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns)\n",
    "_, indexes_approx_final = test_accuracy(current_model @ final_embeddings_texts_rec_embed.T, labels_, label=f\"Approximation of images and texts without mean-ablation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0c5f8",
   "metadata": {},
   "source": [
    "## Test bias removal and subset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we calculate scores for each principal component (PC) while ignoring query info.\n",
    "# We do this by reconstructing embeddings from the principal components alone, both from \n",
    "# the final embeddings perspective and the attention activations perspective.\n",
    "\n",
    "final_embeddings_images_rec_embed_topic = torch.zeros_like(final_embeddings_images)\n",
    "final_embeddings_images_rec_attns_topic = torch.zeros_like(final_embeddings_images)\n",
    "\n",
    "image_emb_cent_embed = final_embeddings_images - mean_final_images\n",
    "\n",
    "for entry in top_k_details:\n",
    "    # Reconstruct embeddings focusing on each principal component:\n",
    "    # 1. Start from the final embeddings, center them, and extract the component of interest.\n",
    "    vh = torch.tensor(entry[\"vh\"])\n",
    "    project_matrix = torch.tensor(entry[\"project_matrix\"])\n",
    "    princ_comp = torch.tensor(entry[\"princ_comp\"])\n",
    "    s = entry[\"strength_abs\"]\n",
    "    projection_image_embed = image_emb_cent_embed @ vh.T\n",
    "    mask_images_embed = torch.zeros_like(projection_image_embed)\n",
    "    mask_images_embed[:, princ_comp] = projection_image_embed[:, princ_comp]\n",
    "    final_embeddings_images_rec_embed_topic += mask_images_embed @ project_matrix @ vh\n",
    "\n",
    "    # Repeat for attention-based activations:\n",
    "    mean_values_att = torch.tensor(entry[\"mean_values_att\"])\n",
    "    image_emb_cent_attns = attns_[:, entry[\"layer\"], entry[\"head\"], :] - mean_values_att\n",
    "    projection_images_attns = image_emb_cent_attns @ vh.T\n",
    "    mask_images_attns = torch.zeros_like(projection_images_attns)\n",
    "    mask_images_attns[:, princ_comp] = projection_images_attns[:, princ_comp]\n",
    "    final_embeddings_images_rec_attns_topic += mask_images_attns @ project_matrix @ vh + mean_values_att\n",
    "\n",
    "final_embeddings_images_rec_embed_topic /= final_embeddings_images_rec_embed_topic.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# Compute accuracy using the reconstruction from final embeddings, ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_embed - final_embeddings_images_rec_embed_topic\n",
    "_, indexes_approx_final_rem = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with current topic final embeddings (Bias removal)\")\n",
    "print_diff_elements(indexes_approx_final, indexes_approx_final_rem)\n",
    "\n",
    "# Compute accuracy using the reconstruction from attention activations, also ignoring the query information.\n",
    "current_model = mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns - final_embeddings_images_rec_attns_topic\n",
    "_, indexs_approx_activ_rem = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with current topic final embeddings (Bias Removal)\")\n",
    "print_diff_elements(indexes_approx_activ, indexs_approx_activ_rem)\n",
    "\n",
    "# Compute accuracy using the reconstruction from final embeddings, ignoring the query information.\n",
    "current_model = final_embeddings_images_rec_embed_topic\n",
    "_, indexes_approx_final_rem = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with current topic final embeddings (Subset)\")\n",
    "print_correct_elements(indexes_approx_final_rem)\n",
    "\n",
    "# Compute accuracy using the reconstruction from attention activations, also ignoring the query information.\n",
    "current_model = mlps_.sum(axis=1) + current_mean_ablation_per_head_sum_ + final_embeddings_images_rec_attns_topic\n",
    "_, indexs_approx_activ_rem = test_accuracy(current_model @ classifier_, labels_, label=f\"Approximation with current topic final embeddings (Subset)\")\n",
    "print_correct_elements(indexs_approx_activ_rem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee21ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
