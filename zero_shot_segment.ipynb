{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da9875-f73a-4d20-94f0-8bb09288f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torch.nn import functional as F\n",
    "import einops\n",
    "from utils.models.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.misc.visualization import image_grid, visualization_preprocess\n",
    "from utils.models.prs_hook import hook_prs_logger    # Logger for applying hook to model\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.scripts.algorithms_text_explanations_funcs import *\n",
    "from utils.datasets_constants.imagenet_classes import imagenet_classes\n",
    "from utils.datasets_constants.cifar_10_classes import cifar_10_classes\n",
    "from utils.datasets_constants.cub_classes import cub_classes, waterbird_classes\n",
    "import os\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c455c7a2-fdb8-446b-ad9b-d768939be423",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cpu'\n",
    "model_name = 'ViT-B-32' # 'ViT-H-14'\n",
    "seed = 0\n",
    "num_last_layers_ = 4\n",
    "subset_dim = None\n",
    "tot_samples_per_class = None\n",
    "dataset_text_name = \"top_1500_nouns_5_sentences_imagenet_clean\"\n",
    "datataset_image_name = \"binary_waterbirds\"\n",
    "algorithm = \"svd_data_approx\"\n",
    "path = './datasets/'\n",
    "\n",
    "if model_name == \"ViT-H-14\":\n",
    "    pretrained = \"laion2B-s32B-b79K\"\n",
    "elif model_name == \"ViT-L-14\":\n",
    "    pretrained = \"laion2B-s32B-b82K\"\n",
    "elif model_name == \"ViT-B-16\":\n",
    "    pretrained = \"laion2B-s34B-b88K\"\n",
    "elif model_name == \"ViT-B-32\":\n",
    "    pretrained = \"laion2B-s34B-b79K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f170be-ab26-4405-bf49-c44f9c1644b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Model\n",
    "model, _, preprocess = create_model_and_transforms(model_name, pretrained=pretrained, cache_dir=\"../cache\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model.visual.transformer.resblocks))\n",
    "\n",
    "prs = hook_prs_logger(model, device, spatial=True) # This attach hook to get the residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65357dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new created attention datasets\n",
    "attention_dataset = f\"output_dir/{datataset_image_name}_completeness_{dataset_text_name}_{model_name}_algo_{algorithm}_seed_{seed}.jsonl\"\n",
    "\n",
    "# Load necessary data\n",
    "attns_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_attn_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], attention values\n",
    "mlps_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_mlp_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], mlp values\n",
    "classifier_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_classifier_{model_name}.npy\", mmap_mode=\"r\"))  # [b, l, h, d], embedding of the labels\n",
    "labels_ = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_labels_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\")) # Position of the labels in the cosndiered dataset\n",
    "final_embeddings_images = torch.tensor(np.load(f\"output_dir/{datataset_image_name}_embeddings_{model_name}_seed_{seed}.npy\", mmap_mode=\"r\"))\n",
    "final_embeddings_texts = torch.tensor(np.load(f\"output_dir/{dataset_text_name}_{model_name}.npy\", mmap_mode=\"r\"))\n",
    "with open( f\"utils/text_descriptions/{dataset_text_name}.txt\", \"r\") as f:\n",
    "    texts_str = np.array([i.replace(\"\\n\", \"\") for i in f.readlines()])\n",
    "# Get mean ablation\n",
    "no_heads_attentions_ = attns_.sum(axis=(2))  # Sum over heads dimension\n",
    "last_ = attns_.shape[1] - num_last_layers_\n",
    "# Replace attention activations until 'last' layer with their average, while keeping later layers intact.\n",
    "current_mean_ablation_per_head_sum_ = torch.mean(no_heads_attentions_[:, :last_ + 1], axis=0).sum(0)\n",
    "\n",
    "# Save important stuff\n",
    "nr_layers_ = attns_.shape[1]\n",
    "nr_heads_ = attns_.shape[2]\n",
    "\n",
    "if datataset_image_name == \"imagenet\":\n",
    "    ds_ = ImageNet(root=path+\"imagenet/\", split=\"val\", transform=visualization_preprocess)\n",
    "elif datataset_image_name == \"binary_waterbirds\":\n",
    "    ds_ = BinaryWaterbirds(root=path+\"waterbird_complete95_forest2water2/\", split=\"test\", transform=visualization_preprocess)\n",
    "elif datataset_image_name == \"CIFAR100\":\n",
    "    ds_ = CIFAR100(\n",
    "        root=path, download=True, train=False, transform=visualization_preprocess\n",
    "    )\n",
    "elif datataset_image_name == \"CIFAR10\":\n",
    "    ds_ = CIFAR10(\n",
    "        root=path, download=True, train=False, transform=visualization_preprocess\n",
    "    )\n",
    "else:\n",
    "    ds_ = ImageFolder(root=path, transform=visualization_preprocess)\n",
    "\n",
    "classes_ = {\n",
    "        'imagenet': imagenet_classes, \n",
    "        'CIFAR10': cifar_10_classes,\n",
    "        'waterbirds': cub_classes, \n",
    "        'binary_waterbirds': waterbird_classes, \n",
    "        'cub': cub_classes}[datataset_image_name]\n",
    "# Depending\n",
    "ds_vis_ = dataset_subset(\n",
    "    ds_,\n",
    "    samples_per_class=subset_dim,\n",
    "    tot_samples_per_class=tot_samples_per_class,  # or whatever you prefer\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Print metadata accuracy if waterbird\n",
    "if classes_ == waterbird_classes:\n",
    "    root = \"datasets/waterbird_complete95_forest2water2/\"\n",
    "    df = pd.read_csv(root + \"metadata.csv\")\n",
    "    filtered_df = df[df['split'] == 2]\n",
    "\n",
    "    s = [(os.path.join(root, filtered_df.iloc[i]['img_filename']), filtered_df.iloc[i]['y'], filtered_df.iloc[i]['place']) for i in range(len(filtered_df))]\n",
    "    background_groups_ = list([x[2] for x in s])\n",
    "\n",
    "# Retrieve Rank\n",
    "data = get_data(attention_dataset, skip_final=True)\n",
    "mean_rank_ = 0\n",
    "for entry in data:\n",
    "    mean_rank_ += entry[\"rank\"]\n",
    "mean_rank_ /= len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1227de5-e3ee-41b3-be91-f5504e69e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load image\n",
    "image_pil = Image.open('images/heart.png')\n",
    "image = preprocess(image_pil)[np.newaxis, :, :, :]\n",
    "_ = plt.imshow(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eebe1-d598-4c8f-a23d-45eff948cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the image and compute attentions and mlps\n",
    "prs.reinit()\n",
    "with torch.no_grad():\n",
    "    # Get the embedding of the image\n",
    "    representation = model.encode_image(image.to(device), \n",
    "                                        attn_method='head', # get patch contribution per fixed head\n",
    "                                        normalize=False)\n",
    "    attentions, mlps = prs.finalize(representation)  # attentions: [1, 12, 197, 16, 512], [b, l, n, h, d], mlps: [1, 13, 512], [b, l + 1, d]\n",
    "    attentions = einops.rearrange(attentions, \"b l n h d -> b l h n d\")\n",
    "    print(attentions.shape, mlps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85562f9b-65a6-4835-92ba-893e85db2407",
   "metadata": {},
   "source": [
    "## Visualize token decomposition on full output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a28a34-4121-49bd-b69c-16d19e4989e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the texts\n",
    "lines = [\"heart\"]\n",
    "texts = tokenizer(lines).to(device)  # tokenize text for encoder\n",
    "class_embeddings = model.encode_text(texts)\n",
    "print(class_embeddings.shape)\n",
    "print(attentions[0, :, :, 1:].sum(axis=(0,1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73673e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segment_map(attentions, class_embeddings, image_pil, lines, device, layer=None, head=None):\n",
    "    \"\"\"\n",
    "    Generate and display zero-shot segmentation maps for an arbitrary number of text embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        attentions (torch.Tensor): Attention maps from the model with shape [B, heads, tokens, tokens].\n",
    "                                   (Assumed that the first token is the CLS token.)\n",
    "        class_embeddings (torch.Tensor): Text embeddings for each class with shape [num_classes, embed_dim].\n",
    "        image_pil (PIL.Image): The original image (as a PIL image) on which to overlay the segmentation maps.\n",
    "        model: The model object (used here to get the patch size).\n",
    "        device: The torch device to perform interpolation on.\n",
    "        lines: class_names (list of strings)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Compose the attention map.\n",
    "    #    Skip the first token (CLS), sum over heads and remaining tokens,\n",
    "    #    then perform a dot product with the text embeddings (transposed) to get one score per patch per class.\n",
    "    #    Shape: [num_patches, num_classes]\n",
    "    if layer is None:\n",
    "        attention_map = attentions[0, :, :, 1:].sum(axis=(0, 1)) @ class_embeddings.T\n",
    "    else:\n",
    "        if head is None:\n",
    "            attention_map = attentions[0, layer, :, 1:].sum(axis=(0)) @ class_embeddings.T\n",
    "        else:\n",
    "            attention_map = attentions[0, layer, head, 1:] @ class_embeddings.T\n",
    "\n",
    "    # 2. Recover the spatial dimensions of the patches.\n",
    "    num_patches = attention_map.shape[0]\n",
    "    # Assuming a square grid of patches (this works for most vision transformer models)\n",
    "    dim_patch = int(np.sqrt(num_patches))\n",
    "\n",
    "    # 3. Rearrange the flat attention map into [1, num_classes, height, width]\n",
    "    #    and upscale it to the original image resolution.\n",
    "    attention_map = einops.rearrange(attention_map, '(N M) C -> 1 C N M', N=dim_patch, M=dim_patch)\n",
    "    # Get original image size; note that PIL returns (width, height)\n",
    "    width, height = image_pil.size  \n",
    "    attention_map = F.interpolate(\n",
    "        attention_map,\n",
    "        size=(height, width),  # F.interpolate expects size as (H, W)\n",
    "        mode='bilinear'\n",
    "    ).to(device)\n",
    "    \n",
    "    # Detach and convert to NumPy for visualization.\n",
    "    # Now attention_map has shape [num_classes, H, W]\n",
    "    attention_map = attention_map[0].detach().cpu().numpy()\n",
    "    \n",
    "    num_classes = attention_map.shape[0]\n",
    "    \n",
    "    # 4. Use the attention map directly as the difference map.\n",
    "    difference_maps = attention_map\n",
    "\n",
    "    # 5. Normalize globally (across all classes) for consistent color scaling.\n",
    "    global_min = difference_maps.min()\n",
    "    global_max = difference_maps.max()\n",
    "    \n",
    "    # 6. Plot the overlay for each class.\n",
    "    for i in range(num_classes):\n",
    "        diff = difference_maps[i]\n",
    "        # Avoid division by zero in case global_max equals global_min.\n",
    "        if global_max - global_min > 0:\n",
    "            normalized = (diff - global_min) / (global_max - global_min)\n",
    "        else:\n",
    "            normalized = np.zeros_like(diff)\n",
    "        \n",
    "        # Now `normalized` has values between 0 and 1.\n",
    "        plt.figure()\n",
    "        plt.imshow(image_pil)\n",
    "        # Overlay the normalized map using a colormap with fixed vmin/vmax to [0, 1]\n",
    "        plt.imshow(normalized, cmap='jet', alpha=0.8, vmin=0, vmax=1)\n",
    "        plt.title(f\"Class {lines[i]} Segmentation\")\n",
    "        plt.colorbar()  # This colorbar will now reflect values between 0 and 1.\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8c7b8-cf07-4fcd-aa6f-afdcdb8794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(nr_layers_):\n",
    "    plot_segment_map(attentions, class_embeddings, image_pil, lines, device, layer=k)\n",
    "plot_segment_map(attentions, class_embeddings, image_pil, lines, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628606d8",
   "metadata": {},
   "source": [
    "## Visualize token decomposition on partial output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77634ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 11\n",
    "head = 7\n",
    "pc = 0\n",
    "max_pcs_per_head = -1\n",
    "look_at_pc = False\n",
    "look_at_head = False\n",
    "mean_of_pc = False\n",
    "query_system = True\n",
    "pcs_per_class = 500\n",
    "all_layer = False\n",
    "\n",
    "embedding = mlps.sum(axis=(1)) + attentions.sum(axis=(1, 2, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b663ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "# Decide where to look at\n",
    "if look_at_pc:\n",
    "    data = get_data_component(data, layer, head, pc)\n",
    "elif look_at_head:\n",
    "    data = get_data_head(data, layer, head)\n",
    "# Whether to use the query system or not\n",
    "elif query_system:\n",
    "    # Patch model if needed\n",
    "    mean_final_texts = torch.mean(final_embeddings_texts, axis=0)\n",
    "    classes_centered = class_embeddings - mean_final_texts.unsqueeze(0)\n",
    "    sorted_data = []\n",
    "    for text_idx in range(classes_centered.shape[0]):\n",
    "        # Perform query system on entry\n",
    "        concept_i_centered = classes_centered[text_idx, :].unsqueeze(0)\n",
    "\n",
    "        data = get_data(attention_dataset, max_pcs_per_head, skip_final=True)\n",
    "\n",
    "        _, data_abs = reconstruct_embeddings(\n",
    "            data, \n",
    "            [concept_i_centered], \n",
    "            [\"text\"], \n",
    "            return_princ_comp=True, \n",
    "            plot=False, \n",
    "            means=[mean_final_texts],\n",
    "        )\n",
    "\n",
    "        # Extract relevant details from the top k entries\n",
    "        data_pcs = sort_data_by(data_abs, \"correlation_princ_comp_abs\", descending=True)\n",
    "        top_k_entries = top_data(data_pcs, pcs_per_class)\n",
    "\n",
    "        # Derive nr_pcs_per_class\n",
    "        sorted_data += top_k_entries\n",
    "        print(f\"Currently processing label {lines[text_idx]} with nr_pcs_per_class: {pcs_per_class}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    entries_set = []\n",
    "    entries_meta = []\n",
    "    for entry in sorted_data:\n",
    "        layer = entry[\"layer\"]\n",
    "        head = entry[\"head\"]\n",
    "        princ_comp = entry[\"princ_comp\"]\n",
    "        if (layer, head, princ_comp) not in entries_meta:\n",
    "            entries_meta.append((layer, head, princ_comp))\n",
    "            entries_set.append(entry)\n",
    "\n",
    "    attentions_rec = reconstruct_all_embeddings_mean_ablation_pcs(\n",
    "        entries_set,\n",
    "        mlps,\n",
    "        attentions,\n",
    "        attns_,\n",
    "        nr_layers_,\n",
    "        nr_heads_,\n",
    "        num_last_layers_,\n",
    "        ratio=-1,\n",
    "        mean_ablate_all=False, \n",
    "        return_attention=True\n",
    "    )\n",
    "\n",
    "    for k in range(nr_layers_):\n",
    "        plot_segment_map(attentions_rec, class_embeddings, image_pil, lines, device, layer=k)\n",
    "    plot_segment_map(attentions_rec, class_embeddings, image_pil, lines, device)\n",
    "\n",
    "else:\n",
    "    top_k_entries = top_data(data, pcs_per_class)\n",
    "\n",
    "    attentions_rec = reconstruct_all_embeddings_mean_ablation_pcs(\n",
    "        top_k_entries,\n",
    "        mlps,\n",
    "        attentions,\n",
    "        attns_,\n",
    "        nr_layers_,\n",
    "        nr_heads_,\n",
    "        num_last_layers_,\n",
    "        ratio=-1,\n",
    "        mean_ablate_all=False, \n",
    "        return_attention=True\n",
    "    )\n",
    "    plot_segment_map(attentions_rec, class_embeddings, image_pil, lines, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076542ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dc4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
